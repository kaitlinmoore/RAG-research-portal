# rand_ai_ssa2024 -- Artificial Intelligence and Machine Learning for Space Domain Awareness: The Development of Two Artificial Intelligence Case Studies

**Authors:** Jonathan Tran, Prateek Puri, Jordan Logue, Anthony Jacques, Li Ang Zhang, Krista Langeland, George Nacouzi, Gary J. Briggs
**Venue:** RAND Research Report (RR-A2318-2) (2024)
**DOI:** https://doi.org/10.7249/RRA2318-2

---

## sec0 -- Summary

[sec0_p1] To address the growing demands of operating in the space domain, space domain awareness (SDA) operators must determine how to prioritize sensor observations more effectively, scale up to meet the sheer volume of resident space objects, and develop analytic capabilities that reflect the complexity of orbital mechanics and space operations, all while maintaining the responsiveness necessitated by operations in a warfighting domain. These factors present significant challenges to those tasked with the SDA mission and point to this mission as a prime candidate for support from artificial intelligence (AI) and machine learning (ML) tools, as such tools have the potential to increase the analysis tempo, expand the amount of usable data for this analysis, and free up operator time for more-complex tasks. AI/ML tools may help SDA operators meet these increasing challenges.

[sec0_p2] Although AI/ML tools have the potential to help meet these SDA challenges, the impact of these tools on the overall success of the SDA mission is not well understood, and this lack of understanding is a barrier to plan for and optimize the integration of these tools. The research documented in this report seeks to develop insight into the following questions through the development of AI/ML case studies: How can existing AI/ML and automation technologies benefit the SDA mission? Which processes within the SDA mission area can be improved from increased automation and AI/ML? What are the prerequisites and enabling technologies that are necessary for AI/ML implementation? What limitations (inherent or built in) should be considered when employing these technologies?

[sec0_p3] The approach outlined in this report focuses on the development of two AI/ML case studies that explore the value of AI/ML for SDA mission processes. These two case studies offer a way to investigate in detail how AI/ML could be leveraged and to emphasize potential challenges to implementing an operational AI/ML tool. Accomplishing this first requires an understanding of the processes that make up this mission, the requirements and metrics for these processes, where there are opportunities for impact through leveraging AI/ML tools, and the nature of what these tools would change in the process and the outcomes. The work presented here focuses on the highly resource-intensive conjunction assessment mission under the broader SDA mission set. The case studies focus on prediction and classification capabilities of neural networks and use of the capabilities to improve the conjunction assessment. The report also addresses the need for uncertainty quantification in AI/ML tools for both improving the prediction and classification performance and mitigating some of the potential AI/ML policy barriers, including facilitating operator-machine teaming and operator trust.

[sec0_p4] Neural networks are capable approximators for complex nonlinear functions. The report demonstrates that prediction and classification capabilities can be applied to SDA mission processes with standard and Bayesian neural networks. AI/ML tool developers should focus on tools that are compatible with the operational SDA architecture, data infrastructure, and processes. Quantification of risk and uncertainty tolerance provides an operator or analyst with the capability to provide feedback to an AI/ML model by setting thresholds based on acceptable risk. Quantification of risk and uncertainty tolerance can support improved performance of AI/ML tools focused on prediction and classification when compared with standard neural network approaches. Active learning may be an attractive AI/ML feature when paired with SDA mission processes.

[sec0_p5] The development of AI/ML tools requires significant investment in ensuring high-quality training data. The Office of the Chief Scientist of the U.S. Air Force, with input from SDA operators, should consider the availability of such data for AI/ML investment. AI/ML tool developers should consider SDA processes and limitations that have implications for tool design, including the selection of algorithms, metrics of performance, and benchmark requirements. The U.S. Department of Defense should continue to consider the value of uncertainty quantification methodologies when developing and deploying operational AI/ML tools. SDA operators should be trained to use the uncertainty quantification provided by these methodologies.

---

## sec1 -- Space Domain Awareness Mission Overview

[sec1_p1] The third space age began around 2015, and since then we have seen significant changes to the space domain: (1) Space is becoming increasingly congested and contested with the proliferation of low-earth-orbit (LEO) objects and burgeoning interest from both commercial and national actors; (2) the United States is increasingly reliant on space as a major component of military, commercial, and civilian operations; and (3) there is a real risk of space becoming a semipermanently degraded and operationally limited warfighting domain due to intentional or unintentional fragmentation events.

[sec1_p2] The U.S. government's space domain awareness (SDA) mission "encompasses the effective identification, characterization and understanding of any factor associated with the space domain that could affect space operations and thereby impact the security, safety, economy, or environment of our Nation." SDA is vital to maintaining safe and secure access to space, deterring adversary actions in space, and achieving space superiority should a crisis or conflict occur. As a result, SDA plays a pivotal role within the space mission assurance framework.

[sec1_p3] The changing SDA landscape introduces new requirements and challenges for operators and warfighters. Present-day U.S. Department of Defense (DoD) SDA infrastructure is a complex system of systems from the Cold War era, and the underlying design assumptions were not optimized for today's congested and contested space environment. However, making improvements to the underlying infrastructure has proven to be a challenging task; Space Systems Command has numerous efforts underway to update the software and hardware that facilitate SDA. In parallel with improvements to data infrastructure, artificial intelligence (AI) and machine learning (ML) technologies present one opportunity to improve SDA mission processes by providing operators with methods to quickly analyze large volumes of data. Understanding how to optimally leverage these technologies and planning for their expected impact on the SDA mission could potentially help improve mission effectiveness, optimize resource management, and inform investment decisions. This chapter provides an overview of the technologies, processes, and organizations that compose SDA operations at U.S. Space Command (USSPACECOM), focusing on select current mission processes that provide context to the AI/ML case studies presented later in the report.

---

### sec1.1 -- Space Command and Control

[sec1.1_p1] The space command and control (C2) program is the backbone of SDA operations at USSPACECOM. The program ingests data from a variety of sensor sources into a data repository. The data are then processed using numerous SDA applications. The processed data are then passed along to decisionmakers for both mission planning and execution of tactical operations, as well as for tasking the sensor network for additional observations to refine SDA analyses. This process is iterative and continuously ongoing at the 18th and 19th Space Defense Squadrons (SDSs), components of Space Delta 2 tasked with performing USSPACECOM's SDA mission.

[Figure 1.1: Space C2 Process. Source: Reprinted from U.S. Government Accountability Office, Space Command and Control.]

#### sec1.1.1 -- Historical Context of SDA

[sec1.1.1_p1] The U.S. government has a long history of executing something akin to SDA, culminating in its prioritization as one of the U.S. Space Force's (USSF's) five core competencies. The system architecture known as the Space Defense Operations Center (SPADOC) and the attendant Space Surveillance Network (SSN) trace their histories back to the early 1960s, when they primarily supported ballistic missile defense. In the early days of space operations, operators held that space was so voluminous that the odds of collision were remote--this is sometimes referred to as the big sky theory. However, there remained a national security imperative for tracking domestic and foreign objects in orbit. Once someone realized that man-made satellites were merely objects on noncolliding ballistic trajectories, many missile defense sensors became dual-hatted with supporting the new space situational awareness (SSA) mission in their spare time.

[Figure 1.2: SSN. Source: Reprinted from NASA, NASA Spacecraft Conjunction Assessment and Collision Avoidance Best Practices Handbook. Note: Det = detachment; GSSAP = Geosynchronous Space Situational Awareness Program; LSSC = Lincoln Space Surveillance Complex; MSSS = Maui Space Surveillance System; ORS = Operationally Responsive Space; RTS = Reagan Test Site; SBSS = Space Based Space Surveillance; SPCS = space control squadron; SST = Space Surveillance Telescope; STSS = Space Tracking and Surveillance System.]

[sec1.1.1_p2] Over the course of the next few decades following the 1960s, the SSA mission began to grow, alongside the population of space objects. The 18th SDS and its past forms became known for coordinating the detection, identification, and tracking of all earth-orbiting resident space objects (RSOs) above 10 cm. These actions compose the critical task of space catalog maintenance: the bookkeeping of RSO custody over time by way of position and velocity state variables for the purpose of attribution.

[Figure 1.3: Tracked Space Objects, as of April 2022. Source: Reprinted from Astromaterials Research and Exploration Science, NASA, "Legend."]

[sec1.1.1_p3] In October 2019, Air Force Space Command (AFSPC) coined the phrase space domain awareness to reflect the acknowledgment of space as a domain of warfare--as stated by then-Maj Gen John Shaw: "the implication of space as a warfighting domain demands we shift our focus beyond the Space Situational Awareness mindset of a benign environment to achieve a more effective and comprehensive SDA." That environment includes the "identification, characterization and understanding of any factor, passive or active, associated with the space domain that could affect space operations and thereby impact the security, safety, economy or environment of our nation." Recent doctrinal guidance now identifies SSA as the subset of SDA that focuses on the orbital segment--in particular, the provisional knowledge and characterization of space objects for space safety and sustainability. Characterization adds the requirement for some modicum of understanding regarding an RSO's intended usage and capabilities.

#### sec1.1.2 -- Key SDA Organizations and Systems

[sec1.1.2_p1] The 18th SDS is the component of the USSF Space Delta 2 tasked with executing USSPACECOM's SDA mission out of Vandenberg Space Force Base (SFB). In particular, the 18th SDS coordinates the acquisition and postprocessing of SSN data to monitor activity in space and maintain custody of RSOs. The 18th SDS does so through the aggregation of observations from the SSN: calculating current and predicted orbital states for RSOs, conducting launch detection, tracking, and identifying potential collisions for NASA manned and unmanned assets. The squadron's comprehensive catalog of satellites had more than 46,000 objects, including 6,000 active spacecraft, as of 2022. Activated in 2022, the 19th SDS is also a component of Space Delta 2 and has assumed the mission of conjunction assessment (CA) from the 18th SDS. The 19th SDS serves as a primary backup to the mission functions at the 18th SDS. Together with the 18th SDS, the 19th SDS maintains and populates the publicly available space catalog Space-Track.org.

[sec1.1.2_p2] The two primary systems used by the 18th SDS as part of the SDA mission are the SPADOC and the Command, Analysis, Verification, and Ephemeris Network (CAVENet). SPADOC was brought online in 1979 in the Cheyenne Mountain Complex as a cataloging system, and its last significant upgrade was in 1989. By the 2000s, the challenge of a growing satellite catalog was apparent, as was the opportunity to take advantage of advances in computing to help address it. This prompted the USSF to develop CAVENet to augment SPADOC's capabilities and provide data analysis and sharing capabilities on a local network. CAVENet workstations run the Astrodynamics Support Workstation (ASW) software, which provides analytic tools that were originally intended to conduct analysis of historical element sets and observations. CAVENet is an offline system for conducting classified analysis; therefore, this system runs the ASW software for collision analysis and threat assessment at the classified level. As the growing SDA missions continue to outpace SPADOC computing infrastructure, CAVENet has taken on expanding tasks. The ASW software on CAVENet now keeps a separate space catalog for analysis, and CAVENet is responsible for developing the daily tasking plan for the SSN. ASW currently hosts several typically custom-built software tools that maintain the High Accuracy Catalog (HAC) and facilitate analysis to support different SDA tasks, among them Automatic Differential Corrections (ADC), Dynamic Calibration of the Atmosphere (DCA), and Automatic Special Perturbations Ephemeris Generation (ASP).

#### sec1.1.3 -- Sensor Tasking

[sec1.1.3_p1] Contemporary SSA data originate from various sources, including the SSN, commercial networks, and allies and partners that contribute data from both ground- and space-based sensors. However, the legacy systems underlying USSPACECOM's SDA mission currently support only observations from the SSN network. Data flow from dedicated and nondedicated SDA sensors, the latter of which remain dual-hatted with early warning and missile detection missions. Each sensor in the SSN exhibits different phenomenologies, reliabilities, percentage downtime for maintenance, and precisions; all of these must be accounted for as part of the sensor tasking process. For example, phased-array sites use narrowband radar with electronic steering to cover wide swathes of the sky--the trade-off is that these waves do not provide adequate resolution at higher altitudes and so are mostly limited in use to LEO. Additionally, narrowband radar is more greatly affected by scintillation, the disruption of electromagnetic waves by water molecules suspended in the atmosphere. There are also both ground- and space-based optical sensors that are more suitable for deep space observation, but there are numerous factors that can affect the sensor performance, including weather for ground-based sensors and the orientation of the sun for space-based sensors.

[sec1.1.3_p2] Even under normal operating conditions, there is far more demand on the SSN than availability. SDA tasking must account for the relative priority of observation, so analysts must coordinate observation collection according to sometimes conflicting data needs, priorities, and sensor suitability and availability. For example, high-interest objects (HIOs) may require a higher revisit rate for sensor collection than an object characterized as space debris. In addition to these varying priorities, the sheer data volume, the complexities of orbital mechanics and space operations, and the accelerated operations tempo required to conduct the SDA mission in a crisis or conflict present significant challenges to the orbital analyst (OA).

[sec1.1.3_p3] The daily sensor tasking process begins with the swing-shift OA from the 18th SDS. One of the OA's responsibilities is building the daily composite (COM) file, which includes taskings for all the sensor sites and may also be referred to as the consolidated tasking list (CTL). The sensor pointing is based on locally stored sets of two-line elements (TLEs), called field elements sets. The sensor sites record their raw observations as time series of range, elevation angle, and azimuth angle, which are then reported back to the 18th SDS to be merged with data across all sensor sites. The cyclical process is then completed with an update of the HAC using the orbit determination process described below.

#### sec1.1.4 -- Orbit Determination

[sec1.1.4_p1] Orbit determination describes the process employed by the 18th SDS for determining a space object ephemeris from raw sensor observations. These raw observations are collected from a multitude of sensor types and are composed of such data as range, range rate, azimuth angle, and elevation angle--all with respect to the sensor site's local frame of reference. The raw data collected by the sensor sites accumulate on SPADOC's Space Data Server. Each observation is then transformed into a common reference frame and reformatted into cartesian position and velocity vectors for compatibility with the CAVENet's ASW software suite.

[sec1.1.4_p2] An ephemeris represents an average state calculated from numerous individual observations according to a linear algebra regimen known as differential corrections (DC). Strings of ephemerides are then referred to as tracks if they were calculated from past observations in the space catalog or trajectories if they were predicted into the future using orbit propagation methods. The ADC tool is the nominal, automated tool in charge of calculating new HAC ephemerides as new observations come into CAVENet from SPADOC. ADC applies a weighted root-mean-squared threshold to screen the combined error carried forward into each new ephemeris from its constituent observations while rigorously accounting for the underlying uncertainties; those ephemerides failing this test get flagged for manual inspection by human analysts. ADC then produces both a mean estimate of the state and a covariance for the RSO.

[sec1.1.4_p3] The NASA human spaceflight orbital safety analyst (OSA) initiates the process of updating the 18th SDS's predictions for all future RSO trajectories from the most-recent ephemerides produced by ADC. The resulting trajectory updates go on to inform the CA process performed by the 19th SDS and NASA, facilitate SSN sensor pointing, and support on-orbit operations by the commercial owner-operator (O/O), among other tasks.

[sec1.1.4_p4] A process known as extended general perturbations (EGP) converts the RSO state vectors contained within the HAC into the canonical TLE format long used by SPADOC and the broader space community to represent RSO states. The TLEs that are transferred and cataloged in SPADOC for internal use are known as operational element sets and are updated as frequently as new values arrive.

#### sec1.1.5 -- Measurement Uncertainty

[sec1.1.5_p1] Measurement uncertainty begins at the observation level and is carried forward through each downstream SDA mission process. Remote sensor observation is DoD's primary method for measuring RSO state, so it is important to characterize the numerous sources and impacts of uncertainty--this is the primary reason for performing DC. Note that the satellite O/O oftentimes has access to more-precise methods of orbit determination, such as GPS navigation or transponder pings. USSPACECOM can integrate these data into the SDA mission process but has no authority to require the O/O to provide them. This situation is especially true for adversarial satellites, noncooperative space objects, and inert debris. Sensor uncertainty is quantified using covariance, the matrix of values containing the standard deviation of each state variable with respect to itself and the other state variables. The standard deviation of a given observation state ultimately depends on such sources of uncertainty as structural modeling uncertainties, dynamic parameter uncertainties, measurement noise and sensor bias, compounding numerical errors, and hardware and software faults.

[sec1.1.5_p2] To characterize these sources of uncertainty, the sensor sites regularly calibrate against more-accurate independent observations. For example, there exists a network of laser calibration sites concurrent with the SSN that provide highly accurate observations of specialized calibration satellites. Additionally, certain satellite constellations--such as GPS or the Tracking and Data Relay Satellite system--accurately account for their states as part of their core functions and therefore can also provide reference for calibration.

[sec1.1.5_p3] The state of an RSO can never be definitively established because of these and other sources of uncertainty. However, it is possible to reduce the relative impact of the combined uncertainty on the final-state estimate using such methods as nonlinear batched least squares. Numerous analysis methodologies leverage the standardized behavior characteristic of otherwise random normal distributions of observations to facilitate the SDA mission.

#### sec1.1.6 -- Simplified General Perturbation Propagation

[sec1.1.6_p1] Simplified general perturbation (SGP) is a group of standard mathematical algorithms used to calculate future orbital state vectors and trajectories of an RSO. The SGP algorithm was originally developed in 1959 by Yoshihide Kozai, updated by C. G. Hilton and J. R. Kuhlman in 1966, and later made publicly available in 1980. Since its release, it has become the most widely used orbit propagator in the world. Over time, there have been numerous updates, enhancements, and refactoring efforts, but the fundamental algorithm has remained largely unchanged. More recently, the USSF has continually released an official implementation of the SGP4 algorithm that is used in operations as part of the AstroStds software library.

[sec1.1.6_p2] The SGP algorithms use TLE sets and employ either an analytic or semianalytic approach for orbit propagation. Although SGP4 propagation is now widely adopted in various applications, it was originally designed to offer fast and computationally efficient trajectory predictions, where a certain level of error was deemed acceptable. One limitation to the general perturbation methods is it does not produce a covariance matrix, requiring new techniques, such as special perturbation, to be developed.

[sec1.1.6_p3] SGP4-XP was an algorithm developed by the USSF that was released in December 2020 and was designed for the propagation of TLEs featuring extended perturbations. The release notes associated with this release claim that SGP4-XP is appropriate for applications that require HAC-level accuracy. Although the SGP4-XP algorithm expands on the capabilities of its predecessor, SGP4, with minimal additional requirement in terms of input TLE format and computational cost, SGP4-XP is still an analytic method that does not produce a covariance matrix. At the time of writing, the authors were uncertain whether SGP4-XP was being used in an operational context by either the 18th or the 19th SDS.

#### sec1.1.7 -- Special Perturbation Propagation

[sec1.1.7_p1] The 18th SDS leverages a more accurate special perturbation (SP) methodology for maintaining its internal HAC and propagating future trajectories for conjunction screening. SP algorithms are necessary for precise orbit determination because they facilitate bookkeeping of RSO state uncertainty by taking a numerical, sequential approach to orbit propagation. This approach involves dividing the satellite's motion into discrete time steps and calculating each following position and velocity based on the effect of higher-order perturbation forces modeled across the time step. Such higher-order forces extend beyond the idealized two-dimensional gravitational pull from earth to include the temporal difference of gravity caused by earth's oblateness, other nearby celestial bodies, and other irregularities, such as atmospheric drag and solar radiation pressure.

[Figure 1.4: Higher-Order Perturbation Forces. Source: Adapted from Montenbruck and Gill, Satellite Orbits. Note: TDRS = Tracking and Data Relay Satellite.]

[sec1.1.7_p2] The specific SP implementation in use operationally has not been publicly released, but the authors speculate based on methodologies described by NASA to guide AI algorithm development. A commonly cited algorithm for SP is Cowell's method, which simply performs a linear summation of the perturbing forces. Cowell's method assumes that the perturbing forces are small relative to the primary Keplerian forces, requires a high-order numerical integration method to evolve the equations of state, and relies on the user to account for many significant digits.

[sec1.1.7_p3] Previous sections discussed the sensor measurement uncertainty captured at the time of observation and accounted for by the DC process to produce HAC ephemerides. There is also propagation error present in numerical methods: errors or uncertainties in the input data or initial conditions of a computational problem that can affect the accuracy of the computed results. Numerical methods involve approximations and computations, and even small errors in the input data or numerical procedures can accumulate and lead to significant discrepancies in the final output. Forms of numerical errors include roundoff error, truncation error, and algorithmic error, all of which can exacerbate the initial measurement uncertainty. These numerical errors are essential for the CA process and are output by the SP algorithms in the form of a covariance matrix.

---

### sec1.2 -- Conjunction Assessment Process

[sec1.2_p1] CA is the probabilistic process by which the 18th and 19th SDSs quantify risk of collision in space. The process is probabilistic rather than deterministic because of the numerous and chaotic orbital forces that progressively perturb an object's trajectory away from its theoretical path over time. Because the closing velocities involved in space collisions necessitate avoidance maneuvering far in advance of the predicted time of closest approach (TCA), it is (currently) impossible to know exactly where the two colliding objects will be at the TCA. The CA process flags pairs of RSOs that are at high risk of collision.

[sec1.2_p2] In the early days of space operations, operators held that the space domain was so voluminous that the odds of collision were remote--this was sometimes referred to as the big sky theory. Subsequent collisions demonstrated the invalidity of this assumption, as well as the outsize impact of even a single collision on the health of the space environment, leading to broad agreement on the need to assess the risk of collision between space objects; specifically, the risk of collision requires a dynamic assessment of risk that could support continuous ingestion of new information to improve prediction confidence and avoid unnecessary maneuvering. Probability of collision (Pc) serves as this measure of confidence in a predicted collision actually occurring. Increasing Pc accuracy is thus a key objective for CA; to achieve it, the 18th and 19th SDSs leverage their orbit determination and sensor tasking processes to aggregate raw sensor observations in sufficient quantities and qualities.

[sec1.2_p3] The 19th SDS is currently responsible for conducting CA for all space objects. Using the latest RSO epoch states and trajectories from the orbit determination process, the 19th SDS continuously conducts CAs using the ASW tools Automatic Conjunction Assessment (AutoCA) and Super Computation of Miss Between Orbits (SuperCOMBO). AutoCA performs the all versus all initial screenings that check each HAC object against every other HAC object for five to ten days into the future, depending on the orbital regime. For those satellites whose O/Os provide screening ephemerides, the 19th SDS performs additional all versus O/O and O/O versus O/O screenings.

[sec1.2_p4] [Table 1.1: CA Screening Volumes.] Deep space ellipsoid screening (period 1,300--1,800 min, eccentricity < 0.25, inclination < 35 degrees) uses a 10-day propagation with 10 km radial, in-track, and cross-track volumes. LEO 4 covariance screening (perigee 1,200--2,000 km, eccentricity < 0.25) uses a 5-day propagation with 0.4 km radial, 2 km in-track, and 2 km cross-track. LEO 3 covariance screening (perigee 750--1,200 km) uses 5-day propagation with 0.4 km radial, 12 km in-track, and 2 km cross-track. LEO 2 covariance screening (perigee 500--750 km) uses 5-day propagation with 0.4 km radial, 25 km in-track, and 25 km cross-track. LEO 1 covariance screening (perigee <= 500 km) uses 5-day propagation with 0.4 km radial, 44 km in-track, and 51 km cross-track. O/O deep space ellipsoid screening (period > 225 min) uses 10-day propagation with 20 km volumes. O/O near earth covariance screening (period <= 225 min) uses 7-day propagation with 2 km radial and 25 km in-track and cross-track.

[sec1.2_p5] The 19th SDS applies the same underlying screening mathematics across all orbital regimes. Potential collisions are flagged by AutoCA when a secondary RSO's trajectory pierces the perimeter of an ellipsoid centered on the primary object being tested. For deep space objects, an elliptical screening method is employed in lieu of calculating Pc. All other RSOs are analyzed using a covariance-based screening approach that employs a joint uncertainty volume that is based on time-varying covariance values for the primary and secondary objects at TCA. Depending on the results of this test, the 19th SDS may deem it necessary to refine the orbits of the two objects to obtain a more confident assessment of collision risk--additional SSN taskings and updated orbit determinations may be requested from the 18th SDS. SuperCOMBO handles this iterative refinement and rescreening process, freeing AutoCA to continue screening the vast majority of RSOs.

[sec1.2_p6] [Table 1.2: Basic CA Reporting Criteria.] For deep space HAC screening, a conjunction data message is posted to Space-Track.org when TCA <= 10 days and overall miss <= 5 km; emergency notification occurs when TCA <= 3 days and overall miss <= 5 km. For near earth HAC screening, a conjunction data message requires TCA <= 3 days, overall miss <= 1 km, and Pc >= 1e-7; emergency notification requires TCA <= 3 days, overall miss <= 1 km, and Pc >= 1e-4. O/O ephemeris screenings use the same thresholds as their corresponding HAC screenings. Different reporting criteria may be used for O/Os with advanced reporting agreements with USSPACECOM.

[sec1.2_p7] The exact calculation of Pc may proceed from several different formulations. One such formulation is the long-practiced two-dimensional-projection methodology. This method applies a number of assumptions to enable decomposition of the problem from three to two dimensions: (1) near-instantaneous rectilinear motion, (2) negligible velocity uncertainty, (3) constant position uncertainty, and (4) linearly independent Gaussian uncertainty distributions. Additionally, the objects' geometries are greatly simplified down to circumscribing spheres; these are combined to form a single combined object sphere--a zone of exclusion--whose violation is taken to imply collision.

[Figure 1.5: Relative Positions, Uncertainties, and Velocities of Two Objects. Source: Adapted from NASA, NASA Spacecraft Conjunction Assessment and Collision Avoidance Best Practices Handbook, pp. 109-110.]

[sec1.2_p8] This zone of exclusion is collocated with the estimated location of the primary object at TCA. The assumption of linearly independent Gaussian uncertainty distributions allows the superposition of both objects' uncertainty ellipsoids into a single distribution, by convention collocated with the secondary object. Finally, the velocities for the two objects are combined into a single relative velocity vector, which is the velocity bringing them closer together or farther apart relative to the primary object.

[Figure 1.6: Decomposition of Collision Geometry. Source: Reproduced from Alfano and Oltrogge, "Probability of Collision," Figures 2 and 3 (CC BY 4.0); see also Alfano, "Relating Position Uncertainty to Maximum Conjunction Probability."]

[sec1.2_p9] The right side of Figure 1.6 represents the final two-dimensional geometry of the conjunction at TCA. This geometry is by definition static at TCA, so the Pc at TCA may be taken as the area integral of the combined uncertainty distribution over the location of the zone of exclusion.

[Equation 1: Probability of collision (Pc) computed as the area integral of the combined Gaussian position uncertainty distribution over the circular zone of exclusion in the encounter plane, where C is the combined position covariance matrix at TCA, d is the combined exclusion zone radius, and the secondary object position is along the x-axis in the encounter plane.]

---

### sec1.3 -- Future Data Architecture Uncertainties

[sec1.3_p1] To meet the emerging SDA challenges, there is a need to decommission and replace legacy systems used for this mission. The recognition of this need is not new, and finding a way to modernize or replace these systems has been a goal for DoD since the 1980s. One particular challenge emerges because sensor systems in the SSN were designed during the Cold War to address threats from nuclear weapons. Replacing computing and software to better address the complexities of today's SDA must be done without disrupting the ability to continue to monitor, detect, and assess these nuclear threats. Another challenge is turning the SSN sensor data into actionable information, requiring computers and software to integrate, manage, and analyze these data.

[sec1.3_p2] An effort to modernize the computing and data infrastructure of the SDA is currently underway at such organizations as L3Harris with the Advanced Tracking and Launch Analysis System and other products managed by Kobayashi Maru and Space Systems Command, Special Programs Directorate.

[sec1.3_p3] The need for accurate SSA has never been more widely distributed across actors in space as it is today. DoD finds itself supporting not only the national security space architecture but also scores of multinational commercial O/Os. Government-subsidized CA has artificially warped the economic calculus for potential commercial SSA providers, although more than a few currently exist. Research by these and other academics has called into question the validity of USSPACECOM's legacy CA methodology. As the orbital environment, and in particular LEO, grows increasingly crowded, conjunctions pose a growing threat. The transfer of non-DoD CA to the Department of Commerce may free up USSPACECOM personnel to interrogate their internal assumptions, and the department will be unencumbered by the same national security concerns that constrain innovation at the 18th and 19th SDSs.

[sec1.3_p4] Algorithms that support the SDA mission, whether AI or otherwise, should be developed and implemented with future operating conditions in mind. Additionally, SDA is likely to be augmented by non-SSN data in the future as the commercial and civil SSA sectors build out capabilities. AI/ML applications should be compatible with such an environment to ensure that future operations are supported effectively.

---

## sec2 -- Artificial Intelligence and Machine Learning Overview

[sec2_p1] This chapter introduces the two case studies developed to better understand the application of AI and ML for the SDA mission. It also provides a few examples of current efforts to leverage different AI capabilities and the technical background required to understand the methodology. AI, ML, and deep learning are phrases that are often perceived by the general public as interchangeable but, though related to one another, have distinct definitions. A comprehensive discussion of AI taxonomy is not within the scope of this report; thus, for the remainder of this work, the term AI/ML is used inclusively when referring to concepts within this taxonomy.

---

### sec2.1 -- Current AI/ML Efforts in SDA

[sec2.1_p1] AI/ML technologies, which offer many capabilities with different degrees of complexity, are being leveraged to build scalable, flexible solutions to challenges in a wide variety of sectors. Given the breadth of these capabilities and their overlap with SDA processes, many organizations have recently explored how these tools can be adapted to address SDA needs. Because of the high-stakes nature of many SDA processes, many decisions are currently too complex, high stakes, and nuanced to automate fully, even with advanced AI/ML systems. Therefore, many efforts to integrate AI/ML into SDA processes have focused on decision support (providing suggestions to operators) or automation of lower-stakes, repetitive tasks to free up analyst bandwidth for more-complex decisionmaking.

[sec2.1_p2] [Table 2.1: AI Capabilities.] Computer vision involves models that ingest visual imagery and perform tasks such as object identification, classification, and detection. Automated planning and scheduling, as well as execution, involves systems optimized to identify a sequence of actions that leads to a defined goal in a complex, multidimensional decision space. Prediction and classification involves discriminative models that perform categorizations of inputs or predict future elements of a sequence, often by drawing on historical data. Generative learning involves systems that synthetically produce language, images, or decisionmaking plans by leveraging past data. Decision support systems are rule-based systems created from expert and heuristic knowledge used to either support operators or automate tasks. Natural language processing involves systems designed to ingest, process, or generate human language for the purposes of topic modeling, text classification, and the synthesis of human-quality text.

[sec2.1_p3] To provide additional context on existing AI for SDA integrations, a subset of core SDA operational needs and ongoing efforts in the academic, private, and government sectors includes the following areas. Increased sensor detection and accuracy: Space Systems Command, Space Domain Awareness (SZG), is developing computer vision algorithms to improve the detection rate and precision for satellite observations in the Machine Learning for Space Superiority (MISS) program. The SensorCapsule program connects nontraditional sensors to the unified data library, enabling increased use of commercial sensing data. The mission-driven, autonomous, collaborative, heterogeneous, intelligent, network architecture (MACHINA) is an ongoing effort to optimize sensor management through sensor orchestration.

[sec2.1_p4] AI sandbox: Space Systems Command/SZG has the Pivot SDA portfolio and aims to provide a digital sandbox for AI and automation tools. This tool interfaces with unified data library data and enables individual providers to develop analytic tools that suit their specific needs on a government-owned platform, removing barriers to third-party development of tools and more-rapid deployment of models.

[sec2.1_p5] Anomaly detection: The Aerospace Corporation published research on calibrating uncertainties in AI anomaly detection algorithms. Aptima, a private firm that was awarded a SpaceWERX contract, is developing unsupervised AI algorithms to detect anomalous maneuvers. The Massachusetts Institute of Technology is researching the performance of tree-based algorithms and neural networks in identifying object maneuvers through simulations. Commercial organizations, such as ExoAnalytic Solutions, are also researching a variety of ML techniques and methods to perform anomaly detection and classification.

[sec2.1_p6] Data integration: Palantir won a $32.5 million contract to develop an AI-powered software platform to merge data from the USSF, the U.S. Air Force, and North American Aerospace Defense Command. SCOUT Space Inc. earned a small-business innovation research contract to develop its AI-centric software platform, Vision, for merging sensor data from multiple satellites to track space objects.

[sec2.1_p7] CA: Booz Allen Hamilton is developing a platform for mapping space-object trajectories and assessing the downstream outcomes of potential conjunction-avoiding maneuvers. The University of Colorado, Boulder, won a $5.5 million contract to automate collision-avoidance maneuvers. Other universities and private organizations are launching efforts to aid the USSF in this area.

---

### sec2.2 -- Selection of Two Case Studies

[sec2.2_p1] In this section, the report explores how the CA process may be improved through AI/ML tools. The goal is to use AI/ML as a function approximator to provide fast, albeit less accurate, trajectory estimations, thereby speeding up bottlenecks in existing processes. The two case studies were used to inform whether AI/ML tools could feasibly meet this goal in two aspects of the CA process.

[sec2.2_p2] The first case study imitates the elliptical screening process (ESP), and the second attempts to predict the future state and covariance of a given RSO ephemeris. As stand-alone applications, these two case studies may assist operators in key but limited aspects of SDA. Cooperatively, the case studies may alleviate the computational and manpower demands of CA analysis at a more significant level.

[sec2.2_p3] These case studies are selected for four primary reasons: (1) relevance to SDA operational needs, (2) the availability of training data representative of what is used by SDA operators, (3) a desire to deconflict with use cases being studied at other organizations, and (4) a desire to evaluate the extent to which SDA ML solutions can be prototyped on short time frames using open-source code.

[sec2.2_p4] With these considerations in mind, the authors observed that covariance propagation and elliptical screening are focus areas in which case study training sets can be built through internal RAND systems and are spaces that are relatively unsaturated by other existing AI/ML in SDA efforts, marking them as well-suited case study candidates. Lastly, the case studies were developed with consideration of current SDA processes. Although they may still have applicability if SDA infrastructure undergoes significant changes, as anticipated in the future, many of the design considerations and performance benchmarks would need to be reevaluated.

---

### sec2.3 -- Neural Network Architecture

#### sec2.3.1 -- Standard Neural Networks

[sec2.3.1_p1] Standard neural networks (SNNs) are ubiquitous AI/ML architectures that serve as the foundation for large language models, computer vision platforms, and audio processing systems. SNNs generally consist of a series of input, hidden, and output network layers that themselves are populated with network neurons, each of which is connected to neurons within other layers through linear combination operations and nonlinear activation functions. The weights and biases within each linear combination operation can be tuned to minimize a network objective function through a process known as backpropagation. The combination of tunable model parameters and nonlinear activation functions enables an SNN to capture highly nuanced, sophisticated relationships present within its underlying training sets.

#### sec2.3.2 -- Bayesian Neural Networks

[sec2.3.2_p1] A Bayesian neural network (BNN) is a probabilistic approach to neural networks that extends standard feedforward neural networks by incorporating Bayesian principles. BNNs provide a framework for modeling uncertainty in neural network predictions, making them particularly useful in tasks in which uncertainty quantification is essential, such as in medical diagnosis, autonomous driving, and financial forecasting.

[sec2.3.2_p2] Like SNNs, BNNs consist of interconnected layers of neurons, including input, hidden, and output layers. These neurons apply activation functions to their inputs and collectively learn to map input data to output predictions. In BNNs, instead of using fixed weights, as in SNNs, the parameters (weights and biases) of each neuron are treated as probability distributions. These distributions capture the uncertainty in the parameters. Common choices for these distributions include Gaussian distributions, but other distributions, such as scale mixtures or Cauchy distributions, can also be used, depending on the specific problem.

[Figure 2.1: SNN (Left) and BNN (Right). Source: Reprinted from Yu et al., "Structural Health Monitoring Impact Classification Method Based on Bayesian Neural Network," Figure 1 (CC BY 4.0). Note: W represents a scalar weight that conveys the importance of that corresponding feature.]

[sec2.3.2_p3] To make predictions for a new input data point, the BNN performs a prediction by sampling from the posterior distribution of the underlying model parameters. This process yields multiple predictions, each corresponding to a different set of sampled weight and bias values. The uncertainty in the predictions arises from the uncertainty in the parameter samples. One of the primary advantages of BNNs is their ability to provide uncertainty estimates along with their predictions. This uncertainty quantification can be used to assess the model's confidence in its predictions. For instance, a wider posterior distribution indicates higher uncertainty, whereas a narrower distribution indicates lower uncertainty.

[sec2.3.2_p4] BNNs can be computationally intensive and require specialized inference techniques. Additionally, given their modeling of uncertainties, BNNs generally require a larger number of parameters than SNNs of the same neuron number, leading to increased training times and potential hyperparameter optimization challenges. Finally, choosing appropriate prior distributions and inference methods is also crucial to application success and therefore is an active area of research.

---

### sec2.4 -- Managing Uncertainty

[sec2.4_p1] In modeling problems, there are two sources of uncertainty that are typically discussed: aleatoric and epistemic. Both combine to form the aggregate uncertainty of a model.

[sec2.4_p2] Epistemic uncertainty refers to uncertainty related to a model's inability to fully capture the patterns within its training data. Generally speaking, no model is perfect at reproducing the statistical process it is attempting to represent, and epistemic uncertainty characterizes the degree by which a model fails to do so. Epistemic uncertainty can often be reduced by increasing the amount of data a model is trained in, leveraging more-sophisticated model architectures or modifying optimization algorithms.

[sec2.4_p3] Aleatoric uncertainty, on the other hand, refers to uncertainty inherently present within the training dataset used to construct a model. A training set obtained from a real-world phenomenon may contain two rows of vectors with identical inputs yet different output values. A model trained on these data will be generally incapable of recapturing this pattern, since it is a result of irreducible label noise that is not detectable from the model inputs. For example, if you are predicting the outcome of a dice roll, the aleatoric uncertainty would account for the randomness in that outcome. This form of uncertainty cannot be reduced by gathering more data. In the context of SDA, the uncertainties and errors from sensor measurements and limitations in numerical orbit propagation algorithms contribute to the aleatoric uncertainty.

[sec2.4_p4] BNNs are particularly well positioned to characterize epistemic uncertainty. A single input can be passed through the model multiple times to generate a distribution of outcomes that reflects the model's uncertainty of how the inputs map onto the output response variable.

---

### sec2.5 -- Model Evaluation

[sec2.5_p1] In the subsequent two chapters, the report makes use of four common metrics to evaluate the performance of the AI/ML case studies. The first is mean absolute percentage error (MAPE), which is a commonly used metric in the field of statistics and data analysis to assess the accuracy of a predictive model, particularly in the context of regression analysis. It measures the percentage difference between the predicted and actual values of a variable, making it a useful tool for evaluating the relative accuracy of a model's predictions.

[Equation 2: MAPE is computed as (1/n) times the sum over all data points of the absolute value of (actual minus predicted) divided by actual, multiplied by 100.]

[sec2.5_p2] Mean squared error (MSE) is a widely used metric in the field of statistics, ML, and data analysis. It quantifies the average squared difference between predicted values and actual (observed) values in a dataset. MSE is a measure of the quality of a predictive model or estimator, and it is particularly useful in regression analysis, where the goal is to predict continuous numerical values.

[Equation 3: MSE is computed as (1/n) times the sum of squared differences between actual and predicted values.]

[sec2.5_p3] In the use cases explored, the output of an AI/ML model will be a three-dimensional (3D) probability distribution intended to represent knowledge of a satellite object's spatial position. The quality of the produced distributions will be assessed by measuring their similarity with a set of target probability distributions generated from traditional numerical distances. Therefore, additional metrics to assess the difference between probability distributions are needed.

[sec2.5_p4] The first metric used for this purpose is Kullback-Leibler (KL) divergence, a measure of the difference between two probability distributions. It is used to quantify how one probability distribution diverges from another.

[Equation 4: KL divergence between distributions P and Q is computed as the integral of P(x) times log(P(x)/Q(x)). For 3D Gaussians with diagonal covariance matrices, this reduces to a closed-form expression involving the ratios of variances, squared differences of means divided by variances, and log ratios of variances across the three dimensions.]

[sec2.5_p5] The second metric proposed is the Wasserstein distance, colloquially known as Earth mover's distance because of its interpretation as the cost of transforming one probability distribution into a second reference probability distribution. For 3D Gaussians with diagonal covariance matrices, the Wasserstein metric reduces to a sum over the three dimensions involving squared differences in means, and a term combining both variances.

[sec2.5_p6] There are key differences between Wasserstein distance and KL divergence that make each better suited to particular components of the methodology. One key factor considered in model evaluation is whether the uncertainties produced by the model are well calibrated with respect to the model distribution's mean offset from the target distribution. Unlike KL divergence, there are no terms within the Wasserstein distance that couple the means and standard deviations of the two respective distributions; thus, Wasserstein distance is not well suited to evaluate the uncertainty calibration of the model.

[sec2.5_p7] On the other hand, for neural network model training, a similarity metric must also be selected as an objective function. Although KL divergence has been used in many applications as an objective function, partly because of the natural logarithm term, it can produce more-volatile gradients that lead to unstable training. Within these case studies, KL divergence is used to evaluate model performance and Wasserstein distance as an objective function for model training. Over the experiments conducted, selecting Wasserstein distance as an objective function for model training generally mapped to high performance on KL divergence while also enabling fewer trials to be run in each experiment because of more stability in training loss.

---

### sec2.6 -- Summary

[sec2.6_p1] This chapter highlighted existing efforts to develop AI/ML for SDA mission processes. It described two AI/ML case studies, which use neural networks to perform prediction and classification tasks relevant to SDA operations. Finally, it provided an overview of neural network architectures and performance metrics, both of which will be referenced in the following chapters, which describe the case studies in greater depth.

---

## sec3 -- Elliptical Screening Process Tool

[sec3_p1] The first case study is the ESP tool, a BNN designed to replicate the conjunction screening process. The conjunction screening process involves screening through every combination pair of RSOs to identify and flag combination pairs whose future trajectories violate either an elliptical screening volume constraint or a Pc threshold. The tool presented here provides the operator with estimates of the distance of closest approach and an uncertainty. This tool may serve as a prescreen to reduce the number of RSO combination pairs fed into the computationally limited CAVENet for subsequent high-fidelity conjunction screening.

---

### sec3.1 -- Elliptical Screening Workflow Description

[sec3.1_p1] In the case study presented in this section, a BNN is provided two RSO states at a given epoch. The two RSO states may be the observation state provided directly from the sensor on the SPADOC system or the product of the DC process on the CAVENet system. The only requirement is that the two RSOs have an identical initial epoch. From these two initial states, the BNN performs a regression analysis and produces two outputs: (1) a prediction of the distance of closest approach over the next five days and (2) an estimate of uncertainty in the prediction. Postprocessing classification analysis of the BNN output is then conducted to determine whether the RSO pairing should be flagged for further review. When a possible conjunction event is flagged, additional resources and more-accurate algorithms can be dedicated to better characterize the combination pair.

[Figure 3.1: Case Study 1: Elliptical Screening Tool Workflow. The workflow shows a combination pair of RSO1 and RSO2 states input to a BNN, which produces a distance of closest approach prediction and uncertainty, followed by postprocessing classification for the SDA operator.]

---

### sec3.2 -- Data Generation

[sec3.2_p1] Data generation for AI/ML involves creating a dataset that can be used to train, validate, or test a neural network model. For SDA applications, a dataset can be real-world data collected by SDA operators stored on the CAVENet system or synthetic data produced using modeling and simulation. To a certain extent, the analysis tools used by SDA operators to produce real-world data also leverage modeling and simulation, but the exact algorithm, calibration parameters, and assumptions would be considered classified information. The decision to present the analysis to a general audience is the primary motivating factor to create an entirely synthetic dataset representative of data, modeling, and simulation methods used at the 18th and 19th SDSs.

[sec3.2_p2] When generating a synthetic dataset for the elliptical screening tool case study, the authors first referenced general perturbation orbit data provided by the U.S. government via CelesTrak in an Orbit Mean-Elements Message (OMM) Extensible Markup Language (XML) data format. To scope the development effort, the RSOs considered were constrained to Starlink satellites, which operate approximately between 540 and 570 km altitude--in inclinations that include 53.0, 70.0, and 97.6 degrees. Researchers have recently stated that the number of Starlink conjunction data message events with a computed Pc < 1e-5 accounted for 43,042 events; 35,047 of those were versus Starlink. The authors claim that a Starlink-focused dataset would provide enough diversity in orbit to remain nontrivial and prevent overfitting while providing enough conjunction warnings to adequately train the neural network.

[sec3.2_p3] Using the general perturbation data from CelesTrak, a random combination pair of satellites was sampled. For each satellite, an ephemeris contained in a single orbital period was randomly assigned to act as the starting condition. These initial conditions provide an infinite number of permutations to construct the training dataset.

[Figure 3.2: Notional Combination Pair Selection.]

[sec3.2_p4] Once the combination pairs are sampled and initial conditions are selected, the distance of closest approach must be evaluated over a time horizon of five days. The authors leveraged the Cowell's method implementation in an open-source Python library for astrodynamics and orbital mechanics called Poliastro. The algorithm accounts for the Keplerian forces in addition to higher-order dynamical forces. The initial-state conditions are then propagated to the time horizon, and the distance between the two objects is evaluated at each step. The distance and epoch of closest approach are then stored as an output variable. This process is repeated to generate a 2 million data point training dataset.

---

### sec3.3 -- Model Architecture

[sec3.3_p1] After a synthetic dataset has been produced, the next step is to use the data for designing and optimizing a BNN to predict the distance of closest approach between the combination pair of Starlink satellites. The model uses a Gaussian prior distribution in which the Gaussian mean prediction is interpreted as the predicted distance of closest approach and the Gaussian standard deviation provides a quantification of the prediction uncertainty. Using the open-source Python package Hyperopt, the authors experimented with a variety of negative log likelihood (NLL) loss functions, loss function weights, and neural network architectures. They evaluated the different NLL loss functions and neural network architectures using common metrics, including MAPE and MSE, ultimately resulting in an architecture with a batch size of 256; three hidden layers with 243, 104, and 113 neurons for each, respectively; the Adam (adaptive moment estimation) optimization algorithm, a learning rate of approximately 1.608e-3, and a modified NLL loss function.

[Equation 5: The modified NLL loss function is computed as the sum over data points of log(sigma) times 0.5 times lambda times ((y - mu)/sigma) squared plus alpha times MAPE(y, mu), where mu is the mean prediction, sigma is the standard deviation, y is the true distance of closest approach, lambda is a weight for prediction accuracy, and alpha is the weight for the MAPE metric.]

[sec3.3_p2] The training data were scaled using minimum-maximum scaling, and dropout regularization was set at 20 percent to prevent overfitting and increase accuracy on the validation set. The algorithm underwent 100 training epochs. Of the nearly 2 million data points, 30 percent was allocated as a hold-out test dataset to evaluate the final performance of the algorithm and 10 percent as a validation dataset. The results below are how the model performed on the hold-out set. The algorithm predicted on the hold-out set 1,000 times, and the results are the means of both those predictions and standard deviations.

[Equation 6: The aggregate standard deviation is computed as the square root of the mean of all individual standard deviations squared plus the variance of predictions around the mean prediction.]

[sec3.3_p3] Figure 3.3 presents the accuracy of the model plotting the true values versus the predicted mean values. This figure shows how the predictions align with a perfect model significantly more when the true closest approach distance is large.

[Figure 3.3: BNN Distance of Closest-Approach Optimization. The plot shows the network is very capable of identifying object pairs that are far apart but less capable at smaller distances. A 25 km threshold and y=x perfect model line are indicated.]

[sec3.3_p4] In general, the algorithm is capable of predicting the distance of closest approach between a combination pair of Starlink satellites. The algorithm appears to be more accurate for large distances of closest approach and less accurate for small distances of closest approach. This is suspected to be due to at least two reasons: (1) There is significantly more data representation for the larger distances of closest approach, and (2) the number of features provided is limited. When binning the data according to the true distance of closest approach, the average MAPE is poor for the small distance of closest approach and approaches zero after 1,000 km. The first bin (0 to 25 km) then represents about 3.4 percent of all the training data, large relative to the proportion of other bins but small in comparison with the aggregate training dataset.

[Figure 3.4: MAPE Versus Distance of Closest Approach. The plot demonstrates that the BNN MAPE performance increases for larger true distances of closest approach despite the increased proportion of data points in this region.]

[Figure 3.5: Root MSE Versus Distance of Closest Approach. RMSE = root mean squared error.]

[sec3.3_p5] The BNN presented here can predict the distance of closest approach between a combination pair of Starlink satellites; however, the performance is inconsistent depending on the true distance of closest approach and in general performs worse when that distance is small. Nevertheless, the BNN mean prediction and uncertainty can then be leveraged by the postprocessing classifier to screen through combination pairs for potential conjunction events.

---

### sec3.4 -- Model Output and Postprocessing

[sec3.4_p1] The focus now turns to the postprocessing of the BNN output and how an operator may interpret the measure of uncertainty. The process of screening through every combination pair of RSOs to identify and flag combination pairings that violate a 25 km elliptical volume for near-earth objects is based on reporting criteria. It is important to acknowledge that, although the BNN is attempting to make accurate predictions of the distance of closest approach, what is more operationally relevant is the correct classification of combination pairs (whether the combination pair is flagged for further analysis by an SDA operator). This is the final step of the ESP workflow.

[sec3.4_p2] To prepare the BNN output for postprocessing, the test data used to validate the BNN were split into two groups: Group A includes data points with a true closest approach less than or equal to 25 km, and group B includes data points with a true closest approach greater than 25 km. A simple confidence metric was also formulated to represent the level of uncertainty the algorithm has in a given prediction; the confidence metric is defined as the prediction standard deviation divided by the mean prediction.

[sec3.4_p3] The criteria for flagging a combination pair for further analysis by an SDA operator are as follows: (1) The mean prediction violates the 25 km distance of closest approach, or (2) the confidence metric violates the confidence threshold assigned by an SDA operator. If either criterion is violated, the combination pair is flagged. The ESP performance for group A is then measured by the percentage of combination pairs flagged for further review. The performance of group B is measured by the percentage of combination pairs not flagged for further review. Incorrect classification of the combination pairs is not of equal consequence, as an incorrect classification of group A results in a missed conjunction event. The incorrect classification of group B results in additional manpower required to review nonthreats. This dynamic is important for the SDA operator to consider when assigning a confidence threshold.

[sec3.4_p4] [Table 3.1: Classification Matrix.] With a 10 percent confidence threshold, for Group A: 72.48% had correct predictions with low confidence (flagged), 0.00% had correct predictions with high confidence (flagged), 27.52% had incorrect predictions with low confidence (flagged), and 0.00% had incorrect predictions with high confidence (not flagged). For Group B: 21.74% had correct predictions with low confidence (flagged), 55.05% had correct predictions with high confidence (not flagged), 23.24% had incorrect predictions with low confidence (flagged), and 0.00% had incorrect predictions with high confidence (flagged). All of group A is correctly categorized because of low prediction confidence despite 27.52 percent of the mean predictions being larger than 25 km. For group B, 44.95 percent of combination pairs are incorrectly flagged for further analysis, equating to a 55.05 percent reduction in combination pairs that require further analysis by an SDA operator.

[Figure 3.6: Categorization of BNN Output for Group A (Left) and Group B (Right). The plots show the parameterization of the confidence metric variable and the resulting classification matrix. A confidence metric of zero indicates the operator is skeptical of all BNN predictions, and every combination pair is flagged. An infinite confidence metric is equivalent to an operator having absolute trust in the BNN prediction.]

[sec3.4_p5] The left panel of Figure 3.7 illustrates model performance as a function of uncertainty. If an SDA operator is willing to accept a 30 percent confidence metric, the algorithm will correctly flag 99.7 percent (true positive rate) of combination pairings in group A, near-perfect performance. This comes at the cost of misidentifying and flagging 35.5 percent (false positive rate) of group B. As a result, this confidence threshold filters out 55.6 percent of all possible RSO pairs, reducing the workload in subsequent steps. Contrast this with an infinite confidence metric meant to represent an SNN that performs significantly worse. The right panel depicts this information as a receiver operating curve (ROC). The area under ROC (AUROC)--the closer to 1.0, the better--indicates the overall strength of this classifier.

[sec3.4_p6] It is noteworthy to mention the computational efficiency inherent to this model because the BNN does not require an evaluation of the orbit propagation trajectory. After predicting on 599,985 unique combination pairs, 1,000 times, the average time the model took to generate predictions on the hold-out set was only 0.03418 seconds (evaluated on a NVIDIA T4 Tensor Core GPU). After linearly extrapolating the computational time to reflect a more expansive dataset representative of what may be currently required by SDA operators, the estimated time for predictive analysis is approximately 56.2 seconds.

[Figure 3.7: Performance Optimization. The left panel shows true positive rate and false positive rate versus confidence threshold. The right panel shows the ROC curve with AUROC value.]

---

### sec3.5 -- Implementation Challenges and Future Work

[sec3.5_p1] The authors first acknowledge the limitations in available training data and resources to optimize the BNN performance. Despite minimal hyperparameter tuning and a modest modification and optimization of the loss function, the workflow provides surprisingly robust categorizations. This may be a testament to the added value provided by the BNN uncertainty quantification in contrast to an SNN. A clear opportunity for improvement is continued refinement of the BNN algorithm and, subsequently, characterization of the performance optimization curve for the improved algorithm.

[sec3.5_p2] In the analysis presented, the initial orbital state parameter space is constrained to the Starlink constellation satellites for either RSO in the combination pair. The BNN is expected to perform worse for both predictions of closest approach and prediction uncertainty in regions of phase space where it is untrained. Although a large prediction uncertainty may signal to an SDA operator that the BNN is outside the region of expertise, the degree of correlation between prediction precision and accuracy would still require exploration. Expanding the initial orbital state parameter space would require more data than the authors were able to synthetically generate on the project timeline. Organizations such as the 18th SDS and the 19th SDS are expected to have historical data that could be leveraged for this purpose.

[sec3.5_p3] Another opportunity for increasing the capability of the BNN is to include a prediction of the TCA for flagged combination pairs. The value of this capability will become more relevant when used in combination with the second case study, on the orbital state propagator (OSP) tool, described in the next chapter.

---

### sec3.6 -- Summary

[sec3.6_p1] This chapter demonstrated feasibility of an AI/ML case study that flags RSO pairs for possible conjunctions. The ESP tool provides the operator with estimates of the distance of closest approach and an uncertainty, enabling decisionmaking based on the operator's risk acceptance level. The model is fast, and the estimated time to analyze all current RSO combination pairs in the unclassified space catalog is approximately 56.2 seconds. The tool is also accurate in classifying combination pairs of concern when accounting for both the distance of closest approach prediction and uncertainty. A tool similar to what has been demonstrated here trained on data in an operational environment may serve as a prescreen to reduce the number of RSO combination pairs fed into the computationally limited CAVENet for subsequent high-fidelity conjunction screening. It may also serve as a screening process for high-priority combination pairs identified outside the typical eight-hour conjunction process timelines.

---

## sec4 -- Orbital State Propagator Tool

[sec4_p1] The second case study is the OSP tool, designed to predict the future state and covariance from an initial RSO state and covariance. There are many instances within SDA processes in which the future state and covariance of a space object is required for analyses or task execution. Examples include (1) knowing where to point ground-station sensors for observations and (2) the calculation of Pc. Covariance calculations are currently predominantly performed through SP calculations that offer high-fidelity performance but are also time and resource intensive. The ability to quickly evaluate future states with minimal computational load without requiring access to the central repository of trajectories stored on CAVENet has implications for the ability to perform analysis at the tactical edge and for creating flexible timelines for covariance-related analysis not amenable to current processes.

[sec4_p2] This task aims to assess whether AI/ML can accurately replicate low-tolerance SP calculations. The opportunities for the orbital state propagator to provide operational value increase with better performance. For this reason, attention here is focused on producing a well performing BNN as opposed to designing a workflow that is integrated with SDA operators. Although it is challenging to establish minimum operational requirements for such a tool because of individual-level comfort with AI, close-enough rapid covariance propagation can accelerate many SDA tasks.

---

### sec4.1 -- Covariance Propagation Workflow Description

[sec4.1_p1] The workflow process begins with an initial state and covariance of a space object. The initial state and covariance are a probability distribution; the parameters associated with this distribution are input to the neural network, which in turn produces the output state and covariance of the object at a specified time in the future.

[sec4.1_p2] For this prototype, the BNN is trained to output an RSO's covariance at a fixed future time (time of flight = 1,000 seconds). Various statistical measures are used to gauge performance and assess how closely the BNN uncertainty matches the true covariance (as informed by Monte Carlo simulations of orbit propagation). However, operators often require knowledge of a space object's future covariances across a wide variety of time horizons. In a later section, the report also discusses (1) how a fixed time-of-flight model may be operationalized to provide covariance estimates across a range of flight and (2) the feasibility of variable time-of-flight models.

[Figure 4.1: OSP Tool Workflow. The workflow shows initial state and initial covariance as inputs to a neural network (NN), which produces final state, final covariance, aleatoric uncertainty, and epistemic uncertainty as outputs. NN = neural network; arb. = arbitrary.]

---

### sec4.2 -- Data Generation

[sec4.2_p1] Like the ESP case study, the authors lacked access to operationally relevant data and chose to generate a synthetic dataset for the OSP tool. The OSP tool consists of a neural network that ingests an initial RSO state and covariance and predicts the RSO future state and covariance. Generating a training dataset for this case study required an algorithm to propagate orbit trajectories and covariances in a way that is representative of how the process is performed by SDA operators in a system such as CAVENet. Because the authors did not have access to the specific algorithms implemented on CAVENet, it was challenging to compare and benchmark the case study performance against operational performance.

[sec4.2_p2] When generating a synthetic dataset for the OSP, the authors first referenced general perturbation orbit data provided by the U.S. government via CelesTrak in an OMM XML data format. To scope the development effort, the RSOs considered were constrained to Starlink satellites, which operate approximately between 540 and 570 km altitude, in inclinations that include 53.0, 70.0, and 97.6 degrees. The same assumptions from the previous chapter were maintained--that a Starlink-focused dataset would provide enough diversity in orbit to remain nontrivial and prevent overfitting.

[sec4.2_p3] Using the general perturbation data from CelesTrak, a random Starlink satellite was sampled and an ephemeris arbitrarily selected in a single orbital period of the satellite was assigned to act as the mean starting condition. A Monte Carlo sample of a distribution of initial conditions around the mean was then conducted to represent the covariance. The initial-state (position and velocity) distribution's standard deviation is spherical. The standard deviation for the positional measurement is 50 m and is 5 m per second for the velocity, which the authors think is representative of U.S. government ground sensor capabilities. The initial state could represent either a sensor observation or the product of the DC process, and the source of this initial state has implications for the accuracy and precision of the measurement.

[sec4.2_p4] The initial-state distribution was then propagated for a fixed time of flight to an end-state distribution. Consistent with the SP implementation at the 18th and 19th SDSs, an expansion of the state covariance was observed as the propagation time increases. Statistics such as mean and standard deviation of the end-state distribution were determined and provided as output of the numerical orbit propagation algorithm. As a simplification, only the diagonal elements of the output covariance matrix were extracted within the simulations. However, in future iterations, the process can be modified to extract all nine elements of the output covariance matrix.

[Figure 4.2: Increase in Covariance Versus Time of Flight.]

[sec4.2_p5] Similar to the synthetic data generated for the ESP tool, the authors leveraged the Cowell's method implementation in the Poliastro open-source Python library. The algorithm accounts for the Keplerian forces in addition to higher-order dynamical forces. The numerical algorithm used is the Dormand-Prince integration method of order 8(5,3), with a relative tolerance of 1e-8. The aleatoric uncertainties and the rate at which the covariance expands are specific to the numerical algorithm used; therefore, the methodology is reflective of current SDA processes, but there will be discrepancies between the training dataset and an operational dataset.

[sec4.2_p6] For the majority of experiments, the authors randomly sampled across the input parameter space to generate a training set of approximately 140,000 events and subsequently trained a variety of models on this set. However, because the training data for the models were generated via simulation, dynamic active-learning training protocols can also be leveraged.

[sec4.2_p7] An important parameter to consider was how many Monte Carlo samples are required to accurately represent the uncertainty for each set of input parameters. To identify an acceptable trade-off point, the authors monitored how the variance in the X, Y, and Z output positions changed as a function of Monte Carlo trial number for a set of 1,000 randomly chosen input states. Following standard practice, for each input state, the trial number at which the variances appeared to converge was extracted. Ultimately, 1,500 Monte Carlo trials per input were chosen to construct the training set, a trial number that was associated with convergence in approximately 80 percent of inputs. When building the training set, simulation runs were parallelized across eight computing cores, producing event data at a rate of approximately 1,500 events per hour.

[Figure 4.3: Monte Carlo Trial Convergence Versus Trial Number. CDF = cumulative distribution function; MC = Monte Carlo.]

---

### sec4.3 -- Model Architecture

[sec4.3_p1] The model inputs consist of the following sets of data arranged in a tabular format: an input 3D position vector, an input 3D position covariance matrix, an input 3D velocity vector, an input 3D velocity covariance matrix, a space object's cross-sectional area, a space object's drag coefficient, and, lastly, a time of flight (for fixed time-of-flight models, this input is excluded). The BNN is a five-layer feedforward network, constructed via the BliTZ Python package, that ingests these inputs and produces six outputs, representing an output 3D-position vector and an output 3D-position covariance.

[sec4.3_p2] The model possesses a twin architecture consisting of three fully connected layers, followed by two separate two-layer heads, one for covariance calculations and one for mean position calculations. The output of each head consists of three parameters that specify the 3D mean position and positional covariance of the final-state multivariate Gaussian distribution. Because the training dataset was built under the assumption that all input-output covariance matrices were diagonal, the same assumption was made with the BNN inputs and outputs.

[Figure 4.4: Notional Neural Network Model Architecture. The architecture shows inputs flowing through feature extraction layers, then splitting into a mean head and a covariance head, producing output 3D probability distribution parameters.]

[sec4.3_p3] The number of nodes within each layer was determined by leveraging the Hyperopt package to perform a hyperparameter search over the node number in each layer. Here, 300 nodes were chosen as an upper bound for the search space to keep the total model parameter number less than the total number of training data rows available (approximately 140,000). The learning rate of the Adam optimizer used for model training and the batch size were also optimized in this manner. After the above parameters had been optimized, training was carried out over 1,000 epochs, with a validation set included to monitor overfitting.

[sec4.3_p4] Once the model was trained, the BNN was evaluated on a hold-out test set by running N = 100 forward passes through the model for each input state. This resulted in 100 generally distinct output 3D probability distributions that were aggregated together to form a final output state.

[Equation 7: The aggregate positional mean is computed as (1/N) times the sum of all N BNN sample position vectors. The aggregate standard deviation is computed as (1/N) times the sum of individual standard deviations plus the squared positional offset of each sample from the aggregate mean, providing more-informative uncertainty estimates.]

[Figure 4.5: BNN Sample Output. For a given RSO input state, 100 output state predictions are generated using the BNN and then aggregated into a single distribution (left panel). This aggregated 3D distribution is compared against the numerical model output, labeled "SP Output," in the right panel.]

[sec4.3_p5] The same general training procedure was followed for a non-Bayesian SNN for comparison purposes, with the modification that no output aggregation was necessary because of the deterministic nature of inference within these models.

---

### sec4.4 -- Model Performance

[sec4.4_p1] By repeating the data-generation process, a hold-out test of approximately 100,000 events was assembled on which to evaluate both the SNN and BNN models.

[sec4.4_p2] [Table 4.1: BNN and SNN Comparative Performance.] For MAPE, both BNN and SNN produce low values (optimal is low). For covariance root MSE, BNN produces high values while SNN produces low values (optimal is low). For KL divergence, BNN produces low values while SNN produces high values (optimal is low). For training time, BNN requires high time while SNN requires low time (optimal is low). For probability of target position, BNN produces high values while SNN produces low values (optimal is high).

[Figure 4.6: BNN and SNN Proof-of-Concept Performance. The panels depict histograms for four metrics on 100,000 RSOs in the validation set. Panel A shows MAPE. Panel B shows normalized standard-deviation distributions. Panel C shows KL divergence. Panel D shows the probability (log scale) of the true mean in each of the predicted distributions.]

#### sec4.4.1 -- SNNs

[sec4.4.1_p1] SNNs produce MAPEs on the same order as BNNs (median value approximately 0.3 percent) and produce relatively lower covariance root MSEs; however, SNNs also produce high KL divergences. The produced covariances are generally too small relative to the positional error of the model. If the output probability distribution of the SNN is evaluated at the actual position of the final-state vector target data, it typically yields a probability lower than the probability from a corresponding BNN by much greater than 10 orders of magnitude. If operators were using such probabilities to guide risk calculations associated with conjunction events or other position-dependent scenarios, they would have a skewed understanding of the actual state of the probed space object. This poses challenges to implementing such models in scenarios in which position-based risk calculations are critical.

#### sec4.4.2 -- BNNs

[sec4.4.2_p1] The BNN-produced MAPE is on the same order as the SNN. However, in contrast to the SNN, the BNN produces a higher covariance root MSE but lower KL divergences. Rather than underapproximating the covariances, the BNN approach produces covariance values that are more consistent with their positional errors with respect to SP calculations. In most cases, if the output probability distribution of a BNN is evaluated at the actual position of the final-state vector's target data, the produced probabilities would be much higher than if calculated by the SNN--but still lower than if this probability were calculated using the target probability distributions themselves.

[sec4.4.2_p2] However, BNNs do generally produce positional standard deviations a factor of approximately two to three times higher than the target data, a feature that would need to be mitigated to achieve optimal operational performance. Besides the performance criteria listed above, BNNs possess additional features, such as out-of-distribution data detection, that are difficult to extract with an SNN and may offer additional operational advantages. On the other hand, a BNN contains roughly twice as many parameters as an SNN and is more computationally expensive to train and deploy. Over the data regime studied, the BNN required more than approximately ten times the training periods--meaning that additional engineering would likely be needed to scale the model to larger data regimes because of the increase in model complexity.

#### sec4.4.3 -- Performance Summary

[sec4.4.3_p1] The proof-of-concept model demonstrates that covariances can be propagated forward using a neural network; however, there is still much room for improvement. An idealized model would jointly satisfy all evaluation metrics mentioned above. This would result in a model whose final-state covariances and mean predictions both closely matched those of the underlying training data, which in turn would translate to near-zero KL divergences.

[sec4.4.3_p2] Over the data regime studied, the BNN generally offered a higher performance across the discussed performance criteria compared with SNN models. However, further research must be conducted to understand the upper performance bounds of both these methods in operational data regimes, as well as any trade-offs that may be encountered because of the BNN's comparatively longer training times. In addition, both of these performance bounds must be considered with respect to minimum operational requirements, which could not be established during this study. Once both the performance bounds of the considered AI models and minimum operational performance requirements have been characterized, an assessment can be made on whether AI models offer a comparative advantage for rapid covariance propagation over alternative techniques, such as low-tolerance SP calculations or SGP models.

---

### sec4.5 -- Additional BNN Features

[sec4.5_p1] Apart from the ability to produce more-realistic uncertainties, BNNs also offer other advantages that may make them operationally advantageous.

#### sec4.5.1 -- Out-of-Distribution Detection

[sec4.5.1_p1] Because of their inclusion of epistemic uncertainty, BNNs can express when they are ingesting data that are out of distribution from their training set, producing high-uncertainty estimates in such scenarios that could serve as an alert flag.

[sec4.5.1_p2] Operationally, this situation may arise if the BNN was fed inputs from a space object orbiting in an orbital regime outside the BNN training set. Since the model was not trained on orbits of this nature, it is unlikely to produce accurate results. Ideally, no model would be deployed outside its operating requirements; however, because of process complexities, it is possible that an operator would not always be aware of the input limitations of a black-box model. In such scenarios, operators risk receiving unknowingly inaccurate information. The high output errors that can be produced by a BNN in such scenarios can act as a flag for operators to reevaluate model results.

[sec4.5.1_p3] To demonstrate this feature, the optimized BNN model was evaluated both on one set of inputs from the Starlink constellation, which the model was trained on, and on a second set of inputs from the OneWeb constellation, which the model was not exposed to during training. The MAPE performance metric on the OneWeb constellation (approximately 20 percent) was much higher than the corresponding MAPE for the Starlink constellation (approximately 0.05 percent), as expected. However, the produced uncertainties were also generally higher for the OneWeb constellation, suggesting that such inputs might not be processed by the model accurately. Ideally, the error distributions between these two sets would be even more distinct, and such metrics can be considered in future optimizations.

[sec4.5.1_p4] Even within the constellation that a BNN is trained on, there may be certain regions of phase space in which the model has poor training-set coverage, especially considering the near-infinite possibilities of input states. Inputs evaluated from these regions are likely to lead to poorer-quality output states. Once again, a BNN may be able to indicate that it is operating in an out-of-distribution zone by producing large epistemic errors in these scenarios.

[sec4.5.1_p5] To evaluate this property, over the Starlink constellation evaluation set, the overall 3D uncertainty for each prediction was plotted against the positional MAPE. There is positive, yet weak, correlation between uncertainty and MAPE, indicating that the BNN generally produces higher uncertainties for the output states that it predicts less accurately. Over the experiments, the strength of this correlation did vary, and, in certain instances, no correlation could be observed for specific training epochs. Further optimization could be conducted if this property is advantageous or necessary for an operational use case.

[Figure 4.7: BNN Error Flagging, Relative Frequency (Left) and MAPE (Right). Out-of-distribution predictions have higher uncertainties (left). Within the Starlink dataset, there is a correlation between high uncertainties and higher percentage error (right). These two plots provide evidence that the BNN is able to (1) detect out-of-distribution scenarios and (2) flag in-distribution scenarios in which model performance is poor, both serving as potential alerts to operators.]

#### sec4.5.2 -- Active Learning

[sec4.5.2_p1] Given that the training data for the OSP are generated synthetically using a Monte Carlo simulation, one could theoretically continuously generate additional training data to augment the training process. However, generating data randomly across the input parameter space might not be as effective as focusing on regions of the parameter space in which the model is likely to benefit most. This type of dynamic adjustment of training sets is known as active learning. BNNs have been suggested as more-effective active learners than SNNs in certain scenarios. If integrated into the OSP tool training process, active learning may prevent model holes--regions of input parameter space in which model performance is subpar--from forming. Further, this process could be applied successively to millions of data points to continuously increase model quality.

[sec4.5.2_p2] As a proof of concept, the authors trained an active-learning BNN by executing the following pseudocode: For k in N, process k input states generated randomly across parameter space through SP simulations to generate k output states, then split into training set T and validation set V. If k is not 1, concatenate T with F from the previous step. Train the BNN for E epochs on T. Sort V by the KL divergence. Select the top H elements from V and generate F additional input states randomly perturbed from H by a specified percentage delta.

[sec4.5.2_p3] The training curve for a model built using the above procedure is presented with parameters K = 5,000, E = 300, H = 1,250, F = 1,250, and delta = 10 percent. The training and validation loss continue to decrease as a function of epoch number, indicating successful training. A grid plot of the KL divergence of the trained model on an evaluation set, binning across spatial dimensions, shows several areas of position space in which the model performs poorly at epoch 1, producing large KL divergences. At epoch 1,000, these regions of parameter space have lower KL divergences, indicating that the training process was successful at addressing these poor-performance zones.

[Figure 4.8: Active-Learning Training Curve.]

[Figure 4.9: Active-Learning Spatial Distribution of KL Divergences Versus Training Epoch. The plots show KL divergence across XY, XZ, and YZ spatial dimensions at training epoch 1 (top row) and epoch 1,000 (bottom row).]

[sec4.5.2_p4] Active learning is a well-studied field, and there are many ways in which this simple proof-of-concept procedure could be optimized, depending on operator needs. Because the SDA data infrastructure provides regular updates to state and covariance for all RSOs with consistent data formatting and labeling, active learning may be an attractive AI/ML feature when paired with SDA mission processes. In addition to having access to the abundance of RSO data, active learning has the advantage of reducing maintenance and resource cost, being more adaptable to the rapidly changing space environment, and being more scalable than the alternative.

---

### sec4.6 -- Operational Frameworks

[sec4.6_p1] The proof-of-concept model would need to be modified in several ways to fit operational use cases. To offer operational value, the OSP tool would likely need to provide covariance propagation across a wide range of time horizons (calculated in approximate days). Currently, the model propagates only at fixed time intervals of 1,000 seconds. Two frameworks are proposed to modify this functionality.

#### sec4.6.1 -- Recursive Propagation

[sec4.6.1_p1] The outputs of the model could be fed into the inputs of the model to extend predictions for longer time horizons. For example, ten successive 1,000-second propagation model calls could be daisy-chained together in this way to propagate a covariance forward 10,000 seconds.

[sec4.6.1_p2] There are a few challenges and drawbacks to this method. First, further research would need to be conducted on how model error propagates across recursive calls similar to numerical propagation techniques. The current model was trained on a limited set of input states. A daisy-chained model would need the ability to ingest a wide range of input states with differing covariances; adding such flexibility to a model may introduce error that would cause recursive propagation to produce covariances too large for operational use.

[sec4.6.1_p3] Second, recursive propagation offers only time resolutions that are a fixed multiple of the model propagation time step (in this case, 1,000 seconds). Model time steps could be scaled up or down to offer coarser or finer resolutions, respectively. If finer time steps are chosen, timing resolution will be increased, but more recursive calls would be needed to propagate over a given time horizon, increasing computational time and also potentially increasing model error. On the other hand, coarse time steps reduce computational time but offer reduced timing resolution.

[Figure 4.10: Notional Recursive Propagation. A recursive propagation of orbit trajectory may use a much coarser time step when compared with numerical propagation techniques. The trade-off for using larger or smaller time steps is presented.]

[sec4.6.1_p4] Lastly, although the proof-of-concept model produced only 3D positions and 3D position covariances as outputs, a recursive model would also need to output all of its inputs so that these quantities can be forecasted for future model calls. In this case, this would require also producing 3D velocity values and covariances.

[sec4.6.1_p5] With these challenges in mind, BNNs do offer advantages for recursive iteration compared with SNNs. In recursive propagation, a single severely miscalculated time step could cause massive deviations to the final produced output state. BNNs, with the ability to flag out-of-distribution inputs and to produce epistemic uncertainties, could produce warnings for operators in such situations. Hybrid BNN-SP systems could be developed that switch BNN propagation over to SP methods whenever epistemic uncertainty is raised above a certain threshold, allowing the speed advantages of AI/ML methods to be partially realized while also ensuring some degree of error tolerance.

#### sec4.6.2 -- Flexible Time-Step Models

[sec4.6.2_p1] Another option is to produce a model that can predict covariances over flexible time horizons, increasing the timing resolution of covariance estimation and potentially reducing the need for recursive calls. To provide a proof of concept, the authors constructed a BNN that predicted future covariance states across a range of times from 100 to 5,000 seconds by ingesting time of flight as an additional model input.

[sec4.6.2_p2] MAPE grows with increasing time of flight, but the model does offer relatively consistent performance across certain time-horizon zones. The model was trained on 90,000 examples, as compared with the approximately 140,000 for the fixed time-step model. The average MAPE for this model is approximately 4 percent, higher than the corresponding value of approximately 0.5 percent for the fixed time-step model, because of both the smaller training set size and the increased difficulty of the prediction task.

[Figure 4.11: Flexible Time-Step BNN Performance. TOF = time of flight.]

[sec4.6.2_p3] Research must be done on how the following concepts are related: (1) the time horizon over which a flexible time-step model can make predictions, (2) the upper performance bounds of such a model, and (3) the amount of training data needed to achieve stated performance bounds. Flexible time-step models with wide prediction time horizons offer opportunities for the faster covariance propagation and reduce the need for error-inducing recursive calls; however, the extent to which such models can be trained effectively is unknown.

[sec4.6.2_p4] Ultimately, discrete time-step models, flexible time-step models, and recursive model calls may be used in conjunction with one another rather than being considered separately. For example, one could deploy a coarse discrete time-step model to recursively propagate through the bulk of a desired time horizon. Next, an operator could deploy a fine-grain flexible time-step model to propagate the final state to a more specific time of interest within the time resolution of the coarse model.

---

### sec4.7 -- Implementation Challenges and Future Work

[sec4.7_p1] Although the authors are able to demonstrate that an OSP is feasible using a BNN algorithm, it is challenging to compare the case study performance (speed or accuracy) against operational performance. The authors are unaware of existing research that has characterized SP model performance. SP methods are currently the only algorithm available to SDA operators for propagating states and covariances in the HAC. The performance of SP methods is interdependent on three key components: (1) the relative or absolute error tolerances used, (2) the physical assumptions made by the algorithm, and (3) the computing hardware capabilities at SDA facilities, including those operated by the 18th and 19th SDSs. Despite the lack of a comparative study, BNNs are generally faster than the numerical methods they approximate and may exhibit "good enough" estimates to be useful for SDA. Further analysis should be done to benchmark the BNN prediction with real, operationally relevant data.

---

### sec4.8 -- Summary

[sec4.8_p1] In the second case study, the authors attempted to predict the future states and covariance of an RSO over various time horizons. Two algorithms were explored, BNNs and SNNs, and the performance of these algorithms was evaluated with different metrics. Both algorithms have relative strengths and weaknesses, but, over the explored data regime, BNNs offer higher performance over metrics most likely to be relevant to risk-based SDA decisionmaking. Ultimately, the upper performance bounds of both methods, as well as minimum operational requirements, must be better established before an assessment can be made of operational feasibility.

[sec4.8_p2] In addition, the report explored how other features of BNNs, such as out-of-distribution detection and active learning, may be leveraged to further optimize the prototype model. Lastly, it investigated how variable time-step models, recursive propagation, and discrete time-step models may each be leveraged to unlock a wider variety of operational use cases for the OSP tool.

---

## sec5 -- Conclusion

[sec5_p1] This report provided a detailed look at the technical approaches taken to demonstrate the feasibility of AI/ML within SDA. Managing uncertainty and predicting the future are the core mathematical principles underlying SDA, and the authors tested AI/ML algorithms, particularly BNNs, to determine their suitability for accomplishing these tasks.

[sec5_p2] Through the case studies presented in this report, the authors demonstrated the feasibility of using AI/ML applications to improve the CA process and predict the future states and covariance of RSOs. The first case study showed that an AI/ML application can flag RSO pairs for possible conjunctions, providing operators with estimates of the distance of closest approach and an uncertainty. In the second case study, the use of BNN and SNN was explored to predict the future states and covariance of RSOs over various time horizons. BNNs offer higher performance on metrics most likely to be relevant to risk-based SDA decisionmaking.

[sec5_p3] Neural networks are capable approximators for complex nonlinear functions. The report demonstrated that prediction and classification capabilities can be applied to SDA mission processes with SNNs and BNNs. The findings suggest that AI/ML applications can significantly improve the CA process and enhance SDA capabilities. Feasibility and pathways to incorporate these tools into existing SDA processes were demonstrated. The BNN models can be applied in a variety of ways, such as variable time-step models, recursive propagation, and discrete time-step models. However, there is significant work to be done before operationalizing these applications.

[sec5_p4] AI/ML tool developers should focus on tools that are compatible with the operational SDA architecture, data infrastructure, and processes. Given limited access to operational datasets, the authors designed a methodology that is representative of the SDA mission processes and tools used by such organizations as the 18th and 19th SDSs to produce model training sets. Although the training sets cannot be directly compared with those that would be produced by current operational systems, in general more-realistic training data can reduce model uncertainty and result in better predictions and classifications. Any operational AI/ML tool would also benefit from access to the operational SDA tools for clearer performance benchmarking.

[sec5_p5] Quantification of risk and uncertainty tolerance provides an operator or analyst with the capability to provide feedback to an AI/ML model by setting thresholds based on acceptable risk. In the first case study, developing an elliptical screening tool, the authors explored how an SDA operator or analyst might leverage the uncertainty quantification produced by a BNN. They characterized the performance of the postprocessing classifier based on the threshold of acceptable risk set by an operator or analyst. In doing so, significant variance was found for the classification of both true positives and false negatives. This performance optimization curve provides an operator with insight into the model discrimination performance versus optimal models, threshold selection, and trade-offs between operating conditions.

[sec5_p6] Quantification of risk and uncertainty tolerance can support improved performance of AI/ML tools focused on prediction and classification when compared with SNN approaches. In both case studies better prediction and classification performance was observed when using methods that account for uncertainty quantification. This is not a general statement, as there may be instances where the reverse is possible. BNNs are preferable to SNNs when uncertainty is a necessary output, when there are limited available training data, and when the detection of outlier predictions is important to flag for an operator.

[sec5_p7] Active learning may be an attractive AI/ML feature when paired with SDA mission processes. The SDA mission processes and supporting data infrastructure provide regular updates to state and covariance for all RSOs, with consistent data formatting and labeling. Provided with this abundance of real-time RSO data, active-learning protocols may offer such advantages as reducing maintenance and resource costs, providing adaptability to rapidly changing space environments, and offering more scalability than alternative model training techniques.

[sec5_p8] The development of AI/ML tools requires significant investment in ensuring high-quality training data. The Office of the Chief Scientist of the U.S. Air Force, with input from SDA operators, should consider the availability of such data for AI/ML investment. The availability of high-quality training data is another enabler for AI/ML impact. To develop effective AI/ML applications for SDA tasks, it is essential to start collecting high-quality input-output data from analysis tasks to curate a database that can train future systems. This enables AI/ML to learn from best analyst practices and slowly automate more-complex tasks as the database grows. Given the evolving nature of the space environment, this effort will need to be continuous.

[sec5_p9] AI/ML tool developers should consider SDA processes and limitations that have implications for tool design, including the selection of algorithms, metrics of performance, and benchmark requirements. Since AI/ML tools are only as good as the rules they are given, more clarification is necessary about what these rules should be based on in terms of mission effectiveness. Continued support for these efforts would enable higher impact from AI/ML tools designed to support sensor tasking and prioritization and would avoid developing AI/ML solutions that operators cannot implement and use.

[sec5_p10] DoD should continue to consider the value of uncertainty quantification methodologies when developing and deploying operational AI/ML tools. Uncertainty quantification has the potential to improve decisionmaking, risk management, model selection and optimization, and robustness. However, using these algorithms may be challenging because of the complexities of these models and their computational overhead requirements. AI/ML practitioners within DoD at large should be knowledgeable of these trade-offs when considering the development of AI/ML tools.
