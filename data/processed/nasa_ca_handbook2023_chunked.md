# nasa_ca_handbook2023 -- NASA Spacecraft Conjunction Assessment and Collision Avoidance Best Practices Handbook (Rev 1)

**Authors:** NASA Office of the Chief Engineer
**Venue:** NASA Special Publication (SP-20230002470 Rev 1) (2023)
**DOI:** https://ntrs.nasa.gov/citations/20230002470

---

## sec0 -- Preface

[sec0_p1] Since Explorer 1 was launched on January 31, 1958, the United States (U.S.) has reaped the benefits of space exploration. New markets and new technologies have spurred the economy and changed lives in many ways across the national security, civil, and commercial sectors. Space technologies and space-based capabilities now provide global communications, navigation and timing, weather forecasting, and more.

[sec0_p2] Space exploration also presents challenges that impact not only the U.S. but also its allies and other partners. A significant increase in the volume and diversity of activity in space means that it is becoming increasingly congested. Emerging commercial ventures such as satellite servicing, in-space manufacturing, and tourism as well as new technologies enabling small satellites and large constellations of satellites present serious challenges for safely and responsibly using space in a stable, sustainable manner.

[sec0_p3] To meet these challenges, the U.S. seeks to improve global awareness of activity in space by publicly sharing flight safety-related information and by coordinating its own on-orbit activity in a safer, more responsible manner. It seeks to bolster stability and reduce current and future operational on-orbit risks so that space is sustained for future generations. To this end, new and better Space Situational Awareness (SSA) capabilities are needed to keep pace with the increased congestion, and the U.S. seeks to create a dynamic environment that encourages and rewards commercial providers who improve these capabilities.

[sec0_p4] The overall purpose of the National Aeronautics and Space Administration (NASA) Spacecraft Conjunction Assessment and Collision Avoidance Best Practices Handbook is to equip owners/operators (O/Os) with the best practices they need to conduct their operations safely. The Handbook reflects the goal of Space Policy Directive-3, the National Space Traffic Management Policy (SPD-3), to develop safety standards and best practices that consider "maneuverability, tracking, reliability, and disposal." It reflects how NASA currently operates in space, which evolves over time. It is organized around a spacecraft's life cycle, with consideration given to important topics such as spacecraft and constellation design; spacecraft "trackability;" pre-launch preparation and early launch activities; on-orbit collision avoidance; and automated trajectory guidance and maneuvering. The Handbook encourages the use of commercially available SSA data and information, and for large constellations, it encourages mitigating light pollution to ground-based astronomy. It focuses on current NASA best practices and will be updated as new approaches for conjunction assessment and on-orbit operations are developed. It provides guidance for the requirements in NASA Procedural Requirements (NPR) 8079.1, NASA Spacecraft Conjunction Analysis and Collision Avoidance for Space Environment Protection.

[sec0_p5] A parallel document may be developed to highlight additional practices as the U.S. Department of Commerce develops the Open Architecture Data Repository (OADR) mandated by SPD-3. Other commercial services may be offered to supplement or replace the government services currently offered.

[sec0_p6] The approaches outlined in this document are offered to spacecraft Owners/Operators (O/Os) as an example of responsible practices to consider for lowering collision risks and operating safely in space in a stable and sustainable manner. Entities offering, or intending to offer, SSA or conjunction assessment services should consider the information in this handbook from the perspective of augmenting or improving upon existing capabilities as the entire space industry benefits from advancing these capabilities. In the near term, raw observation data can be used to improve close approach predictions. Longer-term improvements might be found in enhancing notifications and data sharing, developing new models, and enabling increased automation.

[sec0_p7] NASA continuously examines and actively updates its best practices for conjunction assessment as the industry undergoes rapid evolution. Large constellations of satellites, for example, comprise a new and evolving paradigm for which NASA is developing in-house expertise. NASA seeks input from the community to improve the content presented in this document.

[sec0_p8] For space operations regulated by other U.S. agencies such as the Department of Commerce, the U.S. Federal Aviation Administration (FAA), and the U.S. Federal Communications Commission (FCC), NASA defers to those agencies. As part of interagency consultations and to contribute to safe and sustainable space operations, NASA partners such as the FAA and FCC request NASA review of license, payload, and/or policy applications made by commercial space operators to U.S. Government regulatory agencies. In addition to the information required by those regulatory agencies, NASA has prepared examples of information for various types of missions that is valuable in expediting NASA's review.

[sec0_p9] This document was developed in close collaboration with the U.S. Space Command (USSPACECOM), one of NASA's closest interagency partners in ensuring safe operations in space, as part of the NASA/U.S. Department of Defense (DOD) Interagency Working Group for Large Constellation Conjunction Assessment Best Practices. Special thanks are due to working group members Mr. Jeff Braxton of USSPACECOM's Strategy, Plans, and Policy Directorate, and Ms. Diana McKissock and Ms. Cynthia Wilson of the U.S. Space Force's 18th Space Defense Squadron (18 SDS).

---

## sec1 -- Introduction

[sec1_p1] This document is intended to provide NASA spacecraft program and project managers, generically referred to in this handbook as O/Os, with more details on implementing requirements in NPR 8079.1 and to provide non-NASA O/Os with a reference document describing existing NASA conjunction assessment and collision avoidance practices.

[sec1_p2] Within NASA, space flight missions are typically implemented through a program/project office structure. For example, a human exploration program might involve multiple missions (i.e., discrete launches) to a common goal. In this document, the term "mission" means the end-to-end activity that results in the creation, launch, operation, and eventual disposal of a spacecraft.

[sec1_p3] This document provides detailed information on spacecraft conjunction assessment and collision avoidance topics. Best practices for each topic are described and justified throughout this document by mission phase. A summary list of these best practices without the supporting explanatory text is provided in appendices C and D. Appendix C is a complete listing of all the best practices so that non-NASA mission managers can review them to determine which are useful for their mission. The mission manager would then need to identify the person or group responsible for performing the best practice and arrange for that effort. For NASA missions, Appendix D lists the best practices and the party responsible for performing them.

[sec1_p4] Different organizations use the term "conjunction assessment" in different ways, but NASA defines a 3-step process: (1) Conjunction assessment (otherwise referred to as "screening") -- The process of comparing trajectory data from the "primary object" (the "protected asset" in NPR 8079.1) against the trajectories of the objects in the applicable database and predicts when a close approach will occur within a chosen protective volume placed about the asset. (2) Conjunction risk assessment -- The process of determining the likelihood of two space objects colliding and the expected consequence if they collide in terms of lost spacecraft and expected debris production. (3) Conjunction mitigation -- An action taken to remediate conjunction risk including a propulsive maneuver, an attitude adjustment (e.g., for differential drag or to minimize frontal area), or provision of ephemeris data to the secondary O/O to enable that spacecraft to plan and execute an avoidance maneuver.

[sec1_p5] For Earth-orbiting objects, screening is performed against the space object catalog maintained by the United States Space Command (USSPACECOM) to predict when a close approach will occur within a volume of space called a "safety volume" placed about the asset. This catalog includes information about international and commercial operational spacecraft as well as all trackable debris. For non-Earth-orbiting objects, the ephemeris of the primary (protected) asset is only screened against other provided ephemerides.

[sec1_p6] This document compiles best practices based on the U.S. Government process that exists today where conjunction assessment screening is performed by USSPACECOM/18 SDS and 19 SDS. The risk assessment process is necessary because the orbit solutions of the catalog objects have varying accuracies depending on factors described in this document. Currently, O/Os need to perform this function for themselves (or hire a third party) because 18 SDS and 19 SDS are not tasked to perform this necessary risk assessment function. In the future, the Department of Commerce may offer risk assessment services as it takes on a space traffic management role, and broader commercial services will be available.

[sec1_p7] Some entities use Two-Line Elements (TLEs) to perform conjunction assessment. This practice is not recommended because the TLE accuracy is not sufficient to perform the necessary conjunction assessment calculations.

[sec1_p8] The terms "satellite" and "spacecraft" are interchangeable in this document. The word "object" means any discretely identifiable debris or other cataloged item in addition to satellites and spacecraft.

[sec1_p9] The term "large constellation" is defined loosely in this document. The U.S. Government Orbital Debris Mitigation Standard Practices (ODMSP) defines "large constellation" as containing 100 or more spacecraft. However, because constellations having as few as 10-20 spacecraft can experience greater conjunction risk, the O/O of any constellation of spacecraft is asked to consider the intent of the best practices in this document and implement all of them to the degree possible.

---

## sec2 -- Roles and Responsibilities

[sec2_p1] This section provides a brief introduction to the organizations referenced in this document.

[sec2_p2] The U.S. Space Command (USSPACECOM), one of the combatant commands within DOD, is responsible for U.S. military space operations. For the purposes of this document, it is the entity responsible for establishing SSA sharing agreements with domestic and international entities, both governmental and private. It establishes guidance and direction governing execution of the congressionally mandated SSA sharing program on behalf of the U.S. Secretary of Defense. It also oversees execution of the SSA sharing program as well as day-to-day operations of the USSPACECOM SSA and space flight safety support.

[sec2_p3] The U.S. Space Force's 18th and 19th Space Defense Squadron (18 SDS and 19 SDS) maintain the U.S. catalog of space objects and provide 24/7 space flight safety support on behalf of USSPACECOM. 18 SDS conducts advanced analysis, sensor optimization, human space flight support, reentry/break-up assessment, and launch analysis; 19 SDS conducts conjunction assessment and launch conjunction assessment.

[sec2_p4] The NASA Human Space Flight Operations Directorate (FOD) of the Johnson Space Center (JSC), through the console positions Trajectory Operations Officer (TOPO) and Flight Dynamics Officer (FDO), provides conjunction risk analysis support to the space flight missions that fall under NASA human space flight. The International Space Station (ISS) and vehicles visiting the ISS receive conjunction risk assessment support from the TOPO, while Artemis space flight missions receive conjunction risk assessment support from their assigned FDO. The TOPO group is the sole liaison to USSPACECOM and the U.S. Space Force for matters related to trajectory maintenance and the orbital safety of human space flight assets. However, for missions deployed from ISS or visiting vehicles, JSC FOD certifies that the mission has a conjunction assessment process as outlined in jettison policies but does not provide direct conjunction risk assessment support.

[sec2_p5] The NASA Conjunction Assessment Risk Analysis (CARA) Program located at the Goddard Space Flight Center (GSFC) provides conjunction analysis and risk assessment services for all NASA spacecraft not affiliated with human space flight. CARA is responsible for protecting the orbital environment from collision between NASA non-human space flight missions and other tracked and cataloged on-orbit objects. CARA is responsible for routinely collecting predicted orbital information from NASA spacecraft operators, passing it to NASA Orbital Safety Analysts (OSAs) for screening against the space object catalog, analyzing the screening results to determine the risk posed by predicted close approaches, and working with NASA spacecraft operators to determine an appropriate mitigation strategy for the risks posed by close approaches. CARA is the sole entity with authority to submit Orbital Data Requests (ODRs) to DOD on behalf of NASA non-human space flight entities in accordance with NPR 8715.6, NASA Procedural Requirements for Limiting Orbital Debris and Evaluating the Meteoroid and Orbital Debris Environments. CARA is the designated point of contact between NASA and USSPACECOM and the U.S. Space Force for matters related to trajectory maintenance and orbital safety of non-human space flight assets.

---

## sec3 -- Conjunction Assessment: Past, Present, and Future

[sec3_p1] This section provides a brief history of the conjunction assessment function, including an overview of who the actors are and why the activities are arranged as they are. It explains why the conjunction assessment and related safety-of-flight services currently available exist in their present forms and how they are evolving to accommodate new requirements.

---

### sec3.1 -- Origins

[sec3.1_p1] The present conjunction assessment process, which predicts close approaches between space objects, began in support of the NASA Space Shuttle Program return-to-flight effort after the Challenger accident in 1986. Drawing on the DOD's ability to track space objects for missile defense and SSA, the two U.S. Government entities partnered to expand the capability to offer a methodology that would protect humans in space.

[sec3.1_p2] What most people consider the "space catalog" is a database of TLE sets that permits a medium-fidelity propagation and orbit prediction of all tracked objects larger than approximately 10 cm in Low Earth Orbit (LEO) using Simplified General Perturbation Theory #4 (SGP4). The satellite catalog was originally designed not for conjunction assessment but for generalized space safety, predicting the location of objects with sufficient accuracy for sensors in the network to reacquire those objects, thereby maintaining custody for purposes such as ensuring that intercontinental ballistic missile threats could be differentiated from satellites in orbit. The catalog was then, and is today, maintained using the radars, telescopes, and other sensors comprising the Space Surveillance Network (SSN). In recent years, however, USSPACECOM has been working to augment the existing network by incorporating observations from international and commercial entities.

[sec3.1_p3] For ISS conjunction assessment to better protect humans in space, NASA collaborated with the U.S. Air Force to develop an improvement to the TLE catalog that would be accurate enough to compute a Probability of Collision (Pc) between two objects. To compute Pc, orbit determination covariance data were needed. Since the general perturbations theory used to maintain the TLE catalog (SGP4) did not produce a usable covariance, the current Special Perturbations (SP) Space Object Catalog was developed. 18 SDS, the U.S. Space Force unit that currently maintains the catalog at Vandenberg Space Force Base (VSFB) on behalf of USSPACECOM, and 19 SDS at Dahlgren Naval Base, Virginia have greatly expanded the process of screening protected assets since that time (especially since the Iridium 33/COSMOS 2251 collision in 2009) to include all operational assets currently on orbit.

[sec3.1_p4] However, since the process was developed 50 years ago and was grown piecemeal to meet existing needs, it contains certain oddities of evolution that may not be anticipated by O/Os who are accustomed to using current technology and methods. Understanding how the 18 and 19 SDS process works is key to using it properly to protect on-orbit assets and the space environment.

---

### sec3.2 -- USSPACECOM Conjunction Assessment Process

[sec3.2_p1] The full conjunction assessment screening process used by USSPACECOM with all accompanying details is documented in the Spaceflight Safety Handbook for Satellite Operators (the Spaceflight Safety Handbook), which can be obtained along with other helpful information from the USSPACECOM website space-track.org. This website is the principal way USSPACECOM communicates and exchanges conjunction assessment-related information with O/Os. All non-NASA spacecraft O/Os should familiarize themselves with the Spaceflight Safety Handbook as well as other website contents and offerings. NASA spacecraft O/Os do not need to be familiar with the Spaceflight Safety Handbook since CARA or JSC FOD will perform all actions on their behalf.

[sec3.2_p2] Commercial conjunction assessment providers, often using space catalogs assembled from private sector space tracking assets, are now an available resource for O/Os who wish to obtain conjunction assessment services. NASA encourages the use of validated commercial conjunction assessment services when they can augment the existing USSPACECOM capabilities. NASA recommends that all O/Os avail themselves of the USSPACECOM conjunction assessment service at a minimum as an adjunct to a commercially procured service.

[sec3.2_p3] To address this topic, the following specific practices are recommended: (A) Obtain a space-track.org account. (B) Become familiar with the Spaceflight Safety Handbook contents. (C) Use USSPACECOM conjunction assessment service as a minimum for screenings, even if additional commercial data or services are used.

[sec3.2_p4] The Space Surveillance Network (SSN), a global sensor network made up of optical telescopes and radars, is used to collect the tracking observations that are processed to maintain the High Accuracy Catalog (HAC) of on-orbit space objects.

[Figure 3-1: Locations and Types of SSN Sensors and Nodes]

[sec3.2_p5] Because most sensors have other purposes in addition to collecting data for SSA, tracking data cannot be simply requested and obtained on demand. Objects are placed in tasking categories, and a list of objects to attempt to track by category is sent to each sensor, which then sorts the list to determine which requests are feasible given the sensor's calculation of the probability of detection of the requested satellite and competing activities. Priority is given to military operational needs and special needs within conjunction assessment such as human space flight. If a USSPACECOM conjunction assessment operator determines that there is insufficient tracking data available to create a good orbit determination solution for an object, the operator may attempt to obtain additional data by increasing the priority of the object or by tasking additional sensors to track it. Sensors may not be able to track an object due to geometric and power constraints, weather, equipment outages, and other exigencies.

[sec3.2_p6] The USSPACECOM process is different from a commercial process in which a vendor is compensated directly for the tracking of an object with the flexibility of being able to obtain additional data if more funds are available to support the request. Because of the time required to collect additional data on an object of concern using the USSPACECOM process, it is recommended that O/Os provide a significant length of ephemeris prediction (including planned maneuvers) to USSPACECOM for screening. This screening duration allows time for USSPACECOM conjunction assessment operators to increase tasking for identified secondary objects with insufficient orbit determination solutions, and thus to enable the best Pc to be computed for use in conjunction risk assessment decisions.

[sec3.2_p7] To address this topic, the following specific practice is recommended: (A) Provide seven (7) days of predicted ephemeris (including maneuvers) to USSPACECOM for screening for LEO spacecraft, and provide 14 days for other Earth orbits; e.g., High Earth Orbit (HEO)/Geosynchronous Orbit (GEO).

[sec3.2_p8] Personnel at the USSPACECOM facility at Dahlgren Naval Base currently perform screenings of protected assets against the entire HAC once per day at a minimum and up to three times per day every day. Screenings are performed using both the ephemeris data that O/Os share (which include planned maneuvers for the asset) as well as the SSN-derived orbit determination solution for the asset (which does not consider predicted maneuvers).

[sec3.2_p9] Results from the screenings, in the form of Conjunction Data Messages (CDMs), are posted to space-track.org for customers with accounts. Entities without accounts who have provided an email address to USSPACECOM may receive basic prediction information by email if an emergency is detected.

[sec3.2_p10] Special/off-cycle screenings can be requested per the guidance in the Spaceflight Safety Handbook. It is important for safety of flight for all spacecraft operators to ensure that there are no large latencies or gaps in the process as uncertainties in the propagated orbit continue to grow over time. For instance, submitting an ephemeris for screening today at noon that has an epoch of today at 0 hours means that the data is already 12 hours old. If it then takes 8 hours to screen that data and another 2 hours to analyze the results, the data will be very old indeed. Ensure that the data screened is for a future time and is analyzed and acted upon before it becomes stale.

[sec3.2_p11] To address this topic, the following specific practices are recommended: (A) Provide at least one ephemeris per day to be screened and three ephemerides per day in the lower-drag LEO regime (perigee height less than 500 km). (B) Determine whether the O/O's process for obtaining screening results and performing conjunction risk assessment aligns with the timeline of the USSPACECOM process. If the timelines do not align in such a way as to enable timely and efficient screening support, pursue a rearrangement of the O/O's process to minimize data latency and optimize screening efficiency.

[sec3.2_p12] To facilitate communication between operators to mitigate a close approach, USSPACECOM maintains a list of contacts for each asset. Maintaining this list of contacts is critical so that someone can always be reached. Data sharing is further facilitated through the use of standard formats and coordinate frames.

[sec3.2_p13] To address this topic, the following specific practices are recommended: (A) Populate and maintain the point of contact section on space-track.org with your operations contact data. Be sure that the operations contact can be reached 24/7 due to time zone differences between operators and the immediate nature of certain conjunction assessment emergencies. (B) Use standard ephemeris, CDM, and maneuver notification formats defined by the Consultative Committee for Space Data Systems (CCSDS).

[sec3.2_p14] USSPACECOM/J535, SSA Data Sharing Branch, negotiates SSA sharing agreements, which establish the parameters within which data will be exchanged by the signing parties to facilitate ongoing cooperation and advance space flight safety. These agreements are useful because they substantially expand the types of data products that can be received over the default products provided to entities without agreements. Any member of the space community, including satellite operators, launching agencies, commercial service providers, and research/academic institutions, that wishes more and more frequent SSA products than are provided through a generic space-track.org account should contact USSPACECOM. U.S. Government organizations and their contractors have implied agreements and do not need to pursue a separate formal arrangement with USSPACECOM.

---

### sec3.3 -- NASA Partnership with USSPACECOM

[sec3.3_p1] Since the development of the HAC and its use in routine conjunction assessment screenings, NASA has partnered with USSPACECOM and its predecessor organizations. That partnership currently includes special, dedicated conjunction assessment screening support. The human space flight program uses U.S. Air Force civilians for conjunction assessment screening support, while NASA's non-human space flight missions are supported by NASA contractors called Orbital Safety Analysts (OSAs) who work on the operations floor in the VSFB operations center. While both groups do essentially the same work with the same input data, NASA OSAs focus specifically on NASA needs and can write scripts that are used to tailor the output data for use by CARA.

[sec3.3_p2] Conjunction assessment is a 3-step process: (1) The first step is conjunction assessment "screening," which involves computing the predicted close approaches between the protected asset and the catalog of space objects. This is the step performed at the USSPACECOM facility for mission customers, and screening results are provided that describe predicted close approaches. Both the USSPACECOM conjunction assessment cell and NASA OSAs perform this function. (2) The second step is conjunction "risk assessment," in which the screening results are analyzed to determine the level of risk posed by each predicted close approach and to determine whether the predicted close approaches warrant additional investigation and, ultimately, mitigation. This step is critical because not all predicted close approaches require mitigation, and often close approaches require analysis to determine what action is warranted. This step is not performed by USSPACECOM as it is a responsibility allocated to O/Os or organizations that perform this activity on the O/O's behalf. NASA JSC FOD performs this function for human space flight program assets, and NASA CARA performs this function for all other NASA spacecraft. (3) The third step, if required, is conjunction "mitigation," in which the O/O plans, and perhaps executes, a collision avoidance maneuver or other mitigation solution to reduce the collision risk to an acceptable level. This activity is allocated to the individual O/Os although NASA CARA does provide some basic support tools to aid in the initial choice of mitigation actions for its mission customers.

[sec3.3_p3] Because risk assessment requires different products and yields different analyses than screening, it is important to consider the tools and processes required to assess risk properly and adequately. For large constellations, the cadence of conjunctions and the consequence of a collision for the proper operation of the constellation are both heightened.

[sec3.3_p4] To address this topic, the following specific practices are recommended: (A) Develop a robust safety-of-flight process that includes both conjunction assessment screening and risk assessment to inform close approach mitigation decisions. (B) Large constellation operators should work with USSPACECOM pre-launch to determine if variations from the standard approach are necessary and, if so, to define a tailored screening process. (C) Large constellation operators should consider working with NASA to define a risk assessment process. NASA is an experienced representative operator. Including NASA in discussions regarding establishing a conjunction assessment process will ensure that the process will work with most operators for risk assessment purposes.

---

### sec3.4 -- Conjunction Assessment in Cislunar Space

[sec3.4_p1] Significant investments by the U.S., China, other nations, and commercial companies in missions to cislunar space between Earth and the Moon have highlighted the need for proper measures to ensure the safety of the space environment in that regime. While cislunar space will remain "big" for some time yet, popular locations and transit trajectories are likely to experience rapid growth, requiring effective conjunction risk analysis to maintain safety. Implementing appropriate measures for spacecraft will ensure readiness to perform effective conjunction risk assessment as cislunar space traffic increases.

[sec3.4_p2] In November 2022, the White House National Science & Technology Policy Council released the National Cislunar Science & Technology Strategy, an interagency strategy to guide developments across a spectrum of emerging areas in the cislunar "ecosystem." Several objectives espoused in this document pertain to Space Situational Awareness (SSA) and space safety, including the following: develop technical foundations of best practices for safe cislunar space flight operations; develop an integrated cislunar object catalog; develop procedures for publicly sharing cislunar SSA data as well as navigation and space flight safety support in cislunar space; and ensure that capabilities for U.S. Government cislunar operations are scalable and interoperable with systems operated by private and international actors.

[sec3.4_p3] For the purpose of outlining best practices for conjunction risk assessment, key goals center on sharing data and ensuring interoperability. These goals can be achieved through the delivery of standardized file formats and content, along with the use of consistent trajectory models and coordinate frame definitions.

[sec3.4_p4] The use of consistent trajectory models and coordinate frame definitions is best realized by conforming to the recommendations in the recently released NASA Technical Publication "Astrodynamics Convention and Modeling Reference for Lunar, Cislunar, and Libration Point Orbits" (Folta et al. 2022). This document details coordinate and time systems, numerical integration techniques, and trajectory modeling of three-body systems at different levels of fidelity appropriate for mission design and navigation of spacecraft well beyond geosynchronous orbit (GEO). A cislunar conjunction risk assessment best practice is to conform to the astrodynamics conventions and modeling recommendations of this NASA technical publication to enforce consistency across ephemerides from different sources, thereby enabling more accurate conjunction assessments.

[sec3.4_p5] The goal of standardizing file formats and content has been addressed by DOD, NASA, and academia through a Cis-Lunar SSA Technical Steering Group led by the National Geospatial-Intelligence Agency (NGA). The Steering Group has produced a draft document outlining an enhanced message set that enables communication of space object catalog and observational data for the cislunar regime (Butt 2021). The recommended set covers object state and covariance information in the catalog content message and sensor observations in the tracking data message. These formats overcome shortcomings of the current SSA message set that prohibits accurate expression of cislunar data and also update features of the current set that have not evolved with advances in SSA since their inception in the 1960s. A cislunar conjunction risk assessment best practice is to conform to these recommendations when developing SSA architectures to support cislunar missions.

[sec3.4_p6] Sharing of standardized ephemeris and covariance data is the intent of the current practice of Earth orbiter owner/operators (O/Os) delivering files to conjunction screening providers such as the U.S. Space Force 18th Space Defense Squadron (18 SDS), 19th Space Defense Squadron (19 SDS), or CARA for use in computing a probability of collision (Pc) during encounters. However, cislunar conjunction assessment currently has no catalog of cislunar objects with independent trajectory and covariance solutions and so requires a different screening process.

[sec3.4_p7] NASA currently performs conjunction assessment around Mars, the Moon, and Sun/Earth libration points through the Multimission Automated Deepspace Conjunction Assessment Process (MADCAP) based at the Jet Propulsion Laboratory (JPL). Spacecraft operators, including for non-NASA and even non-partner international spacecraft, deliver ephemerides to the NASA Deep Space Network (DSN) Service Preparation Subsystem (SPS) portal, through which MADCAP can access them. MADCAP assesses conjunction risk based on Pc if a covariance is provided, or (if necessary) via typical radial and timing prediction uncertainties provided by the respective mission navigation teams. These uncertainties are fit to polynomials in time and mapped to the relative node of the two orbits to assess the risk of collision at the crossing. However, as described in Tarzi et al. 2022, MADCAP has implemented the capability to compute two-dimensional Pc (2D-Pc) when given ephemerides with covariance. When potentially risky conjunctions are discovered, notifications and conjunction event data are forwarded to both satellites' operations teams, who then are expected to work with each other to determine if a mitigation action is necessary and, if so, execute it. JPL's MADCAP Team will facilitate the decision process if the operations teams cannot agree on a course of action.

[sec3.4_p8] Regarding the lack of a centralized catalog of cislunar objects, that is currently in work by 19 SDS. In fact, many cislunar conjunction assessment capabilities are evolving with the potential for significant advances in deep space tracking, trajectory modeling, and ephemeris sharing interfaces. Because many of these advances are still in development, the baseline for conjunction risk assessment must rely on ephemeris and covariance produced by the O/O rather than catalog screening.

[sec3.4_p9] As such, the O/O should deliver a CCSDS Orbit Ephemeris Message (OEM) formatted ephemeris with covariance to both space-track.org and to the DSN SPS portal. This will allow access to the trajectory and its uncertainty for screening by both 19 SDS (through SpaceTrack) and MADCAP (through SPS). The CCSDS format is the current industry standard, aligning with the standardization goal outlined above. Because a non-cooperative orbit solution for cislunar spacecraft is not currently available, inclusion of the covariance in the O/O ephemeris file is essential to allow computation of the probability of collision. Establishing the best practice of O/Os providing accurate trajectory and covariance for their spacecraft in cislunar space to current ephemeris interfaces sets the stage for effective conjunction screenings as operational advances continue to be made in the cislunar regime.

[sec3.4_p10] To address this topic, the following specific practices are recommended: (A) Owner/operators should conform to recommendations in the NASA Technical Publication "Astrodynamics Convention and Modeling Reference for Lunar, Cislunar, and Libration Point Orbits." (B) Cislunar mission support entities (sensors, catalog maintainers, etc.) should conform to the enhanced SSA message set (Butt 2021) when creating cislunar SSA architectures. (C) Owner/operators should maintain accurate orbit determination solutions and predicted covariance for objects in cislunar space. (D) Owner/operators should deliver predicted ephemeris with covariance in the CCSDS format to space-track.org and to the DSN SPS portal.

---

## sec4 -- Spacecraft and Constellation Design

[sec4_p1] Safety of flight is an integral aspect of satellite space operations and should be considered as part of design decisions to enable cost- and mission-effective solutions so that potential impacts to other operators in the space environment can be avoided. Specific design areas for which the consideration of on-orbit safety issues is appropriate include: orbit selection, spacecraft ascent and disposal activities, sensor trackability of the spacecraft, spacecraft reliability, capabilities for ephemeris generation, and capabilities for risk assessment and mitigation. Each of these items is treated in more detail in this section.

---

### sec4.1 -- Ascent to/Disposal from the Constellation's Operational Orbit

[sec4.1_p1] Safety-of-flight issues may be considerable when related to the ascent from the launch injection point to the satellite's on-station position or to the descent to an orbit from which disposal/re-entry can be accomplished directly. Typical ascent and descent trajectory design using now common electric propulsion or low-thrust chemical propulsion can take months to accomplish, potentially passing through highly populated regions of space along the way. For large constellations, the amount of nearly continuous transiting satellite traffic could be especially large as old satellites pass out of service and replacement satellites are added.

[sec4.1_p2] All missions should plan for orbit disposal to reduce long-term orbital debris. Reducing the number of inactive space objects minimizes the probability of generating debris through orbital collisions, and the fastest disposal option should be pursued; for example, existing large constellations are promptly deorbiting end-of-life spacecraft. The U.S. Government has established Orbital Debris Mitigation Standard Practices (ODMSP) including for post-mission disposal. NASA missions are required by NPR 8715.6 to comply with NASA Standard 8719-14B, Process for Limiting Orbital Debris, which defines how NASA implements the ODMSP.

[sec4.1_p3] During ascent and descent, a spacecraft will pass by other assets that are already on orbit. Some but not all of those may be maneuverable. The ascending/descending spacecraft that is equipped to maneuver needs to yield the right-of-way to existing on-orbit assets by performing risk mitigation maneuvers or ascent/descent trajectory alterations.

[sec4.1_p4] Special caution is needed to protect humans on orbit. If the ascent or descent trajectory will pass through the ISS altitude, operators should coordinate with the NASA JSC FOD to avoid perigee-lowering approaches that pose persistent and problematic orbital crossings with ISS and other human space flight assets.

[sec4.1_p5] To address this topic, the following specific practices are recommended: (A) Perform a study to compute the number of expected close approaches anticipated during ascent and descent as well as the imputed additional satellite reliability that will be required to meet satellite disposal requirements at the chosen operational orbit. (B) If the results of the study show a large burden, consider choosing a different mission orbit with a lower burden. (C) All missions should review and follow the ODMSP guidance standards. (D) When practicable, pursue active disposal using the fastest disposal option available. (E) Recognize that transiting spacecraft should yield way to on-station spacecraft and thus take responsibility for any conjunction risk mitigation maneuvers or transient trajectory alterations that may be required. (F) When planning descent using thrusters, if not planning an approach of circular-orbit altitude reduction, coordinate with the NASA JSC TOPO to ensure that perigee-lowering approaches do not present persistent and problematic orbital crossings with the ISS and other human space flight assets.

---

### sec4.2 -- General Orbit Selection: Debris Object Density

[sec4.2_p1] Space debris is not uniformly distributed about the Earth. Because most of the present space debris is generated from a relatively small number of satellite collisions or explosions, the orbital parameters of these collided or exploded satellites determine where the large debris fields reside. For example, the debris object density is much greater in the 750-900 km altitude band than in other parts of LEO. If possible, this region is one to avoid as a destination orbit since a larger number of serious satellite conjunctions with debris objects can be expected. An orbit that results in a high number of close approaches can lead to the need to perform many maneuvers that may not fully mitigate the risk of close approach. Spacecraft that cannot maneuver will operate at higher risk of debris creation, effectively presenting additional risk to other operators.

[sec4.2_p2] Orbit selection should be informed by a study to determine expected conjunction rates; that is, the expected number of lifetime conjunction risk mitigation maneuvers and the amount of satellite fuel needed to permit the number of desired years on orbit. Published results by NASA CARA are available and can be consulted to obtain first-order high-interest event density information. Use this information to assess the mission impact for conjunction risk mitigation maneuvers, given the expected lifetime of each satellite (e.g., additional propellant required, impacts on data collection).

[sec4.2_p3] To address this topic, the following specific practices are recommended: (A) Include considerations of orbital debris density when selecting candidate mission orbits. Debris density diagrams, indexed by object size, are published by the NASA Orbital Debris Program Office (ODPO). (B) When choosing from the final set of candidate orbits, perform a conjunction frequency analysis to determine the number of conjunction high-interest events per spacecraft per year that can be expected in each of the proposed orbital areas.

---

### sec4.3 -- Vehicle- and Constellation-Specific Orbit Selection: Spacecraft Colocation

[sec4.3_p1] While there is a process used by the International Telecommunication Union (ITU) to ensure that spacecraft locations are deconflicted from a radio frequency perspective, there is no similar centralized process for ensuring that spacecraft locations are deconflicted from a location perspective. This gap has resulted in spacecraft injecting into their planned mission orbit only to find another spacecraft already in or very near that location (colocated), which causes close approaches that then have to be mitigated, potentially very frequently, over the whole mission lifetime.

[sec4.3_p2] Another similar problem is that of systematic or repeating conjunctions in which a spacecraft has multiple repeated close approaches on successive orbits with another object over a long period of time; for example, when using electric propulsion to move slowly through a congested area. Two actively maintained spacecraft with very similar orbits can present a permanent recurring collision hazard because both spacecraft can be planning future maneuvers that result in a collision if not coordinated. Therefore, active coordination for every maneuver is required to prevent the execution of simultaneous maneuvers that could create a collision.

[sec4.3_p3] Due to the lack of centralized coordination, it is incumbent upon individual operators to protect themselves and the space environment by performing their own colocation analysis. The goal would be to prevent choosing a location already occupied by another spacecraft, or, if that is not possible, to work with the other operator before launch to devise a strategy to share the space. Colocation situations can often be ameliorated through relatively small changes to the planned orbits during the design phase. For NASA missions, CARA can perform this analysis using the current USSPACECOM catalog data. For non-NASA missions, CARA will make their software tool available via public release.

[sec4.3_p4] In performing the analysis, it is not necessary to consider proximity to objects that are not maneuverable (debris, rocket body, dead spacecraft) since the non-maneuverable object will pass through the area under the control of non-conservative forces such as atmospheric drag and not present a future issue for the (largely) static orbital parameters of the asset that is maintaining its orbit using maneuvers.

[sec4.3_p5] To address this topic, the following specific practices are recommended: (A) During orbit selection, perform an orbit colocation analysis to determine whether any of the proposed orbits is likely to create systematic conjunctions with existing actively maintained satellites. (B) If the orbit colocation analysis identifies systematic conjunctions, consider modifying the proposed orbit(s) slightly to eliminate this possibility. Optimizing orbit placement may be appropriate. (C) If the selected orbit is likely to present systematic conjunctions with a pre-existing spacecraft, then coordinate with the other operator(s) to arrange a process to coordinate maneuver plans routinely during the life of the mission.

---

### sec4.4 -- Launch-Related Conjunction Assessment

[sec4.4_p1] Launch Collision Avoidance (Launch COLA or LCOLA) is the evaluation of a particular launch trajectory to determine whether the launch assembly, following this trajectory, will produce any close approaches with other space objects. LCOLA is sometimes expanded to include more indirect analyses to ensure that launch offscourings (rocket bodies, fairings, etc.) do not produce close approaches with other objects during the first three days after launch, after which they can be presumed to be cataloged and thus handled by the usual on-orbit conjunction assessment methods.

[sec4.4_p2] LCOLA requirements are governed by the individual launch range or launch licensing agency. Requirements are in the form of stand-off distances or Pc depending on whether the on-orbit conjuncting asset is a piece of debris, operational payload, or human space flight mission. Examples of the current requirements are 200 km stand-off distance or 1E-6 Pc between the object to be launched and human space flight assets and 25 km or 1E-5 Pc between the object to be launched and non-human space flight operational payloads. In practice, LCOLA covers launched objects from the time they reach 150 km through three hours. Launch times found in violation of these requirements are enforced as launch hold periods by the respective wing commanders.

[sec4.4_p3] A large study conducted by NASA (Hejduk et al. 2014) questioned the added value of LCOLA, given the uncertainties typical of most predicted launch trajectories. However, recent improvements in the fidelity of these predicted trajectories have made the LCOLA enterprise potentially more meaningful, suggesting that the situation may need to be reevaluated.

[sec4.4_p4] The period between when traditional LCOLA ends and when an on-orbit asset can mitigate a risk using standard on-orbit conjunction assessment methods is called the COLA gap. The O/O should ensure that newly launched objects and their detritus will not come into conjunction with protected assets during the COLA gap (i.e., until the 18 and 19 SDS can catalog the new object(s) so it is available for conjunction assessment screening). Such analyses should consider not just the predicted nominal launch injection but the expected (up to three-sigma) launch dispersions.

[sec4.4_p5] Specific criteria defining COLA gap launch cutouts do not exist since they are dependent on both what is being launched and the capabilities of the protected on-orbit asset. For example, ISS needs 36 hours for the COLA gap, which allows 24 hours for 18 and 19 SDS to track the launch objects and then 12 hours for ISS to assess the risk and mitigate if required. Other assets might require more time to close this risk gap. The methodology for this risk assessment can be a stand-off distance phasing analysis, nodal separation and in-track screening, or a probability density model.

[sec4.4_p6] To address this topic, the following specific practices are recommended: (A) Protect human space flight assets from close approaches during the COLA gap using stand-off distance or statistical measures. (B) Conform to additional LCOLA requirements that the launch range may impose.

---

### sec4.5 -- Spacecraft Trackability

[sec4.5_p1] Effective conjunction assessment requires an accurate orbital state. Accurate orbital state data requires tracking data to maintain a comprehensive space catalog available to all space operators. The USSPACECOM catalog serves this purpose and uses the data collected by the SSN. Because even active spacecraft become debris after their end-of-life and therefore need tracking to obtain an orbit solution, all launched satellites need to be acquirable and trackable by the SSN from deployment until their demise so that they can be cataloged and maintained using SSN capabilities alone. Launching an untrackable spacecraft increases risk to all operators.

[sec4.5_p2] Objects are trackable if they have a large enough radar or optical cross section to be tracked by at least two SSN sensor assets. Analytical evaluations of trackability consider both object size and material properties, so there is no absolute size threshold that is determinative. But as a rule of thumb, satellites need to have characteristic dimensions of 10 cm in each major dimension for spacecraft with perigee less than 2000 km and greater than 50 cm in each major dimension for spacecraft with perigee greater than 2000 km.

[sec4.5_p3] If a satellite exceeds the above dimensions, then there is a reasonable expectation of its being tracked by the existing SSN, and typically no further concern with trackability is needed. If a satellite is smaller than the above dimensions, then additional arrangements should be made to see to its orbital maintenance should its onboard navigation fail. Such arrangements could include an agreement with a commercial SSA company to provide tracking and construct a predicted ephemeris until re-entry or to add a trackability enhancement or autonomously powered beacon device to allow the satellite's position to be determined for the whole of its orbital life. For objects that will regularly occupy a large number of altitudes, such as a HEO object, special examination of the situation is required.

[sec4.5_p4] To address this topic, the following specific practices are recommended: (A) Through selection of physical design and materials, ensure that the satellite is trackable by SSN from deployment until demise. For spacecraft with perigee heights less than 2000 km, the spacecraft should have characteristic dimensions of at least 10 cm in each major dimension. For spacecraft with perigee heights greater than 2000 km, the spacecraft should have characteristic dimensions of at least 50 cm in each major dimension. If the spacecraft cannot meet these dimension constraints, use a proven detectability enhancement. For spacecraft having orbits that span a large range of altitudes, ensure that the spacecraft is trackable at all altitudes.

---

### sec4.6 -- Spacecraft Reliability

[sec4.6_p1] Studies related to active debris removal strategies have shown that the greatest contributor to long-term space debris growth is derelict spacecraft left on orbit. Such spacecraft produce opportunities for collision with smaller debris objects that cannot be mitigated, which will then generally produce large amounts of debris. So, it is important to ensure that spacecraft survive until they can be disposed of by using appropriate disposal as described in the ODMSP and by ensuring that the probability of successful post-mission disposal meets or exceeds 99%.

[sec4.6_p2] To address this topic, the following specific practices are recommended: (A) Ensure that spacecraft reliability is high enough that the likelihood of each spacecraft remaining fully functional until it can be disposed of meets or exceeds 99%. (B) Reassess the 99% analysis whenever the underlying assumptions change; for example, extending operations beyond design life or failures in key systems.

---

### sec4.7 -- Development of Capabilities for Ephemeris Generation and Conjunction Risk Assessment and Mitigation

[sec4.7_p1] Conjunction assessment capabilities are needed immediately after launch. The software and process components of the conjunction risk assessment and mitigation capabilities should be fully developed and tested before launch including elements of both the flight hardware and the ground system.

[sec4.7_p2] Three main capabilities should be developed: (1) Predicted ephemeris capability. Predicted ephemeris data that can be shared with other O/Os is needed to assess the potential for collisions between space objects. The current U.S. Government conjunction assessment process uses two predicted ephemeris solutions for the asset: one generated using non-cooperative SSN tracking data and one shared with 18 and 19 SDS by the O/O. For active, maneuverable spacecraft, the O/O-generated ephemeris is often the best representation of the future position because the O/O usually has access to more cooperative tracking data to feed the orbit-determination process (for example, from onboard Global Positioning System (GPS) receivers); has an accurate model of the spacecraft's drag coefficient and frontal area; and, most importantly, can model planned future maneuvers in the predicted trajectory. To enable the conjunction assessment process, predicted ephemerides must be furnished frequently, span an appropriate period of predictive time, employ point spacing close enough to enable interpolation, provide a full state (position and velocity) for each ephemeris point, and provide a realistic 6x6 covariance matrix (with both variance and covariance terms) for each ephemeris point.

[sec4.7_p3] (2) Risk assessment capability. The results of satellite conjunction screening analyses are sent from USSPACECOM to O/Os in the form of CDMs that contain the states, covariances, and amplifying orbit determination information for the primary and secondary object at the Time of Closest Approach (TCA). These messages, however, are only a notification of a predicted potential close approach. For a robust safety-of-flight process, additional risk assessment analysis is needed to determine whether the close approach warrants mitigation. Conjunction risk assessment tools are needed to perform this risk assessment, which includes calculation of Pc and other relevant information such as expected collision consequence and whether the state and covariance information is sufficiently accurate to subtend the risk assessment process. For conjunction events in which the Pc exceeds a severity threshold, mitigation action planning is necessary to plan a mitigation option that will lower the Pc to below an acceptable threshold, usually conducted with the aid of trade-space plots that give conjunction Pc reduction as a function of spacecraft trajectory modification size and execution time. Validating these tools is important well before flight. Some commercial vendors offer risk assessment services. Spacecraft operators who choose to purchase risk assessment services should make these arrangements well before launch to ensure adequate time for testing and development of a corresponding process for choosing a mitigation option given the risk assessment results received from the service.

[sec4.7_p4] (3) Mitigation capability. A spacecraft can mitigate a close approach in several ways: propulsive maneuvers (e.g., performed using thrusters); attitude changes to take advantage of altered drag coefficients from altered frontal areas to effect a relative velocity change between the two objects; changing the spacecraft attitude to present a minimal frontal area to the relative velocity vector to minimize the likelihood of collision; and sharing ephemeris data with the operator of the other spacecraft involved in the close approach so that they can perform an avoidance maneuver. Because the design of the spacecraft and/or ground system needs to accommodate the ability to perform these actions, spacecraft operators should choose a mitigation option when they select their mission orbit and ensure that the capability is built into the system appropriately in time for testing and use.

[sec4.7_p5] To address this topic, the following specific practices are recommended: (A) Develop and implement a capability to generate and share accurate predicted spacecraft ephemerides including any planned maneuvers. (B) Determine during spacecraft design what risk mitigation approaches are possible for the spacecraft. While trajectory modification via thrusting maneuver is the most common approach, other approaches such as differential drag orbit modification are possible. (C) Develop and implement or arrange to acquire a capability to process CDMs and to compute conjunction risk assessment parameters such as the Pc. (D) Develop and implement or arrange to acquire a risk analysis capability to select mitigation actions that will lower the Pc for dangerous conjunctions to a user-selected value. (E) Validate conjunction assessment tools well before flight, typically 6-12 months prior.

---

## sec5 -- Pre-Launch Preparation and Early Launch Activities

[sec5_p1] The pre-launch and launch/early orbit phase of a satellite's lifetime is the period during which safety-related policy decisions are rendered and associated data interfaces established. Safety-related policy decisions carry forward as precedent and actual operational arrangement to form the O/O's safety posture over the satellite's orbital lifetime.

[sec5_p2] Many pre-launch activities such as establishing interfaces, data sharing agreements, and a Concept of Operations (CONOPS) require coordination. Other activities pertaining to the launch and early orbit period such as outlining particular practices and reporting are needed to conduct conjunction assessment activities during this phase of orbital life and to facilitate a smooth transition into on-orbit conjunction assessment. Each activity is treated individually in this section.

---

### sec5.1 -- CONOPS Discussions and Arrangements with USSPACECOM Pertaining to Launch Phase

[sec5.1_p1] A spacecraft's journey from the launch facility to its on-station positioning in its service orbit is complicated and open to mishaps that can damage both the launching spacecraft and other spacecraft in proximity. The U.S. Government performs required activities in support of the launch enterprise including cataloging the spacecraft and any other space objects, debris and otherwise, related to the launch. To meet these responsibilities and provide a safe exercise of the launch process, the spacecraft operator needs to execute several pre-launch activities and points of coordination. This is best done by coordinating with USSPACECOM to discuss all the related issues, exchange information, and generate a comprehensive launch and deployment plan so that all affected agencies can exercise their proper roles.

[sec5.1_p2] First, early cataloging is extremely important to enable the conjunction assessment process to protect all other on-orbit assets. To assist the cataloging function, all aspects of the launch delivery and deployment methodology need to be fully documented and communicated to USSPACECOM so that those performing the launch cataloging process know what to expect at each point and can respond appropriately.

[sec5.1_p3] Aspects of the launch delivery that may affect cataloging and should be documented would include the launch vehicle configuration (rocket versus air launch) and the launch phases (parking orbits, transfer orbits, injection, and post-injection transiting to on-station locations). The deployment methodology includes timing post-separation and number of objects deployed. Examples of unusual deployment features that might create cataloging issues include high-velocity deployments, non-immediate deployments (i.e., subsequent deployment of a satellite from one of the deployed satellites), and tethered satellites. Multiple nearly simultaneous deployments are a challenge for tracking and will delay cataloging which in turn poses a safety-of-flight risk. Deployments should be spaced to allow acquisition of each object. For multiple nearly simultaneous deployments, the expected range of deployment speeds and directions should be provided.

[sec5.1_p4] Input for the R-15 launch form (launch plan and orbital parameters) should also be discussed with USSPACECOM so that they may render any necessary clarification or aid. The launch provider is responsible for completing and submitting the R-15 form; however, the O/O should discuss the content with USSPACECOM well in advance of the launch.

[sec5.1_p5] Second, detailed launch trajectory information for all launch objects needs to be submitted to USSPACECOM with a Form-22 (on space-track.org) well before the launch date so that a colocation and object avoidance analysis for the launch sequence can be conducted and, if necessary, adjustments made to ensure safety. To avoid close approaches with the ISS and other active spacecraft, it may be necessary to modify the launch sequence slightly and eliminate certain launch times from the launch window.

[sec5.1_p6] Third, to avoid any data transfer failures while the launch is in progress, the required exchange of data between O/O and USSPACECOM during the launch process and the associated formats and timelines need to be established, understood, and exercised prior to launch.

[sec5.1_p7] To address this topic, the following specific practices are recommended: (A) Establish contact with USSPACECOM to describe and discuss all aspects of the launch including delivery and deployment methodologies. (B) Space the deployment of multiple spacecraft in a way that enhances the ability of USSPACECOM to quickly identify and differentiate the spacecraft using the SSN. (C) Discuss the preparation of the R-15 launch form with USSPACECOM so that they understand its contents and any implications for safety of flight. (D) Ensure that the launch provider submits the Form 22 and launch trajectory information to USSPACECOM, including ephemerides for the powered flight portions and orbital elements for the parking orbits, so that potential colocation and deconfliction potentialities can be discovered. (E) Provide to USSPACECOM as soon as possible launch-related information (e.g., injection vectors and initial ephemerides for deployed spacecraft) that can be used to assist with the cataloging process, especially to confirm the identity of launch-related objects. Coordinate a satellite numbering scheme (potentially including temporary satellite numbers) appropriate to the launch type and expected degree of cataloging difficulty. (F) Coordinate with USSPACECOM any potential launch anomaly diagnostic products that can be provided if issues arise during the launch and early orbit sequence.

---

### sec5.2 -- CONOPS Discussions and Arrangements with USSPACECOM and NASA Pertaining to On-Orbit Mission Phase

[sec5.2_p1] USSPACECOM is required to maintain orbit determination solutions on all objects arising from the launch as part of its military space object custody responsibilities. USSPACECOM also provides conjunction assessment information to O/Os for safety of flight. In order for USSPACECOM to maintain the catalog most easily for safety of flight for all, the O/O should provide relevant details about its assets. The pre-launch period is the appropriate time to conduct exchanges of information that facilitate these activities.

[sec5.2_p2] (1) Provide operational contact information. It is important for the satellite operator to provide operational point-of-contact information so that USSPACECOM can coordinate spacecraft tracking, cataloging, identification, and provision of space flight safety data for the satellite. Submitting this information allows expeditious contact if issues arise during the launch and on-orbit periods of the satellite's lifespan and provides the information necessary for USSPACECOM to arrange for the delivery of basic space flight safety services.

[sec5.2_p3] (2) Provide information about satellite construction and operation. Basic information about the satellite to be deployed, such as satellite dimensions, presence of deployable structures (e.g., solar panels, antennae), satellite material properties, and expected satellite regular attitude, needs to be communicated to USSPACECOM. These aspects of satellite construction and operation affect trackability, and an understanding of these construction features allows USSPACECOM to assemble a tracking approach, assign appropriate sensors, and predict the regularity of attempted tracking success.

[sec5.2_p4] (3) Provide satellite orbit maintenance strategy. The orbit maintenance strategy for the satellite should be communicated, at least at a high level. Information such as the logic for determining orbit maintenance maneuvers, the maneuver thruster technology, and the frequency/size/duration of orbit maintenance burns will help USSPACECOM to set up the orbit maintenance parameters properly for each satellite to perform the most reliable orbit maintenance possible.

[sec5.2_p5] (4) Provide the satellite flight control and navigation paradigm. Understanding the flight control and navigation paradigm of the satellite is needed, especially whether a traditional ground-control paradigm has been followed or whether autonomous navigation is used. If the latter, the amount of ground-knowledge of satellite activities, such as whether there is foreknowledge of maneuvers and an opportunity to override these maneuvers from the ground before they are executed, is important to understand the degree to which the satellite's navigation can be manually influenced if necessary. In addition to setting general expectations for the satellite's flight dynamics, this information is helpful in determining the degree of expected fidelity for regularly generated satellite O/O ephemerides.

[sec5.2_p6] To address this topic, the following specific practices are recommended: (A) Register the spacecraft with USSPACECOM using the Satellite Registration form on space-track.org. (B) Provide USSPACECOM with basic construction and mission information about the satellite such as stowed dimensions; deployable structures such as solar panels, antennae, booms, including all their (rough) dimensions; satellite material properties and colors; regular satellite attitude; and registered operational radio frequencies. (C) Provide USSPACECOM with a basic overview of the satellite's orbit maintenance strategy including the paradigm for determining when orbit maintenance maneuvers are required; the maneuver technology used (as this relates to burn duration and expected accuracy); and the frequency, duration, and magnitude of typical burns. (D) Provide USSPACECOM with an understanding of the flight control and navigation paradigm, principally whether a ground-based control approach is followed or some degree of (or full) autonomous control is used. If the satellite control does include some autonomous flight dynamics or control features, indicate how much (if any) foreknowledge ground controllers have of autonomous maneuver actions, the amount of information that is communicated to the ground both before and after the maneuver (e.g., maneuver time, delta-V, direction), and whether ground-based overrides are possible.

---

### sec5.3 -- CONOPS Discussions and Arrangements with USSPACECOM and NASA Pertaining to Conjunction Assessment

[sec5.3_p1] On-orbit conjunction assessment is needed to protect the satellite and keep valuable orbital corridors free of debris pollution and sustainable for the indefinite future. The screening process is the first step in completing a risk analysis for a potential conjunction.

[sec5.3_p2] Conjunction assessment screenings typically use O/O ephemerides as a statement of the primary object's position. Ephemeris formats, delivery mechanisms, and screening timetables need to be coordinated with the screening provider. The screening provider should use this information to monitor the satellite's projected position over time and identify potential conjunctions. The O/O needs to choose which screening service provider will be used (i.e., the USSPACECOM free service, a validated commercial conjunction assessment service, or both) and establish a robust interface prior to launch.

[sec5.3_p3] USSPACECOM currently provides a free conjunction assessment service that performs conjunction assessment screenings on behalf of an O/O and sends proximity warnings (CDMs) for each situation in which the distance between the O/O's satellite and another cataloged object is smaller than a set threshold. These messages give O/Os the data they need to assess the collision likelihood of a particular conjunction and, if necessary, plan and take mitigative action. NASA recommends at a minimum using the USSPACECOM conjunction assessment service, which can be augmented with validated commercial conjunction assessment services when desired.

[sec5.3_p4] Commercial providers should provide equivalent support and data formats. Many commercial conjunction assessment services work from the USSPACECOM conjunction assessment data and not from separate or unique commercial SSA data.

[sec5.3_p5] Based on satellite orbit and mission characteristics, a screening volume size will need to be assigned. The screening volume is the physical volume (generally an ellipsoid) that is "flown" along the primary's orbit during the screening process, with any objects found within this volume considered to be conjunctions and associated CDMs generated. If a commercial conjunction assessment provider is selected, the O/O will need to work directly with the provider to discuss the type, timing, and format of the information needed.

[sec5.3_p6] USSPACECOM distributes CDMs through space-track.org. An O/O needs to register on space-track.org and provide contact information to USSPACECOM to receive the basic level of CDMs. If desired, a test instantiation of space-track.org is available to allow O/Os to practice generating, receiving, and processing the conjunction assessment data products.

[sec5.3_p7] Finally, a formal Orbital Data Request (ODR) for any of the desired USSPACECOM-generated conjunction assessment information beyond the most basic products will need to be submitted to USSPACECOM and adjudicated before information exchange can begin. An ODR form is a method to request data and services beyond the basic ones. Non-U.S. Government entities are strongly encouraged to sign SSA sharing agreements with USSPACECOM, which expedites the ODR process. For NASA missions, ODRs are sent to CARA and JSC FOD, who submit them on behalf of the mission.

[sec5.3_p8] Large constellations may require special considerations such as quantity or timeframe of data used for screening and risk analysis. NASA has considerable operational experience with conjunction risk assessment and has assisted previous large constellation operators in designing a conjunction risk assessment process that scales appropriately.

[sec5.3_p9] To address this topic, the following specific practices are recommended: (A) Decide whether the USSPACECOM free service, a validated commercial conjunction assessment service, or both will be used by the mission. (B) Establish a service with the selected service provider. (C) Implement a SSA sharing agreement with USSPACECOM to receive advanced data support and services. (D) Through the registration of the satellite with USSPACECOM, begin the process of arranging for conjunction analysis data exchange including O/O ephemerides, maneuver notification reports, and CDMs. USSPACECOM uses the space-track.org account as the mechanism for product exchange. (E) If needed, complete an Orbital Data Request (ODR) form to arrange for delivery of USSPACECOM advanced conjunction analysis products. (F) For large constellations, coordinate with NASA and the screening provider to identify and address any special considerations.

---

### sec5.4 -- In situ Launch Products and Processes

[sec5.4_p1] Once liftoff is achieved, the foci become those of a safe journey to the final on-station destinations of the spacecraft and the efficient performance of the launch cataloging process. To determine whether the launch is nominal and has deposited its spacecraft as expected, the initial injection vector should be provided to USSPACECOM as soon as it is available. Additionally, once initial contact has been made with each spacecraft and initial position information has been downlinked, the generation and forwarding to USSPACECOM of associated predicted ephemerides is very helpful in properly identifying the new spacecraft and, in some cases, finding them in the first place. Finally, to render any desired anomaly support and to assume the appropriate conjunction assessment posture for non-functional spacecraft, it is important to forward (or update on space-track.org) spacecraft status information, especially in the era of "disposable" satellites in which infant mortality is higher.

[sec5.4_p2] To address this topic, the following specific practices are recommended: (A) To aid in satellite tracking and identification, provide injection vector(s) to USSPACECOM as soon as they are available. (B) To assist in spacecraft identification for the cataloging process and provide general awareness among all O/Os, generate and forward predicted ephemerides for the spacecraft to USSPACECOM and publish the ephemerides (and all subsequent ephemeris updates) publicly as soon as contact is established with each deployed spacecraft. (C) If USSPACECOM has issued TLEs for launched objects, notify USSPACECOM of the TLE and object number associated with your spacecraft. (D) Provide early reporting to USSPACECOM of any spacecraft failures or other operational difficulties, both to obtain any available anomaly support and to assign the appropriate conjunction assessment approach to the spacecraft (i.e., inactive and thus handled in a manner equivalent to a debris object). (E) If using a commercial provider, make sure it has access to information from items A-D.

---

## sec6 -- On-Orbit Collision Avoidance

[sec6_p1] For nearly all spacecraft, the on-orbit phase of their life cycle is the longest, meaning that the conjunction assessment practices and CONOPS in place during this phase will have the greatest impact on the satellite's overall risk exposure.

[sec6_p2] As explained in Section 3, the conjunction assessment process as presently structured comprises three phases: (1) Conjunction assessment screenings identify close approaches between a protected asset, called the primary satellite, and any other space objects, called (from the vantage point of the primary) "secondaries." (2) Conjunction risk assessment examines each of the close approaches produced by the screening activity to determine which may represent dangerous situations and therefore require a mitigation action. (3) Conjunction mitigation constructs a mitigation action, usually a trajectory change for the primary object, that will both reduce the collision risk of the close approach to an acceptable level and not create any new high-risk conjunction events. Before any of these three activities can take place, certain satellite data and position information needs to be produced and made available to the conjunction assessment process.

---

### sec6.1 -- Spacecraft Information and Orbital Data Needed for Conjunction Assessments

[sec6.1_p1] Because conjunction assessment identifies (and if necessary, mitigates) close approaches between spacecraft, it requires access to a comprehensive space catalog of orbital information. The USSPACECOM space catalog is the base catalog used by nearly all conjunction assessment practitioners. Several commercial conjunction assessment service providers offer conjunction assessment products derived from alternative space catalogs. O/Os are encouraged to pursue commercial services, particularly when such services offer improvements and innovations above and beyond what is available from USSPACECOM. However, NASA recommends O/Os use the service offered by USSPACECOM as both a baseline and a supplement to commercially procured services. This is because the objects contained in the commercial catalog may not be the same as those in the DOD catalog and because the commercial vendor may provide conjunction assessment services based on publicly available TLEs along with solutions for the objects in their own catalog. These TLE-based solutions are not sufficiently accurate to be used for conjunction assessment.

[sec6.1_p2] The USSPACECOM space catalog used for conjunction assessment contains position and uncertainty information for the primary object. If the primary object is not maneuverable, it is in principle possible to perform conjunction assessment on its behalf entirely from catalog-based information. However, satellite O/Os generally have a better understanding of the spacecraft's construction and thus its non-conservative force parameters such as the ballistic coefficient and solar radiation pressure coefficient. So, the O/Os' prediction of the satellite's future position, captured in a predictive ephemeris, is often more reliable or at the least an important adjoining datum to the future position information calculated from a space catalog entry. For satellites that do maneuver, future position calculations from catalog information alone will not capture any planned trajectory changes and thus will leave undiscovered any satellite conjunctions that could arise from the modified trajectory.

[sec6.1_p3] Therefore, for both non-maneuverable and maneuverable spacecraft, but especially for the latter, it is necessary that O/Os furnish predicted satellite state and uncertainty information, usually in the form of an ephemeris that includes state covariance at each ephemeris point. While it is most important to submit such ephemerides to the screening entity, it is also helpful to place them on a public-facing website for any space operator to download and process. Mutual sharing of expected future positions is the best way for active satellites to avoid collisions with each other. Claims that predicted ephemerides contain proprietary information of any consequence are simply not compelling and, in any case, are outweighed by the safety benefit of exchanging such information. Such ephemerides should be recomputed and reissued/reposted as soon as a change to a spacecraft's intended trajectory is planned.

[sec6.1_p4] In determining how to react when a primary satellite is found to be in conjunction with a secondary, the logic path is governed heavily by whether the secondary object is an active, maneuverable spacecraft. If the object is non-maneuverable, then the object will follow a Keplerian orbit without unexpected perturbations. Future positions can be predicted in a straightforward way using a dynamical model. If the object is an active spacecraft that either has shown a history of maneuvering or is believed to be maneuverable, then it is quite possible that trajectory-changing maneuvers are planned and the assumption of a Keplerian orbit is not appropriate. For this reason, it is important to know whether any given spacecraft is active, capable of maneuvering, and presently in a phase of satellite life in which maneuvering is possible. This status can be documented and communicated by the O/Os' setting of the spacecraft's active/dead and maneuverable/non-maneuverable flags in its space-track.org record. This allows other O/Os to determine whether the satellite can or will change its trajectory in a non-Keplerian manner and thus consider this possibility in conjunction assessment.

[sec6.1_p5] O/Os of active, maneuverable spacecraft should provide USSPACECOM with information outlining basic information about each planned maneuver. This data should be uploaded to www.space-track.org for the most efficient use. While in principle such information could be reconstructed from submitted ephemerides, it is simpler and more accurate to provide the information directly in this form. Providing maneuver notifications assists USSPACECOM in updating their own catalog as they know when to look for maneuver activity and, if discovered (and subsequently tracked), can expeditiously update the satellite's catalog entry to reflect the new trajectory. Of course, forwarding maneuver notifications is not a substitute for sending updated ephemerides that contain the intended maneuvers. Rather, it is an accompanying notification that allows better use of the received ephemerides and facilitates the USSPACECOM mission.

[sec6.1_p6] More O/Os are considering or are including autonomous flight control features, especially in large constellations in which the constellation management functions are complex. Autonomous flight control features may include orbit maintenance maneuvers and, in some cases, even conjunction risk mitigation maneuvers that are developed, scheduled, and executed from the onboard control system without any active ground-based participation. While such approaches can offer improved efficiencies, they present their own challenges.

[sec6.1_p7] Even if satellite maneuvers are planned and executed autonomously, these planned maneuvers must be included in ephemerides and made available in near real time both to the screening provider and more broadly. This is because, recognizing the latencies of the download and distribution mechanisms, satellite maneuvers must not be executed without sufficient advance notice to allow the conjunction assessment process to become aware of the intended maneuver and ensure its safety. The amount of advance notice required is governed by the latencies in the selected conjunction assessment process, which includes the O/O infrastructure to receive and react to conjunction assessment information from the screening provider. The principal difficulty is notifying other active, maneuverable satellites that may be contemplating their own maneuvers. Maneuver intentions must be shared with other such O/Os in a satellite's vicinity to ensure that intended maneuvers by either or both operators, if executed, do not place both satellites on a collision course.

[sec6.1_p8] To address this topic, the following specific practices are recommended: (A) Actively maintain the space-track.org record for the satellite, updating the active/dead and maneuverable/non-maneuverable flags to reflect the satellite's current status. (B) Furnish predicted ephemerides that include state covariances to USSPACECOM (and any additional commercial screening provider) and set the privileges to allow any interested party to access and download this information. (C) Furnished ephemerides should possess the following characteristics: be of a 7-day predictive duration for LEO and 14 days for other orbits; be issued at least three times daily for spacecraft with perigee heights less than 500 km, daily for other LEO orbits, and twice weekly for other orbits; include all known maneuvers within the ephemerides' prediction duration; provide ephemeris point spacing of approximately 1/100th of an orbit, in either time or true anomaly; contain a realistic covariance at each ephemeris point for at least the six estimated state parameters; and be formatted and distributed in the Consultative Committee for Space Data Systems (CCSDS) standard Orbital Ephemeris Message (OEM) format, preferably in the J2000 reference frame. (D) Furnish maneuver reports to USSPACECOM for any trajectory-altering satellite maneuvers sufficiently in advance of maneuver execution to enable an O/O evaluation of the maneuver's safety. Employ the standard maneuver reporting message for this notification. (E) When a maneuver becomes part of a satellite's trajectory plan, generate and submit to the screening provider an updated ephemeris that contains this new maneuver as early as is feasible but certainly with sufficient advance notice to enable an O/O evaluation of the maneuver's safety.

---

### sec6.2 -- Conjunction Assessment Screenings

[sec6.2_p1] The first step in the conjunction assessment process is to find close approaches between protected (primary) objects and other objects, with the latter object set's positions represented either by ephemerides (if they are active spacecraft) or space catalog entries. Finding these close approaches is accomplished by conjunction assessment screenings in which the predicted positions of the primary object and all other space objects that survive a pre-filtering process are compared.

[sec6.2_p2] At USSPACECOM, conjunction assessment screenings are executed using a volumetric-based approach: a physical screening volume (usually an ellipsoid) is "flown" along the primary object's trajectory, and any secondary trajectories that penetrate this volume are considered conjuncting objects. This screening is accomplished using a tool called the Astrodynamics Support Workstation (ASW). Volumetric screenings generate more close approaches than probabilistic or "covariance-based" methods but are preferable because they essentially give a snapshot of the satellite catalog in the vicinity of the primary's ephemeris. If the screening volume is large enough, the conjunction information can be used to determine the safety of not just the nominal trajectory but also any reasonably sized maneuvers that the primary may choose to make.

[sec6.2_p3] Once these close approach objects are identified, further processing is invoked to determine the precise time of closest approach between the two objects and the two objects' states and covariances at that time.

[sec6.2_p4] A protected asset should be screened for close approaches against a comprehensive satellite catalog at least daily with the results of this process obtained and processed by the O/O, also at least daily. For active, maneuverable satellites that submit ephemerides to the screening process as a way of including their planned maneuvers into their predicted trajectories, these ephemerides should be screened against each other (subject to appropriate pre-filtering) in near real time whenever an O/O submits an updated ephemeris. These "O/O vs O/O" screenings are the best way to ensure that maneuver plans are communicated among O/Os to prevent simultaneous maneuvers from causing a collision.

[sec6.2_p5] Current USSPACECOM practice is to conduct three screenings per day. Each screening predicts close approaches between: (1) The ASW solution for each protected (active) asset against the full catalog; (2) O/O ephemerides submitted after the last screening against the full catalog; and (3) O/O ephemerides submitted after the last screening against all other unexpired O/O-submitted ephemerides. In other words, O/O-submitted ephemerides are screened against the full catalog when initially submitted, then retained and screened against other O/O ephemerides until they expire. Three screenings per day is a minimum frequency for higher-drag orbit regimes such as LEO orbits with perigee heights below 500 km; daily screenings for other orbit regimes yields sufficient accuracy.

[sec6.2_p6] To address this topic, the following specific practices are recommended: (A) Submit predicted ephemerides for the spacecraft to a screening provider to be screened for conjunctions at least daily with spacecraft in higher-drag orbit regimes screened at least three times per day. (B) Ensure that an O/O ephemeris for an active, maneuverable spacecraft is screened against other ephemerides from active, maneuverable spacecraft in near real time after any such ephemeris is submitted to the screening provider. (C) Obtain and process these screening results from the screening provider at the same frequency at which they are produced for both the full-catalog and O/O vs O/O screening cases described above.

---

### sec6.3 -- Conjunction Risk Assessment

[sec6.3_p1] Satellite conjunctions are approaches between two satellites closer than a specified set of distances, which is often chosen to be much larger than the set that would pose an actual collision threat. A key next step, therefore, is to evaluate each conjunction to determine if it poses, or is likely to pose, a substantial risk of collision. This evaluation activity, called conjunction risk assessment, comprises both the key calculations that feed the concept of risk (that is, both collision likelihood and collision consequence) and the evaluations of the input data that subtend these calculations to ensure that they constitute a basis for decision making. Only conjunctions that are determined to be high risk merit the consideration of mitigation actions.

[sec6.3_p2] Methods proposed in the scientific literature for calculating and assessing satellite collision likelihood have different merits and different risk tolerance orientations. Of all the possibilities, Pc is the oldest, most straightforward, and most widely embraced collision likelihood parameter. Because it is a present industry standard, NASA recommends that operators employ Pc as the foundational element of their collision likelihood assessment.

[sec6.3_p3] Two approaches to calculating the Pc are: (1) The "two-dimensional" Pc (2D-Pc) calculation approach, which was originally introduced by Foster and Estes in 1992 with theoretical clarifications by Alfano (2005b) and Chan (2008). This method is a durable simplification of the calculation that is valid in most instances, but there are situations in which it does not perform accurately. (2) The "three-dimensional" Pc (3D-Pc) calculation method, which was originally formulated by Coppola (2012) and "repaired" and extended by CARA. Once it has completed final NASA validation, it is expected to be the recommended analytic approach. This method is accurate for nearly every situation, tests exist to identify those very few cases in which it may not be fully adequate, and it is computationally efficient. Additionally, the primary and secondary object covariances for some orbit regimes can contain significant correlated error, thus introducing inaccuracy in the calculated Pc.

[sec6.3_p4] Software (including source code, test cases, and documentation) to calculate the Pc using both the traditional two-dimensional and (when validated) the presently recommended three-dimensional technique and to remove correlated error between the primary and secondary covariances can be obtained free of charge at the public-facing CARA software repository.

[sec6.3_p5] The value of the Pc threshold at which an O/O would choose to mitigate a conjunction is a function of both the O/O's risk tolerance and the volume of conjunctions that the O/O is able to mitigate. In the conjunction assessment mission area, however, there has been broad convergence on a per-event Pc mitigation threshold value of 1E-04, meaning that remediation actions are recommended when the likelihood of collision is greater than 1 in 10,000. Missions that wish a more conservative risk posture can select a more demanding Pc threshold such as 1E-05. This infused conservatism may allow more streamlined risk assessment techniques in other areas.

[sec6.3_p6] Finally, some practitioners like to include an additional imperative to mitigate when the predicted miss distance is smaller than the hard-body radius or a distance close to the hard-body radius value. This predicted miss distance approach would need to be invoked only rarely because a small miss distance typically produces a Pc that violates most commonly accepted Pc mitigation thresholds.

[sec6.3_p7] Other risk assessment methods exist (e.g., Alfano 2005b, Carpenter and Markey 2014, Balch et al. 2019) and are generally considered to be more conservative than the Pc-based methodology advocated above. O/Os are encouraged to employ a more conservative conjunction assessment approach if it suits their risk posture. However, the Pc method and the mitigation threshold of 1E-04 enjoy wide acceptance in the conjunction assessment industry and historically have provided a sustainable balance between safety and mission impact.

[sec6.3_p8] Risk is the product of the likelihood of an unfavorable event and the consequence of that event, should it occur. For some time, it has been presumed that any satellite collision is a catastrophic event that is to be avoided by any means possible. In most cases, such events will render an active satellite unusable, presenting a very serious risk to the O/O. Therefore, the likelihood (Pc) was computed, and the consequence was not factored into the computation.

[sec6.3_p9] However, as the orbit environment grows, there may come a point when an O/O is faced with too many conjunctions to avoid individually. In this case, one method to triage the conjunctions to determine which should be mitigated to best protect both the spacecraft and the orbital environment is computation and application of the consequence as part of the risk determination. Satellite collisions, depending on satellite masses and relative velocity, can produce wildly different amounts of orbital debris from a handful of pieces to many thousands of pieces large enough to critically damage a spacecraft. To be sure, introducing any debris at all into the space environment is to be avoided. Conjunctions that, if they were to result in a collision, would produce only a small amount of debris (maybe fewer than 50 pieces) could be addressed using a more lenient mitigation threshold to prioritize them appropriately against those that would create more debris. A threshold that is an order of magnitude more lenient than what is used for high-debris conjunctions would align in magnitude with other situations in which relaxing the mitigation threshold is warranted (Hejduk et al. 2017).

[sec6.3_p10] Finally, the data used to calculate the parameters that feed the risk assessment decision, namely the state estimates and accompanying uncertainty volumes (covariances) for the primary and secondary objects, must be examined to determine whether they manifest problematic elements that would prevent the calculated parameters from serving as a basis for conjunction mitigation actions. In such cases, it is possible that executing a mitigation action based on those data could make the conjunction situation worse rather than better.

[sec6.3_p11] A non-actionability situation with USSPACECOM conjunction assessment products occurs occasionally. For example, an object whose state has been propagated longer than the orbit determination fit-span in order to reach TCA is considered under most conditions to be insufficiently tracked and over-propagated and thus not suitable as a basis for conjunction mitigation actions. Occasionally one of the two objects in a conjunction lacks a covariance, making probabilistic conjunction assessment impossible but still enabling other risk assessment methods.

[sec6.3_p12] To address this topic, the following specific practices are recommended: (A) Use the Probability of Collision (Pc) as the principal collision likelihood assessment metric. (B) Pursue mitigation if the Pc value exceeds 1E-04 (1 in 10,000). (C) Pursue mitigation if the estimated total miss distance is less than the hard-body radius value. (D) Employ the current operational NASA Pc calculation methodology for routine Pc calculation. Consider removing correlated error from the primary and secondary object joint covariance. (E) As a prioritization method for situations in which the number of conjunctions meeting mitigation criteria exceeds the ability of the O/O to mitigate, estimate the amount of debris that a conjunction would produce if it were to result in a collision. A less stringent Pc an order of magnitude lower could be appropriate in such cases. (F) If employing USSPACECOM data products for conjunction assessment, use the procedure given in Appendix P to determine whether the data for a particular conjunction are actionable and thus constitute a basis for conjunction assessment-related decisions. (G) If a different conjunction assessment product provider is chosen, develop and employ data actionability criteria for this provider's conjunction assessment information to determine conjunction assessment event actionability.

---

### sec6.4 -- Conjunction Risk Mitigation

[sec6.4_p1] O/Os with satellites that possess the ability to mitigate conjunctions have a responsibility to perform mitigation actions when required. In practical terms, this means that a mitigation action should be tendered when a conjunction's risk parameter, usually the Pc, exceeds the mitigation threshold at the "maneuver commitment point," which is that point in time before the TCA when a decision to mitigate is needed in order for the mitigation action to be in place before the TCA actually occurs. The most typical (and effective) mitigation action is changing the satellite's trajectory to avoid a possible collision with the secondary either by thrusting to effect a satellite maneuver or, in some cases, changing the satellite's attitude to alter its drag coefficient and thus its trajectory. An additional, although generally much less effective approach is to change the satellite's attitude simply to reduce the satellite's cross-sectional area in the direction of the oncoming secondary. This does not fully mitigate the close approach but reduces the likelihood of an actual collision between the two objects.

[sec6.4_p2] When planning a mitigation action, the general practice is to choose a trajectory alteration that reduces the Pc for the conjunction by 1 to 2 orders of magnitude. A recent study (Hall 2019a) showed that a Pc reduction by 1.5 orders of magnitude (that is, by a factor of approximately 0.03) marked the beginning of diminishing return with regard to lifetime satellite collision risk. Therefore, NASA recommends a minimum of 1.5 orders of magnitude reduction in Pc as a post-mitigation Pc goal; i.e., if the recommended mitigation threshold of 1E-04 is used, the post-mitigation goal would be approximately 3.2E-06 or lower.

[sec6.4_p3] In general, there is a trade-off between mitigation maneuver size and maneuver execution time. To mitigate a conjunction adequately, smaller maneuvers can be used if the maneuver is made earlier; that is, farther in advance of TCA. Waiting until closer to TCA will typically require a larger maneuver to achieve the same Pc reduction. However, because most conjunction events drop off to a low level of risk before TCA (due to additional tracking and improved state estimates and covariances), waiting until closer to TCA to perform a mitigation action increases the likelihood that it can be determined not to be necessary. Thus, O/Os must decide in each case whether they wish to act earlier with a less invasive action required or whether they wish to wait until closer to TCA in the hopes of observing the collision likelihood drop below the mitigation threshold and thus being able to waive off the maneuver, but knowing that, if an action is still required, it will be larger and more disruptive. There is no clear rubric for how to proceed in such cases, but external considerations, such as staffing availability for a later maneuver and amount of disruption that a large maneuver would require, often govern the decision.

[sec6.4_p4] When performing a trajectory-changing mitigation action, "new" conjunctions and/or elevated Pc values of existing conjunctions often occur. O/Os should ensure any mitigation action does not render the overall safety-of-flight evaluation worse by producing a more dangerous situation than would have existed without the mitigation action. One could examine the amalgamated risk of all the conjunctions in the near future and conduct mitigation planning on this basis, but it is generally acceptable to pursue mitigation of the one conjunction that violates the threshold (and remediate it to a Pc 1.5 orders of magnitude below the threshold) while ensuring that no new conjunctions are produced that exceed the threshold. Using the thresholds recommended here, this means bringing the violating conjunction down to a Pc of 3.2E-06 without introducing or raising any other conjunction Pc values above 1E-04. The best way to ensure that these standards are met is to generate an ephemeris that includes the mitigation action and submit it to the screening provider for a special screening action. This will produce a fresh set of CDMs against the planned mitigation action and allow easy and reliable verification that the Pc values for both the principal conjunction and any ancillary conjunctions remain below the desired levels.

[sec6.4_p5] Using larger screening volumes and analyzing the maneuver against the resulting CDMs is another method of determining whether the maneuver creates close approaches. This method creates CDMs for conjunctions that are somewhat far away from the nominal trajectory but might constitute worrisome conjunctions after a maneuver. This method for ensuring the safety of the post-maneuver trajectory is serviceable for debris objects, which will not alter their orbits beyond Keplerian models. However, this method does not protect against potential, yet-to-be-disclosed trajectory changes by other maneuverable satellites.

[sec6.4_p6] Choosing a mitigation action for a conjunction against a non-maneuverable secondary is generally straightforward, but the situation is much more complicated when the secondary object is an active, maneuverable spacecraft. While submitted ephemerides from that secondary's O/O is a statement of that spacecraft's intended trajectory at the time of production, it is likely that the secondary's O/O has also noticed the conjunction and may be planning a mitigation action of its own. In addition to the fact that two mitigation actions for the same close approach are unnecessary, there is a real danger that the two mitigation actions could place the two satellites on even more of a collision course than taking no action at all.

[sec6.4_p7] Establishing contact with the O/O of an active secondary is critical to jointly establishing a way forward for the particular conjunction. The O/Os should jointly decide which satellite will pursue mitigation (should it remain necessary at the maneuver commitment point) and what the mitigation action will be and constrain the trajectory of the non-mitigating satellite to that given in its published ephemeris until after the TCA has passed.

[sec6.4_p8] Given the recent industry trend to extremely large satellite constellations, the individual reaching out to other O/Os is unlikely to scale adequately as such constellations continue to grow. A more automated mechanism for exchanging maneuver intentions among O/Os will likely be necessary. SPD-3 directs the movement of conjunction assessment activities away from DOD to a Federal civil agency by 2024, so it makes sense for this new entity to develop an architecture and protocol for the exchange of this type of information. For the present, O/Os need to share their maneuver intentions with other O/Os through direct contact or a third-party organization, such as the Space Data Association, with some amount of manual interaction required.

[sec6.4_p9] To address this topic, the following specific practices are recommended: (A) When a conjunction's Pc at the mitigation action commitment point exceeds the mitigation threshold (recommended to be 1E-04), pursue a mitigation action that will reduce the Pc by at least 1.5 orders of magnitude from the remediation threshold. (B) Ensure that an ephemeris containing the mitigation action is screened against the full catalog, not a large screening volume collection of CDMs. (C) Ensure that the mitigation action does not create any additional conjunctions with a Pc value above the mitigation threshold (for which the recommended value is 1E-04). (D) When the secondary object is an active, maneuverable spacecraft, reach out to the secondary's O/O and jointly establish a way forward for the particular conjunction, including deciding which spacecraft will maneuver and freezing the other spacecraft's planned trajectory until the TCA has passed. (E) Use space-track.org contact information to engage other O/Os.

---

### sec6.5 -- Automated Trajectory Guidance and Maneuvering

[sec6.5_p1] Satellite designers are increasingly taking advantage of the automation of maneuver planning and execution to simplify mission operations. Particularly with large constellations of satellites, automation can substantially increase efficiency over traditional flight control techniques. However, the automation processes in which flight dynamics computations and decision making occur largely or even entirely without human intervention, and especially when they are executed on board the satellites themselves, can present real concerns related to space traffic management and the prevention of collisions between satellites.

[sec6.5_p2] With traditional ground-computed and executed trajectory adjustments that have humans in the loop, satellite operators can compute maneuvers, produce predicted ephemeris data to transmit to the rest of the community to be screened for conjunctions, and command the execution of safe maneuvers to effect that advertised trajectory. Admittedly, such a technique does not scale easily for large constellations, but it has proven reliable and serviceable for securing safety of flight over years of experience.

[sec6.5_p3] However, highly automated systems can now be designed in which satellites, using onboard navigation and a programmed target, can compute their own maneuvers and execute them without notification to, or pre-coordination with, their ground control; in such a case, explicit screening of the maneuver by USSPACECOM or some other screening authority, as well as notification to the rest of the space community, would frequently not be possible. If other space operators are not aware that a satellite is planning a maneuver, the other operators may also plan a maneuver and the two maneuvers taken together may cause a collision.

[sec6.5_p4] Operators of automated satellite systems should ensure that, when contemplating a maneuver, the intended time, direction, and magnitude of that maneuver (as well as other event-related data for conjunction risk maneuvers) be communicated to the screening authority at a sufficient interval of time before the maneuver so that the maneuver ephemeris can be screened before execution. This communication is most important in ensuring safety of flight between active maneuverable spacecraft, each of which may be generating maneuver plans that, if executed as intended, will create a collision. Maneuver plans should be shared with the screening authority as a predicted ephemeris and, if possible, a maneuver notification report; and it is highly desirable that this information be shared without restriction, perhaps by posting predicted ephemerides so that any interested party can download them, without restriction.

[sec6.5_p5] One method that some operators use to facilitate maneuver planning is to request from the screening authority the use of a screening volume size that encompasses a large amount of orbital space, ensuring that they receive a large set of CDMs that is representative of all the spacecraft operating nearby. The operators then use these CDMs to internally plan maneuvers and screen them against this set of CDMs to ensure that the planned maneuver does not cause a close approach with any of the objects for which they have CDM data. While largely effective against non-maneuvering objects, this practice cannot fully account for the fact that the other objects may themselves be maneuverable spacecraft also altering their trajectories. So, if both maneuverable spacecraft do not send their predicted trajectories back to the screening authority for reconciliation, they will not be aware of each other's plan and may in fact cause a collision.

[sec6.5_p6] At some point in the future, an automated clearinghouse may be in place for such information to allow near real time receipt and circulation of ephemerides containing maneuver plans to other O/Os. There is work underway at NASA in partnership with industry to design and prototype a ground node that includes near-real-time screening of submitted ephemerides, as well as mechanisms for the two spacecraft to assign responsibility for mitigating a specific conjunction. Hopefully, this capability will be available as part of the Office of Space Commerce's future space traffic coordination system. Quick and efficient screening of planned maneuvers generated by onboard systems requires the creation and operation of this central, low-latency ephemeris "clearinghouse." So, to achieve this future vision of safe operations, both industrial and governmental action is necessary.

[sec6.5_p7] At present, however, the USSPACECOM screening process is performed only once every 8 hours. It is possible that a file received from an operator may just miss a screening opportunity, and therefore it may take up to 16 hours to screen the predicted trajectory. Therefore, spacecraft using this information to plan onboard maneuvers are obligated to allow this much time in advance for screening their maneuver prior to executing to ensure safety of other on-orbit neighbors. This timeline also affects non-autonomous spacecraft, as they want to ensure that their planned trajectory is taken into account by the autonomous spacecraft, so they also must include the 16 hour advance notice, although that timeline is more typical for traditional ground-based flight dynamics processes.

[sec6.5_p8] Once a maneuver is autonomously planned and submitted for screening, the maneuver should be executed as designed unless an alteration is required for safety of flight. Remaining with the communicated plan, even if more efficient possibilities arise in the interval leading up to maneuver execution, enables the rest of the space community to use the submitted ephemeris as the basis for their own trajectory planning.

[sec6.5_p9] There may be situations in which a satellite should not execute an intended maneuver that the automation computes, such as in the presence of a surprise post-maneuver conjunction or a conjunction with an active satellite involving cooperative, human-in-the-loop conjunction event management. It is thus imperative for safety reasons that automated systems include the ability to pause or abort any maneuver planned for execution in response to ground command.

[sec6.5_p10] To address this topic, the following specific practices are recommended for use by operators of constellations with automated flight dynamics systems: (A) When an onboard flight dynamics system computes any maneuver to alter the satellite's orbit for mission or conjunction mitigation purposes, communicate maneuver details to the operating ground control early enough to enable a USSPACECOM screening and appropriate action in response to the screening results. (B) Share maneuver plans with USSPACECOM both as a predicted ephemeris with realistic covariance and, when possible, as a maneuver report. Allow these notifications to be publicly viewed and downloaded by any interested party. (C) Execute a maneuver as intended once a maneuver is autonomously planned and externally communicated unless an alteration is required for safety of flight. (D) Include the ability to pause or abort, for safety reasons, any maneuver planned by the automated system, regardless of whether the maneuver is planned by a ground system or on board the satellite. (E) Ensure an automated maneuvering system can temporarily suspend automatic conjunction assessment activities to allow another operator to maneuver. (F) Rapidly submit a new ephemeris for screening if a maneuver is not executed as planned in the original file sent for screening, such as if a maneuver fails. (G) Keep spacecraft maneuverability status on space-track.org up-to-date so other O/Os know whether any particular satellite is capable of maneuvering. (H) If onboard flight dynamics planning is used, be able to communicate back to operational secondary satellite O/Os that the relevant information to enable a maneuver to mitigate a conjunction between them is on board the satellite and will be acted upon. (I) Request a larger-than-average screening volume from your screening provider to ensure that the "snapshot" of the space catalog available to your satellites is broadly inclusive and thus allows maneuver planning to take cognizance of all possible hazards in choosing a new trajectory. This maximizes the chances that, when screened before execution, the chosen maneuver will be safe.

---

### sec6.6 -- Other Considerations

[sec6.6_p1] Light Pollution. After the launch of the initial portion of the SpaceX Starlink constellation, astronomers noticed that their observing campaigns were being affected by extremely bright, detector-saturating streaks due to the Starlink vehicles. To minimize interference with Earth-based astronomy, O/Os should ensure that spacecraft are built and flown in such a way as to minimize the creation of light pollution.

[sec6.6_p2] Adverse astronomical effects arise from satellites reflecting sunlight (e.g., in the visible and near-infrared (IR) spectral bands) and emitting radiation (e.g., in the thermal-IR and radio bands), as well as from occultations in which satellites block the light from astronomical objects. Ground-based sensors in the visible and near-IR bands are most challenged by constellations with satellites that are brighter than about 7th stellar magnitude and that glint brightly in reflected sunlight, especially if deployed at inclinations close to the observatory latitude, and, counterintuitively, at higher orbital altitudes (Walker et al. 2020; Bassa et al. 2022). Comparable orbit inclinations and observatory latitudes result in significantly more frequent crossings of the telescope's field of view. Higher altitudes result in a lower relative velocity between satellite and telescope, which increases dwell-time on each sensor pixel, causing saturation that irreparably contaminates data. Lower altitude satellites also tend to enter Earth's shadow earlier during astronomical nighttime periods, decreasing the duration of their adverse effects, even though they still can cause light pollution during twilight periods.

[sec6.6_p3] To preserve the appearance of the night sky and limit adverse effects on ground-based observations, the consensus recommendation of the astronomical community is to keep satellites fainter than the recommended V-band magnitude limit of Mv = 7 + 2.5 * log10(h/550km), with h indicating the altitude (Walker et al. 2020). This corresponds to Mv = 7 for the population of Starlink constellation satellites currently deployed at 550 km altitude, and Mv = 7.85 for the OneWeb satellites at 1,200 km. Factors that affect the brightness of individual satellites include size, albedo, surface characteristics, degree of specular vs diffuse reflection, self-shadowing, and orientation, as well as observation and illumination ranges and angles. Both a constellation's overall population and individual satellite brightnesses are important considerations because ground-based observations will be affected significantly by the existence of large numbers of bright satellites during astronomical nighttime observing conditions. Hall (2022) describes a method of evaluating light pollution levels for proposed or nascent constellations that incorporates a constellation's population, orbital distribution, and empirical or modeled brightness distribution to estimate the number of brighter-than-recommended satellites statistically expected to exist above observatories distributed over the Earth and throughout a year.

[sec6.6_p4] For constellations duplicating the same (or similar) manufacturing design for a large satellite population, O/Os should not rely on rough analyses or first-order assumptions about expected brightnesses. Instead, a full satellite brightness model that includes Bidirectional Reflectance Distribution Function (BRDF) characterization of surface materials should be undertaken during the design phase to estimate the brightness distributions of the deployed constellation. O/Os should consider all mission phases including launch, ascent, and descent (Seitzer 2020). The SATCON-1 Workshop Report (Walker et al. 2020) provides the following specific recommendations for constellation O/Os: LEO constellation operators should perform adequate laboratory BRDF measurements as part of their satellite design and development phase; reflected sunlight ideally should vary slowly with orbital phase as recorded by high etendue, large-aperture ground-based telescopes to be fainter than the recommended magnitude limit; operators should make their best effort to avoid specular reflection (flares) in the direction of observatories; and pointing avoidance by observatories is achieved most readily if the immediate post-launch satellite configuration is clumped as tightly as possible consistent with safety.

[sec6.6_p5] If possible, brightness distributions should also be measured using ground-based photometric observations of actual satellites (if already orbiting for partially deployed constellations) or scaled from observations of orbiting prototype/analog satellites based on a similar manufacturing design (if available). Hall (2022) describes how such empirical, ground-based brightness distributions can be used to estimate astronomical light pollution levels, even for constellations deployed in multiple altitude shells and with different inclinations. The SATCON-2 Workshop Report (Walker et al. 2021) describes the ongoing effort to establish the "Sathub" repository of photometric observations of constellation satellites, as well as several software tools to aid both astronomers and constellation O/Os in the effort to mitigate astronomical light pollution.

[sec6.6_p6] To address this topic, the following specific practices are recommended: (A) As part of spacecraft physical design and orbit selection, perform a spacecraft photometric brightness analysis to determine whether the spacecraft is likely to present an impediment to ground-based astronomy. Consider changes to the satellite's construction, materials, or operating attitudes to reduce expected photometric brightness to levels that will not impede ground-based astronomy. (B) If a large constellation is being planned, either use a full BRDF-based photometric model or ground-based observations of actual orbiting satellites, if available, to obtain a durable estimate of the entire constellation's expected brightness distribution as deployed on orbit. (C) If the constellation, given its population, orbit, and constituent satellites, is likely to affect ground-based astronomy, reassign the satellite orbits or modify the satellite construction to eliminate this effect.

---

## sec7 -- Contact Information

[sec7_p1] [Table 7-1: NASA Contact and Reference Information. NASA JSC FOD provides Human Space Flight Conjunction Assessment Operations, reachable at jsc-dl-topo-iwg@mail.nasa.gov. NASA CARA Operations is reachable at cara-management@lists.nasa.gov. The NASA CARA Software Repository is at https://github.com/nasa/CARA_Analysis_Tools.]

[sec7_p2] For NASA personnel, CARA or JSC FOD serves as the single point of contact between NASA and DOD (e.g., 18 SDS and USSPACECOM) for SSA data and support required for conjunction assessment and risk analysis of NASA missions. The roles of CARA and JSC FOD are defined in NPR 8079.1.

[sec7_p3] [Table 7-2: Contact and Reference Information for Non-NASA Personnel. USSPACECOM / 18 SDS contacts are Diana McKissock and Cynthia Wilson at 18SPCS.DOO.CustomerService@us.af.mil. Key reference documents include the Spaceflight Safety Handbook for Satellite Operators, the Launch Conjunction Assessment Handbook, and the Space-Track Handbook for Operators, all available at space-track.org. SSA sharing agreements can be requested via USSPACECOM.SSA.Agreement-Requests@us.af.mil. NASA examples of information to expedite review of commercial operator applications to regulatory agencies are at https://www.nasa.gov/recommendations-commercial-space-operators.]

[sec7_p4] To provide feedback on this handbook, email: ca-handbook-feedback@nasa.onmicrosoft.com.

---

## sec8 -- Appendix E: Use of Analytic Theory Orbital Data in Conjunction Assessment

[sec8_p1] The following is an amplification of a statement made by the Conjunction Assessment Technical Advisory Committee in 2016 on the use of analytic theory for conjunction assessment, and in particular the Two-Line Element (TLE) set publicly accessible catalog produced by USSPACECOM.

[sec8_p2] Due to enormous increases in computational capacity over the last twenty years, the space surveillance industry has been able to transition from analytic to higher-order theory approaches to space catalog maintenance. This transition brings a number of significant advantages to modern space catalogs such as the modeling of additional perturbations that were not typically represented in analytic theories (e.g., solar radiation pressure and solid earth tides), an improved fidelity in modeling the major perturbations (e.g., geopotential and atmospheric drag), and a durable covariance matrix to accompany each state estimate. Some of these features existed with previous analytic and semi-analytic theories, but it is only in recent times that all the additional attributes are routinely produced and available for space surveillance applications. This development has prompted the question of how data from analytic models (such as the Simplified General Perturbations Theory #4 [SGP4], which is used to produce the publicly available TLE catalogs) and higher-order theory models (such as the USSPACECOM Special Perturbations (SP) Space Object Catalog, although higher-order theory data can also be furnished from other sources) should be employed in the conjunction risk assessment process.

[sec8_p3] Risk assessment, whether it is quotidian consumer risk, the extremely serious discipline of nuclear accident risk, or the present problem of satellite collision risk, must always contain within its calculus some determination of the likelihood of occurrence of the event in question. For CA, this determination is most commonly assessed by calculating the Probability of Collision (Pc), and this calculation requires both the state estimates for the primary and secondary objects and the accompanying measures of these estimates' uncertainties, typically through a covariance matrix, at the time of closest approach (TCA).

[sec8_p4] Most implementations of analytic orbital theories lack the ability to produce a meaningful covariance (this is true of the SGP4 TLEs), so they cannot enable a probabilistic conjunction assessment calculation and thus for conjunction assessment applications will yield only a miss distance at the time of the two objects' TCA. Because no uncertainty information on this miss distance is provided, no probabilistic conclusion can be drawn. A miss-distance threshold can be (arbitrarily) selected to enable a binary "safe / not safe" conjunction evaluation, but there is no ability to determine, for example, how often the real miss distance, for which the nominal miss distance is only an estimated mean value, is actually likely to exceed or fall below such a threshold. No probabilistic assessment of whether a conjunction will be a dangerously close encounter is possible, so the likelihood of occurrence of a collision cannot be determined.

[sec8_p5] This dynamic is illustrated in Figure E-1, which gives a Cumulative Distribution Function (CDF) plot of miss distance distributions, both for critical (Pc > 1E-04) and non-critical (Pc < 1E-04) events. The lack of any strong correlation between Pc and miss distance is clear: the span of miss distances for the critical events (less than 100 m to more than 10 km) is extremely broad and overlaps significantly with the results for the non-critical events. For example, to eliminate 95% of the high-Pc events using a miss-distance criterion alone, a 10 km miss-distance threshold would need to be chosen -- a value that would also include more than 50% of the non-worrisome events, creating a density of events to mitigate so great that an unsustainable number of such mitigation actions would be required.

[Figure E-1: Miss-Distance Distributions for Critical (Pc > 1E-04) and Non-critical (Pc < 1E-04) Events]

[sec8_p6] Additionally, the inherent theory error in general perturbation approaches makes them ill-suited for even a risk-tolerant, miss-distance-based conjunction assessment. A published study (Hejduk et al. 2013) provided TLE accuracy information for the two methods presently used by USSPACECOM to generate them: traditional SGP4 orbit determination and Extrapolated General Perturbation (eGP) TLE generation, which performs the SGP4 orbit determination not against sensor measurements but against pseudo-observations generated from a higher-order theory trajectory for the object, which includes some forward prediction of the trajectory. Both methods are in use presently to generate the publicly-released TLE catalog, and it is not always easy to determine which method was used to produce any given TLE. Performance data for the orbital regime corresponding to Figure E-1 above (LEO with perigee heights greater than 500 km) show that, while the eGP methodology outperforms traditional SGP4 as expected, both are noisy in the epoch to 72-hour propagation states, which is the time-frame during which most conjunction risk mitigation decisions are made; errors from 600 m to 3 km are observed.

[Figure E-2: eGP and Regular SGP4 Errors: LEO > 500 km (log scale)]

[sec8_p7] Figure E-3 is an expansion of Figure E-1 to include CDF plots of miss-distance distributions for events with both Pc > 1E-03 and, separately, 1E-02. Figure E-3 suggests that a miss-distance criterion of several hundred meters, or perhaps 1-2 km, could be used as a simplified method of performing a risk-tolerant conjunction assessment: remediate any conjunctions with a miss distance less than 1-2 km. This move is ill-advised because it would result in a huge number of unnecessary mitigations (as an example, for events with a miss distance less than 1 km, the number with a Pc less than 1E-03 and thus not serious is six times greater than the number with a Pc greater than 1E-03). In addition, the inherent theory error (600 m to 3 km, from above) is a substantial portion of the miss-distance threshold of 1-2 km that would be imposed -- in some cases even greater than the threshold itself. So even under the very limited case described here, the use of general perturbation results for conjunction risk assessment is not tenable. Thus, analytic theory data, such as that contained in the public TLE catalogs, should not, taken alone, serve as a basis for risk assessment decisions and subsequent mitigation actions.

[Figure E-3: Expansion of Figure E-1 to Include CDFs for Pc Thresholds of 1E-03 and 1E-02]

[sec8_p8] Nonetheless, analytic theory orbital data, usually furnished as TLEs, does have its uses in conjunction assessment; some of these have included the following: mission planning activities such as the selection of satellite future orbits and final disposal orbits; general situational awareness information such as the positions of neighboring objects and the general effects of the basic perturbations (e.g., atmospheric drag); pre-filtering of candidates for conjunction assessment screenings for which simple TLE-based filters can eliminate substantial numbers of candidate pairs from requiring explicit screening runs; assessment of the status of a payload that appears as a conjuncting secondary, as active secondaries require special analysis and alerting when performing conjunction remediation; and consistency checks of the higher-order theory data. While the higher-order theory data are typically more accurate, they themselves are vulnerable to updates with bad or inadequate tracking data, errors in force model and integration control settings, and incorrect maneuver solutions. Comparisons with the analytic data can often identify such problems and, while not enabling their repair, can at least make clear that such cases constitute non-actionable conjunction assessment data.

[sec8_p9] Although the above uses of analytic theory data can be tangentially helpful to the conjunction assessment enterprise, these uses do not themselves directly support conjunction risk assessment, for which more accurate state estimate data accompanied by uncertainty information is required.

---

## sec9 -- Appendix F: Expected Conjunction Event Rates

[sec9_p1] The conjunction assessment workload that a mission experiences is a function of: the total number of CDMs that it receives, the number of associated events with risk levels high enough to require enhanced monitoring, and the number of events that have a risk level so high as to require mitigation planning and potential execution. During mission planning and development, it is helpful to have a general idea of the frequency of such situations to model spacecraft fuel consumption, identify staffing needs, and determine the appropriate degree of conjunction assessment-related tool automation.

[sec9_p2] The following profiling of conjunction assessment event densities is intended to provide at least some first-order information to answer these questions. These data summarize three years' recent conjunction assessment data history for the payloads that NASA protects in the LEO orbit regime (event densities in the non-LEO region are both low and highly dependent on the specific orbit, so summary information is not given here). In November 2021, Russia conducted a kinetic anti-satellite (ASAT) test that destroyed a derelict Russian satellite at approximately 480km, which generated an additional 1500 cataloged debris fragments in LEO. This increase is reflected in the three-year averages given below, although by 2024 most of these fragments will have decayed.

[sec9_p3] The graphs presented in Figure F-1 contain several component parts. The x-axis gives the hard-body radius for the conjunction, which is essentially the combined sizes of the primary and secondary objects. Although different methods are used to determine this size, perhaps the most common is to construct a circumscribing sphere about the primary object and add its radius to a standard radius size for the type of secondary object (i.e., payload, rocket body, or debris) in the conjunction (Mashiku and Hejduk 2019). Hard-body radius (HBR) values of 20 m, 10 m, 5 m, 2 m, 1 m, and 0.1 m are used, the last few values included to give some statement of the expected situation for small satellites. All the CARA events were reprocessed with these different HBR values to produce these varied results.

[sec9_p4] The y-axis gives the number of expected events per payload per year, shown on a logarithmic scale, with the type of event indicated by the color in the stacked bar charts: the height of the red bar indicates the frequency of red events, which are defined as events containing at least one CDM with a Pc value > 1E-04 (the most serious events requiring detailed monitoring, mitigation planning, and in a minority of cases, the actual execution of a mitigation action); the height of the yellow bar indicates the frequency of yellow events, which are defined as events containing at least one CDM with a Pc value > 1E-07 (these events generally merit increased monitoring and attention); and the height of the green bar indicates the frequency of events of any type, including those that never manifest a Pc > 1E-07. The overall height of each bar thus indicates the CDM generation rate.

[sec9_p5] Three different graphs are presented representing three different regions of LEO sorted by perigee height (p): a high-drag regime (p < 500 km), a low-drag regime (perigee height between 500 and 750 km) that includes a large concentration of satellites somewhat higher than 700 km, and a second low-drag regime from 750 to 1400 km. There are moderate differences in event frequency among the three different regimes, although some of that variation may be due to the different levels of sampling among the three, given that NASA operates substantially different numbers of payloads in each of these three regions. These results are more notional than precise; nonetheless, they do give some level of orientation to the expected loading for each event type (green, yellow, and red).

[Figure F-1: Conjunction Event Frequencies as a Function of Orbit Regime and Event Severity]

---

## sec10 -- Appendix G: Orbital Debris Density

[sec10_p1] Orbital debris can be generated by a number of processes that have conspired to produce the present debris environment: detritus cast off as part of the launch and injection activities; off-scourings from healthy satellites themselves, such as peeling of Mylar thermal insulation; collisions between satellites; explosions of satellites; and satellite debris-generating events in which (without any clear cause) a satellite is observed to break into more than one piece.

[sec10_p2] Because of the orbital locations in which major debris-producing events have taken place, and due to the differences in efficiency of natural debris cleansing arising from varying drag at these different altitudes, orbital debris density is not uniform. It is thus helpful to examine the current debris density situation when choosing an operational orbit because the orbital debris density does influence the rate of close approach events as well as the likelihood of satellite damage due to collision with an object too small to be tracked.

[sec10_p3] The graphs in Figure G-1 are produced from the NASA Orbital Debris Engineering Model (ORDEM) 3.2 release developed by the NASA Orbital Debris Program Office (ODPO). Generated for the 2023 debris environment, these graphs give debris flux (per satellite unit area and time on orbit) as a function of orbital altitude (a circular orbit is presumed) and inclination. Given that satellites often have many years of orbital lifetime, it could be argued that the graphs should be projected further forward in time and thus reflect debris generation events and natural debris depletion processes over as much as the next decade. However, such a prediction requires a stochastic execution of the model to try to guess at the number, severity, and location of future debris-producing events. Since the purpose here is to compare the relative debris densities of different regimes, the simplicity of a single rather than probabilistic presentation has been selected.

[sec10_p4] The graphs are arranged by inclination, and while it is observed that the highest inclination band represented here does have a slightly higher debris density than the two lower bands, the differences in both absolute debris flux values and the overall curve morphology among the three inclination bands are not great, indicating that inclination is not a major contributor to debris density. However, variation with orbital altitude is more marked: the density peaks at about 850 km and tails off in either direction from that peak at relatively equal rates. And the reduction is significant: about an order of magnitude in each direction within the bounds of the graphs (400 to 1200 km orbital altitude).

[sec10_p5] Three lines are shown on each graph: The amber line represents objects of sizes greater than 10 cm, which is the size of objects in the present space catalog without any contributions from the new Space Fence SSN radar and is thus related to satellite conjunctions presently tracked and reported. The reddish line is for objects of sizes greater than 5 cm, which is the size of objects that can be expected to be included in a catalog that does include tracking data from the new Space Fence radar, which should soon be contributing data to the conjunction assessment enterprise. The blue line is for objects of sizes greater than 1 cm, which is the size of objects large enough to be generally considered lethal to a satellite if there is a collision.

[sec10_p6] The gap between the reddish and blue lines thus represents the flux level of objects that are too small to be tracked yet large enough to potentially be lethally damaging. The gap represents a collision risk that, essentially, cannot be mitigated and so is simply accepted as the cost of operating a satellite in space.

[Figure G-1: Debris Flux as a Function of Orbit Inclination, Orbit Altitude, and Object Size]

[sec10_p7] In some cases, the flux densities between the 1 cm and 5 cm bands represent almost an order of magnitude difference. Considering this difference, some might question why conjunction assessment is pursued at all, given that so much of the collision risk cannot be remediated and simply must be accepted. To this question are two distinct responses: (1) First, within reasonable bounds, it makes sense to protect against safety-of-flight hazards that are known and that can, if necessary, be mitigated. NASA CARA's statement of purpose is "to take prudent measures, at reasonable cost, to enhance safety of flight, without placing an undue burden on mission operations." An unjustified over-investment in safety should be avoided, and mission objectives should not be regularly and substantially impaired by conjunction assessment considerations. But since it is not particularly expensive or difficult to identify high-risk conjunctions from the existing space catalog, and since such conjunctions are relatively infrequent, and furthermore, since the need for mitigation action for such conjunctions is less than an order of magnitude less frequent than that, it is good safety practice to conduct conjunction assessment operations according to the CARA mission statement. Space actors thus employ the conjunction assessment enterprise to "prevent the preventable."

[sec10_p8] (2) Second, the satellite conjunctions that occur between objects large enough to be in the space catalog are the ones that have the potential to generate large amounts of space debris. As the NASA ODPO has shown (Johnson et al. 2001), the debris production potential of a collision is a function of the two objects' relative kinetic energy and masses: the ratio of their two masses and their relative velocity. Collisions in which the two objects' masses are quite different will generally result in only the lighter of the two objects being fragmented with the heavier object being merely cratered. Objects in which the two masses are closer to each other will generally result in complete fragmentation of both objects and thus very large amounts of resultant debris. Extremely small secondary objects (i.e., a few centimeters in size and thus below the level of trackability), even at the large relative velocities of LEO conjunctions (approximately 10,000 m/s), are quite unlikely to cause fragmentation of larger active payloads, so the amounts of debris produced by collisions with untracked objects will in almost all cases be relatively small. Conjunction assessment against tracked and cataloged objects prevents the collisions that will produce large amounts of debris and thus makes a substantial contribution to space sustainability, even though it cannot protect payloads from debilitating conjunctions with very small objects.

[sec10_p9] Choosing a mission trajectory that occupies a region of space with a lower debris density is thus a favorable selection for two reasons: it lowers the risk of a lethal collision with an untracked object; and in general, it reduces the rate at which serious conjunctions with cataloged objects will occur. In addition, choosing orbits with lower orbital altitudes also facilitates compliance with the 25-year disposal guidelines.

[sec10_p10] While there may be a general correlation, it is important to recognize that there is not in fact a direct relationship between the debris flux for catalog-size objects and the actual rate at which conjunctions, even serious conjunctions, will be identified. The debris flux gives the expected frequency of penetration of the satellite's cross-sectional area by some object in space. According to the curves in Figure G-1, a sun-synchronous object at 700 km, placed at the middle of a 20 m radius sphere (to employ an extremely conservative statement of its exposed area), would over a one-year period expect only 0.006 penetrations of this 20 m sphere. In other words, it should take approximately 160 years on orbit for a penetration to occur. However, predicted close approaches for such an object are identified daily, yet mitigation actions would be warranted only a few times per year.

[sec10_p11] When a Pc for a specific event is calculated, it is interpreted as representing the likelihood that the "true" miss distance (which ultimately cannot be known) is smaller than the hard-body radius (HBR) defined about the object. The "true" Pc corresponding to the "true" miss distance is either 0 or 1; i.e., either the satellites will pass within 20 m of each other or they will not. However, the orbit determination process does not predict future positions without error, so a Pc value somewhere between 0 and 1 represents a calculation of the likelihood of a penetration given the uncertainties in the input measurement data to the orbit determination, lack of comprehensiveness of the dynamical model, and additional uncertainties expected in predicting the satellites' epoch states to their TCA (such as atmospheric density forecast error).

[sec10_p12] Generally, when the Pc is greater than 1E-04 (1 in 10,000 likelihood of a penetration), a mitigation action is recommended and executed. This threshold has emerged in the industry as an acceptable balance between safety-of-flight considerations and additional mission burden and encumbrance. Because mitigation actions are recommended when the likelihood of penetration exceeds 1 in 10,000, there is little surprise that the frequency of serious conjunction events exceeds that projected by the debris flux value: in only 1 out of 10,000 cases would the penetration actually occur if the mitigation action were not executed. Fortunately, a conservative approach such as this can be implemented and is still consistent with the mandates of prudent action, reasonable costs, and lack of undue burden on mission operations.

---

## sec11 -- Appendix H: Long-Term Collision Risk and Satellite Colocation Analysis

[sec11_p1] This section describes an approach developed by NASA CARA that uses long-term collision risk assessment methodologies applied to proposed satellites to predict: long-term average conjunction and collision rates that a new satellite can expect to experience; and the potential frequency of repeating conjunctions experienced by a new satellite due to being colocated with another satellite. These methods provide the means to avoid deploying new satellites into crowded regions of orbital space, and to prevent orbital collocation with one (or more) pre-existing satellites. Although the methods also apply to satellite mid-mission repositioning or end-of-life disposal efforts, they are demonstrated here using an example of a newly-proposed satellite.

[sec11_p2] With the ongoing deployment of large constellations, such as the Starlink or OneWeb systems, avoiding overly crowded regions of space is becoming an increasingly important consideration. In addition, avoiding orbital colocation prevents problematic repeating conjunctions with other active satellites. To this end, when a new operational orbit is being selected, a long-term collision risk and orbit colocation analysis should be performed. Fortunately, it is often possible to make sufficient modifications to an intended nominal orbit to avoid these unfavorable situations, yet still meet mission objectives.

---

### sec11.1 -- Long-Term Risk Assessment Theory and Algorithmic Description

[sec11.1_p1] Both semi-analytical and Monte Carlo methods provide the means to estimate statistically expected long-term collision rates between satellites. Monte Carlo methods also provide estimates of the overall expected frequency of conjunctions, as well as the frequency of repeating conjunctions which can be used to identify potentially colocated satellites.

[sec11.1_p2] The analytic theory developed by Kessler (1981) focuses on estimating collision rates between either spherical objects, or, for this application, circumscribing spheres. It provides estimates of such collision rates by neglecting all orbital perturbations except the orbital precession induced by Earth's J2 gravitational term. For each pair of analyzed satellites, the Kessler rate represents an average over six angles (Hall and Matney 2000). More specifically, the method averages collision rates over the three Keplerian orbital element angles for each of the two interacting objects. These three angles are the orbital right ascension of ascending node (RAAN), the argument of perigee, and the mean anomaly. In other words, Kessler rates represent six-dimensional phase space averages, which are equal to long-term temporal averages in idealized situations for which the ergodic hypothesis is valid. For this reason, Kessler rates are often referred to as "long-term" estimates, even when calculated in more realistic, non-idealized situations to which the ergodic hypothesis does not necessarily strictly apply.

[sec11.1_p3] [Equation H-1: The Kessler (1981) formulation expresses the average collision rate between two satellites as an integral over the volume of their overlapping orbital regions. The collision rate equals the combined collisional cross section multiplied by the integral of the product of the two satellites' average spatial densities and their average relative velocity during conjunction events, integrated over the overlapping orbital volume. The collision rate is zero for orbits that do not cross one another at all over long-term J2-only propagations. Calculating the integral entails two-dimensional numerical integration over geocentric latitude and radial distance.]

[sec11.1_p4] Monte Carlo methods provide another means to estimate collision and conjunction rates, and allow for more generalities, including: trajectories that account for orbital perturbations other than just those induced by the J2 gravitational term; both spherical and non-spherical shapes, used to estimate conjunction rates that arise from ellipsoidal or box-shaped screening volumes; and the ability to estimate the frequency of repeating conjunctions, used to assess satellite colocation.

[sec11.1_p5] The CARA Monte Carlo long-term rate estimation software uses the U.S. Space Force's Computation of Miss Between Orbits (COMBO) astrodynamics standards algorithm to determine close approaches between trajectories propagated using the SGP4 orbital theory. Input consists of a two-line element (TLE) set of mean orbital elements for the primary object plus a file of multiple TLEs for the secondary objects being considered. Each Monte Carlo trial entails propagating trajectories over a duration of several days (typically seven or ten), beginning each propagation at the epoch of the TLE catalog. The initial mean orbital elements for the trial propagations are the same as specified by the input TLEs, except that newly sampled orbital angles are generated for both objects, each randomly drawn from a uniform distribution spanning 0 to 2 pi. The Monte Carlo algorithm registers a "hit" for each instance that the trial trajectories approach one another close enough to penetrate a screening volume surrounding the primary object.

[sec11.1_p6] [Equation H-2: The Monte Carlo estimate for the conjunction rate is computed by dividing the total number of hits by the product of the number of trials and the propagation duration. The 1-sigma uncertainty of the estimate due to Monte Carlo counting uncertainties is approximated by dividing the rate estimate by the square root of the number of hits, which decreases as the number of registered hits increases. Rates of first-contact events are similarly calculated and are always less than or equal to the overall rate.]

---

### sec11.2 -- Long-Term Risk Assessment Methodology Demonstration

[sec11.2_p1] To demonstrate long-term risk and satellite colocation assessments, this section introduces a hypothetical proposed satellite called "NewSat" with a nominal initial orbit intentionally designed to be colocated with that of the Hubble Space Telescope (HST). In this hypothetical scenario, NewSat has a hard-body radius of 3.5 m, and is to be launched and reach its mission orbit on 2022-11-18. The mean orbit of HST at this epoch corresponds to an equatorial perigee altitude of about 530.6 km and an apogee altitude of 532.2 km. To create a colocated orbit for this demonstration, the TLE for NewSat's nominal orbit has been made identical to that of HST in all respects except that the inclination has been rounded to 28.5 degrees and the eccentricity increased to 0.001. This nominal NewSat deployment orbit corresponds to an equatorial perigee altitude of about 523.7 km and an apogee of 537.5 km. However, for this hypothetical scenario, NewSat is given the flexibility to deploy into an alternate orbit with the same inclination, but with perigee altitudes anywhere from 500 to 600 km, and with an apogee up to 20 km higher than perigee.

[sec11.2_p2] Figure H-1 shows a plot of Kessler collision rates between NewSat and HST, calculated with equation (H-1), and using hard-body radii of 3.5 m and 10 m for the two satellites, respectively. Specifically, the plot shows the collision rate (on the color axis), as a function of perigee altitude (horizontal axis) and apogee-perigee difference (vertical axis). The white star marks NewSat's nominal deployment orbit. Unsurprisingly, the nominal orbit corresponds to a non-zero long-term collision rate between the NewSat and HST. However, much of the area on the plot corresponds to zero collision rate; these regions represent candidate orbits for NewSat that do not overlap at all with HST's orbit because they are sufficiently higher or lower in altitude. NewSat could change deployment plans by shifting its orbit into one of these regions, but that would only mitigate the long-term collisional threat due to HST and not necessarily that posed to the other cataloged satellites.

[Figure H-1: Kessler Collision Rates between NewSat and HST]

[sec11.2_p3] Figure H-2 shows collision rates summed over all the satellites contained within the full 2022-11-18 TLE catalog. For the nominal NewSat orbit, a total of 2,183 secondary objects were found to have non-zero Kessler rates among the 36,368 cataloged objects. Figure H-2 shows that the nominal NewSat orbit is adjacent to (but not within) the region affected by the Starlink constellation satellites, which occupy several orbital shells at higher altitudes. So, if the NewSat mission were to change its deployment plans, doing so to avoid these already crowded regions would largely eliminate the expected number of Starlink conjunctions and associated risk mitigation actions. Figure H-3 shows collision rates for all cataloged objects, but with Starlink constellation satellites excluded. A comparison of the color axis scales on Figures H-2 and H-3 illustrates how relatively large the long-term rates due to the Starlink constellation are in this specific region of orbital space.

[Figure H-2: Total NewSat Collision Rates Among All Cataloged Satellites]

[Figure H-3: NewSat Collision Rates Excluding the Starlink Constellation]

[sec11.2_p4] Figures H-1, H-2, and H-3 graphically illustrate how long-term collision rate analyses can be used to avoid deploying satellites into crowded regions, and thereby help minimize the long-term collision risk among tracked objects. For the nominal deployment orbit, Figure H-2 indicates that NewSat's predicted collision rate is approximately 6x10^-4 per year, corresponding to a statistically expected collisional "lifetime" of approximately 1,700 years among cataloged satellites. Notably, shifting NewSat's deployment orbit into one of the regions most affected by the Starlink constellation would lead to a significantly shorter collisional lifetime of 30 to 100 years -- in the absence of any risk mitigation maneuvering, which fortunately is being performed by Starlink constellation satellites. However, a careful inspection of Figures H-1, H-2, and H-3 indicates that changing the planned NewSat deployment to an orbit with a mean equatorial perigee altitude of about 558 km and an apogee 7 km or less higher would avoid both the HST and Starlink regions simultaneously and extend NewSat's collisional lifetime to approximately 3,000 years.

---

### sec11.3 -- Colocation Analysis Methodology

[sec11.3_p1] During their on-orbit residence, most spacecraft come into conjunction with other, secondary, objects with some frequency. However, if two active satellites are maintained in similar orbits or in orbits that have consistent intersections, synodic alignment can occur that produces multiple close approaches on successive orbits. If the alignment is very close, such repeating conjunctions can persist over extended periods (e.g., much longer than typical seven- or ten-day conjunction screening durations) and could also recur periodically over a satellite's entire orbital lifetime. Because conjunction management between active payloads is complex, requiring coordination of maneuver plans and joint decisions about near-term courses of action, it is desirable to minimize such situations.

[sec11.3_p2] This section demonstrates how Monte Carlo analysis can be used to identify colocated orbits, which are characterized by a high frequency of repeating conjunctions, or, equivalently, a relatively low frequency of first-contact conjunctions. For this, the Monte Carlo method is used to predict the average rate of conjunctions and the fraction expected to be first-contact events. This specific analysis estimates conjunction rates averaged over a screening period of seven days (which is the same screening duration used for most LEO CARA satellites). Conjunctions occur in the Monte Carlo simulation when an ellipsoidal screening volume centered on the primary satellite is penetrated by the trajectory of a secondary satellite. The ellipsoid used for the present analysis has principal axes that measure 2x25x25 km, aligned with the primary object's radial, in-track and cross-track directions.

[sec11.3_p3] The goal of colocation analysis is to determine the orbital conditions that create repeating conjunctions, which can be accomplished by analyzing the frequency of first-contact events. First-contact conjunctions correspond to the initial instance that a conjunction screening volume is penetrated. This means that pairs of satellites that experience high frequencies of repeating conjunctions necessarily have low first-contact conjunction rates relative to their total conjunction rates.

[sec11.3_p4] The colocation analysis graph in Figure H-4 demonstrates this effect by comparing total conjunction rates and first-contact rates, plotted as a function of the offset of NewSat's orbital semi-major axis (SMA) from its nominal value. The black dashed curve plots the total conjunction rate, and the red dotted curve plots the first-contact rate, which is always less than or equal to the total rate. The black curve shows that conjunctions of either type are predicted to occur for SMA offsets within about 10.5 km of the nominal deployment value. However, the red curve indicates that the rate of first-contact conjunctions is noticeably lower than the total over a range of approximately 6.1 km in either direction. Furthermore, the first-contact rate is less than 50% of the total rate over the more restricted range of approximately 2.2 km in either direction.

[Figure H-4: Predicted Rates for NewSat and HST Satellite Conjunctions]

[sec11.3_p5] This analysis adopts this 50% reduction as a means to identify highly colocated orbits, meaning that, for this specific screening volume and under these conditions, NewSat would need to shift its deployment SMA by at least 2.2 km in either direction from the nominal value to avoid a high level of colocation with HST. Notably, shifting by about 6.1 km would largely eliminate the occurrence of repeating conjunctions, but would not prevent all conjunctions between the two satellites. A similar two-dimensional analysis holding only the inclination constant can also be performed to determine how much the SMA and/or eccentricity would need to change to eliminate high levels of colocation. The generalized method can be extended to even higher dimensions to include all relevant, adjustable orbital elements.

[sec11.3_p6] If the NewSat mission were to change the planned deployment orbit to avoid colocation with HST, then it would be worthwhile to do so in a manner that does not result in it being collocated with any other high value satellites. For example, the Fermi Gamma-ray Space Telescope (FGST) and the Swift satellite, both NASA missions, occupy orbits similar to NewSat's nominal proposed orbit. Figures H-5 and H-6 demonstrate this by showing one-dimensional colocation analysis graphs for FGST and Swift, respectively. Figure H-5 shows that FGST's first-contact rate is less than half of the total rate for SMA offsets in the range of approximately -7.0 km to -2.5 km, so if NewSat were to shift into this range, then the analysis predicts it would become highly colocated with FGST. Similarly, Figure H-6 indicates that shifting NewSat's orbit into the range of approximately +7.8 km to +12.4 km would result in high colocation with the Swift satellite.

[Figure H-5: Predicted Rates for NewSat and FGST Satellite Conjunctions]

[Figure H-6: Predicted Rates for NewSat and Swift Satellite Conjunctions]

---

### sec11.4 -- Long-Term Conjunction Rate and Colocation Analysis Software

[sec11.4_p1] The NASA CARA software instantiation of the long-term risk and colocation analysis capability is an ongoing tool development effort, currently being refactored to execute faster (which is especially required for the Monte Carlo analysis modes), and eventually to be more portable (because it currently uses the U.S. Space Force COMBO tool, which has restricted distribution). The main CARA software tool entitled "LongTermRisk" uses TLE catalogs as input and allows users to estimate long-term Kessler collision rates as well as Monte Carlo conjunction rates and associated first-contact rates. The tool also has the capability to ingest auxiliary TLE catalogs containing predicted future satellite constellations, or model populations of untracked orbital debris. Because the tool uses the limited-distribution AstroStandards version of the U.S. Space Force COMBO close approach analysis function, and because the associated execution environment is relatively complicated to set up, it is envisioned that the ability to run this tool will remain within the NASA CARA organization.

---

## sec12 -- Appendix I: Satellite Covariance Realism Assessment Procedures

### sec12.1 -- Introduction

[sec12.1_p1] This section outlines standard methods for assessing the realism of a covariance produced as part of the state estimates for a spacecraft. Because probability-based methods for satellite conjunction risk assessment in almost all cases require a covariance for their calculation, realistic covariances for both the primary and secondary objects in a conjunction are important.

[sec12.1_p2] The U.S. Space Force has pursued algorithmic improvements over the last twenty years to improve the realism and representativeness of their catalog's covariance matrices, especially in LEO, so covariances that arise from this source have undergone efforts to assess and sustain their trustworthiness. For covariances produced for and contained in satellite O/O ephemerides, the situation is much more uneven. While it is true that in the majority of conjunctions the covariance for the secondary object predominates, in 5% of the conjunctions the primary covariance is larger than the secondary, meaning that it is likely to play a substantial role in collision likelihood assessment in over 10% of conjunctions. This is a large enough subset of a satellite's conjunction history to make covariance realism for the primary object ephemeris an important consideration.

[sec12.1_p3] Because the focus of this section is more practical than theoretical, broader questions of covariance formation, methods of covariance propagation, or approaches within the orbit determination process to improve covariance realism are not addressed in detail. What this section does attempt to do is enumerate the data types needed for covariance realism investigations, introduce the appropriate test statistics, and give some guidance for the evaluation and interpretation of these tests. Most of the information here derives from NASA CARA's practical experience with this problem, as there are few published studies on the subject and no practical guides.

[sec12.1_p4] Covariance realism assessment consists of three parts: collection/calculation of position error data, calculation of covariance realism test statistics, and proper assessment of those test statistics.

---

### sec12.2 -- General Remarks

[sec12.2_p1] It is important to recall that a spacecraft state estimate generated through an orbit determination process is an estimate of a mean state (i.e., mean position and mean velocity), and a covariance is a stochastic characterization of the expected errors about that mean state. The governing assumption is that these errors in the individual components conform to a Gaussian distribution; this is how a covariance matrix alone is able to represent the error distribution without requiring higher-order tensors. The activity of covariance realism is thus to determine how well actual state errors conform to the Gaussian distribution that the covariance specifies.

[sec12.2_p2] The present treatment focuses on the assessment of the realism of the position portion of the covariance, although in most cases the methods advanced here are applicable to a full 6x6 or even larger covariance matrix. Since the principal purpose of covariance generation at present is for conjunction assessment applications, and for many conjunction risk assessment approaches only the position portion of the covariance is used (typically to calculate the probability of collision employing the two-dimensional simplification of the situation), it is acceptable to limit oneself to a Cartesian representation of the covariance and focus only on its position portion. Furthermore, a previous study has demonstrated that potential non-Gaussian covariance behavior brought about by working in a Cartesian rather than curvilinear framework rarely affects the outcome of higher-probability conjunction events, namely those with Pc values greater than 1E-04 (Ghrist and Plakalovic 2012).

[sec12.2_p3] Because a covariance represents an error distribution, its adequacy as a representation can be evaluated only by examining a set of actual error data and determining whether these data conform to the distribution specified by the covariance. In actuality, this is usually not possible in a straightforward way because a particular covariance is propagated to a particular point in time and relevant only at that moment, and at that moment there is only one state estimate and thus only one error point. It is not possible to determine in any definitive way whether a single point conforms to a distribution. Typically, this problem is addressed by generating a test statistic that compares each error point to its associated covariance, determining what the distribution of such test statistics should be, and evaluating the conformity of the set of test statistics to the expected distribution.

[sec12.2_p4] A covariance is always a covariance propagated to a particular time. If it is not propagated at all, it is an epoch covariance and is purported to reflect the error of the fit; if it is propagated, it is intended to be a reflection of the state estimate error of that propagated state. Many different covariance propagation methods are available, each of which achieves a different level of fidelity, so one source of covariance irrealism can be the propagation method itself. It is thus important to group realism evaluations by propagation state. It should also be pointed out that covariance irrealism, however it may be evaluated, is unlikely to correlate directly with propagation time in the sense that if the covariance is made realistic for a particular propagation state, it cannot necessarily be expected to be realistic for all propagation states. Many aspects of the U.S. Space Force's orbit determination process are tuned to optimize performance at 3 days' propagation.

[sec12.2_p5] In the context of a covariance error calculation, the individual state error values will need to be presumed to constitute independent samples, free of correlation between them. While true sampling independence is unlikely to be achieved, there are certain measures that can be taken to promote it. For example, if one has set up a covariance realism assessment scenario by comparing a predicted ephemeris (with covariance) to a definitive ephemeris, it would be best to take only one point of comparison for each propagation state bin that one wishes to evaluate. In past projects, trying to reduce the data overlap to less than 50%, meaning that subsequent ephemerides to be used in covariance realism evaluations need to be spaced so that they share less than 50% of the generating data with the previous ephemeris, has been considered acceptable in practice.

---

### sec12.3 -- Part I: Error Computation

[sec12.3_p1] Satellite state errors are computed by comparing a predicted state estimate to some type of state truth source. The predicted data typically come from a predicted ephemeris with a predicted covariance associated with each ephemeris point. Truth data can come from a variety of sources: onboard GPS precision position data, precision telemetry tracking data, or a precision ephemeris constructed from either of these. Ideally, both the predicted and truth data should align temporally (i.e., their time points should be exactly the same), and they should both possess covariance data at each time point.

[sec12.3_p2] Covariance data are often not available for the truth data source. If no covariance data are available for the truth source, one must then determine whether the errors in the truth data are so much smaller than the errors in the predicted ephemeris under evaluation that the errors in the truth data can safely be neglected. The degree of accuracy difference that would allow this assumption is a matter of opinion, but most commentators would probably agree that an order of magnitude difference would be an acceptable decrement. These differences must be considered at the individual component level. If the error in one component tends to dominate the entire arrangement (as is often encountered with the in-track component due to inadequate drag modelling), then the overall normalized error may be misleading at the component level.

[sec12.3_p3] If the time-points between truth and predicted data do not align, then some sort of intermediate-value-determination scheme must be used. While the interpolation of state values is relatively straightforward and can be accomplished satisfactorily with different interpolation approaches (e.g., Lagrange, Hermite), the interpolation of covariances is much more problematic. Different interpolation methods appear to work better in different situations, and there is a danger of certain interpolation methods producing non-positive-definite covariances, which do not make physical sense. If both data sources can be interpolated, but only one possesses accompanying covariance data (presumably the predicted source), then it is usually preferable to interpolate the data source that lacks the covariance.

[sec12.3_p4] Once position and covariance data for the predicted and truth datasets are aligned at the time-point of interest, calculation of the error is straightforward: it is simply the (subtracted) difference between states as long as they both are rendered in the same coordinate system. The two covariances for each comparison point are simply added together. While one can conduct an entire covariance realism evaluation in the Earth-Centered Inertial (ECI) reference frame, it is often more meaningful to move it into the Radial-In-track-Cross-track (RIC) frame -- a reference frame that is centered on the object itself. The use of the RIC frame is helpful in moving from a finding of irrealism to remediation suggestions.

---

### sec12.4 -- Part II: Test Statistic Computation

[sec12.4_p1] A position covariance describes a three-dimensional distribution of position errors about the object's nominal estimated state, and the test procedure is to calculate a set of these state errors and determine whether their distribution matches that described for the position covariance matrix. To understand the test procedure, it is best to consider the problem first in one dimension. Given a series of state estimates for a given trajectory and an accompanying truth trajectory, one can calculate a set of error values as the differences between the estimated states and the actual true positions. According to the assumptions previously discussed about error distributions, this group of error values should conform to a Gaussian distribution.

[sec12.4_p2] [Equation I-1 through I-3: The error values can be standardized by dividing by the sample standard deviation, producing a z-variable. The sum of the squares of standardized Gaussian variables constitutes a chi-squared distribution. The three position components (radial, in-track, cross-track) can be evaluated together, producing a 3-degree-of-freedom chi-squared distribution.]

[sec12.4_p3] [Equation I-4 through I-7: The test statistic uses variances from the covariance matrix rather than variances calculated from the actual sample of state estimate errors. When the covariance matrix takes on correlation terms, the matrix inverse formulates these terms to properly apportion the variances among the coordinate directions. The chi-squared variable is computed using the formulation epsilon * C-inverse * epsilon-transpose.]

[sec12.4_p4] [Equation I-8: For the two-dimensional case with correlation coefficient rho, the test statistic assumes a form that trades off between the overall inflating effect of the (1-rho) multiplier and the subtracted correlation term. When the correlation coefficient is zero, the equation reduces to the uncorrelated form.] The quantity epsilon * C-inverse * epsilon-transpose is called the Mahalanobis distance (technically, it is the square of the Mahalanobis distance), named after the mathematician P.C. Mahalanobis. This construct is a convenient and useful way to calculate a normalized distance.

---

### sec12.5 -- Part III: Test Statistic Evaluation

[sec12.5_p1] A test statistic can be derived that, if the covariance is realistic, should conform to a known statistical distribution. There needs to be some method for testing a group of these test statistics to determine if in fact they do conform to the expected distribution. Such a desire leads the investigation to the statistical subdiscipline of "goodness of fit."

[sec12.5_p2] A procedure can be applied to evaluate goodness of fit, namely, to evaluate how well a sample distribution corresponds to a hypothesized parent distribution. The general approach is to posit for the null hypothesis that the sample distribution does indeed conform to the hypothesized parent distribution, with a low p-value result counseling the rejection of this hypothesis. This approach is often called "weak-hypothesis testing." In the present case, the question to be posed is whether the observed squared Mahalanobis distance histories conform to a 3-degree-of-freedom chi-squared distribution -- if they do, then the covariance properly represents the actual observed error distribution.

[sec12.5_p3] There are several different mainstream techniques for goodness-of-fit weak-hypothesis testing: moment-based approaches, chi-squared techniques, regression approaches, and Empirical Distribution Function (EDF) methods. The easiest and most direct of these is simply a test of the first moment of the distribution (that is, the mean), which, if normalized by the degrees of freedom of the distribution, should be unity. The square root of this mean, the so-called Mahalanobis distance, is a good estimate of a single-value scale factor describing the covariances departure from reality.

[sec12.5_p4] While the first-moment test is easy to apply, in comparison to other goodness-of-fit tests it lacks power. To evaluate the match between entire distributions, the EDF methodology is generally considered to be both the most powerful and most fungible to different applications. The general EDF approach is to calculate and tabulate the differences between the Cumulative Distribution Function (CDF) of the sample distribution and that of the hypothesized distribution, to calculate a goodness-of-fit statistic from these differences, and to consult a published table of p-values for the particular goodness-of-fit statistic.

[sec12.5_p5] There are two goodness-of-fit statistics in use with EDF techniques: supremum statistics (the Kolmogorov-Smirnov statistics), and quadratic statistics (the Cramer-von Mises and Anderson-Darling statistics). It is claimed that the quadratic statistics are the more powerful approach, especially for samples in which outliers are suspected. [Equation I-9 through I-11: The basic formulation for both the Cramer-von Mises and Anderson-Darling approaches involves integrating the squared differences between the empirical and idealized CDFs, weighted by a function. The Cramer-von Mises statistic sets the weighting function to unity, while the Anderson-Darling uses a function that weights data in the tails of the distribution more heavily.] Given NASA CARA's experience that outliers frequently creep into covariance realism evaluations and introduce statistical processing issues, it is recommended to choose the somewhat more permissive Cramer-von Mises statistic for covariance realism investigation purposes.

[sec12.5_p6] It is a straightforward exercise to calculate the Q-statistic from the discretized form. The step after this is to consult a published table of p-values to determine the p-value associated with each Q-statistic. The usual procedure is to set a p-value threshold (e.g., 5%, 2%, 1%) and then to determine whether the sample distribution produces a p-value greater than this threshold (counseling retention of the null hypothesis) or less than this threshold (counseling rejection). NASA CARA has source code and test cases that perform all the above calculations and reside on the NASA CARA public-facing software repository for free download.

[sec12.5_p7] Even though there are normalization provisions within the EDF formulation, the results do, to some degree, depend on the size of the sample. One approach to mitigating this situation is to pick a standard sample size -- perhaps somewhere in the neighborhood of 50 samples -- and calculate the test statistic in a (with-replacement) resampled manner, producing a CDF of the p-values attained for each sample. What is an acceptable level for a p-value result? In goodness-of-fit practice, rarely is a significance level greater than 5% required, and levels of 2% or even 1% are often accepted. It probably can be said that values less than 1% cannot allow the conclusion that there is any real conformity to the hypothesized distribution.

---

### sec12.6 -- Part IV: Data Outliers and Conditioning

[sec12.6_p1] Goodness-of-fit test results can be sensitive to outliers; this is true whether one interprets results visually or uses a formal technique. The deleterious impact on test results is mitigated somewhat using the resampling approach discussed previously. However, the fact remains that bad data do enter the orbit determination process, and the failure to take some account of this reality can produce situations in which the covariance realism assessment problem becomes intractable.

[sec12.6_p2] The entire covariance realism assessment process is grounded on the notion that individual component errors are normally distributed, and this situation allows for certain techniques to identify outliers. The usual "x-sigma" filter is a naive and unscientific method for outlier identification and is especially difficult to justify for the Gaussian distribution. A superior approach is the Grubbs outlier test, which provides a formal statistical test for outliers but works only in situations with a single outlier and cannot be applied recursively (Grubbs 1969). In the case of multiple outliers, the procedures of Rosner (1975, 1977) are applicable but must be applied with an a priori guess of the number of potential outliers.

[sec12.6_p3] An extended example of the application of the technique described here is available in Zaidi and Hejduk (2016).

---

## sec13 -- Appendix J: CARA Conjunction Risk Assessment Tools Validation

[sec13_p1] CARA and JSC FOD are chartered to perform conjunction assessment screenings, risk assessment, and mitigation action guidance on behalf of NASA missions, so typically there is no particular need for missions to obtain or develop conjunction risk assessment evaluation tools. However, cases can occasionally arise in which it is not practical or desirable for CARA/JSC FOD to perform these calculations, such as for potential autonomously controlled missions that will perform onboard conjunction assessment. In such cases, the needed calculations or tools will be validated by CARA/JSC FOD before operational use.

[sec13_p2] Methods used for validation include the following: Inspection, used to verify that a particular datum or feature is present, that a display or graphic is properly constructed to convey a particular concept. Analysis/Documentation, used to ensure that a particular algorithm is theoretically or practically sound, that a needed parameter is properly converted to a different reference frame or units. Formal Test, used to ensure that critical calculations are performed correctly by executing a formal test with preconfigured input data and expected results, which are compared to the test article's results and any differences satisfactorily explained.

[sec13_p3] Formal test is the most frequently used methodology within CARA validation. Conjunction risk assessment calculations for which formal test cases with expected results presently exist include: two-dimensional Pc calculation with and without covariance correlation correction and non-positive-definite covariance correction; two-dimensional Pc calculation when one covariance is missing (Frisbee method); three-dimensional Pc calculation (Coppola-Hall method) with and without covariance correlation correction; Pc calculation by Brute Force Monte Carlo (from epoch); Pc sensitivity to space weather mismodeling; conjunction relative state comparison; collision consequence evaluation (anticipated number of resultant debris pieces above a certain size); and maneuver trade space calculations for single and multiple conjunctions.

[sec13_p4] The NASA CARA software repository includes test cases for software validation.

---

## sec14 -- Appendix L: Commercial Data in NASA Conjunction Assessment

[sec14_p1] The breadth and quality of commercial SSA data has developed in the last decade from essentially no commercial presence at all to the presence of a number of different vendors, each specializing individually in radar, optical, or passive radio frequency satellite tracking. In addition to tracking measurement data, vendor offerings now include additional SSA products such as vectors, ephemerides, and even conjunction risk assessment analysis outputs such as Conjunction Data Messages (CDMs).

[sec14_p2] In examining and facilitating the use of commercial data in NASA conjunction risk assessment calculations, CARA and JSC FOD adhere to the following principles: (1) Use raw observation (measurement) data only and combine with Space Surveillance Network (SSN) observations for a single solution; (2) Validate all non-SSN data; (3) Undertake a cost/benefit analysis before purchasing commercial SSA data.

---

### sec14.1 -- Use Raw Observation Data Only and Combine with SSN Observations for a Single Solution

[sec14.1_p1] Of all the data types offered by commercial providers, only the raw satellite tracking data are at present fully and unambiguously of use to the NASA conjunction assessment enterprise. Because NASA has access to both the USSPACECOM satellite tracking data from the SSN and the USSPACECOM space catalog maintenance operational system, NASA has the ability to include raw commercial observation data along with SSN tracking data into a combined close approach prediction. This process involves combining all of the measurement data on an object of interest from both the SSN and commercial sources to perform a single orbit determination. The CARA and JSC FOD personnel who are embedded in the 18 SDS work center at VSFB presently have this capability in a "sandbox" area on the operational system that is not used for routine operations.

[sec14.1_p2] Work is also progressing on a regularization of the reception of commercial data into the 18 SDS enterprise; when such a capability is in place, conjunction assessment products deriving from combined commercial and DOD tracking data will be possible routinely. But receiving higher-level products from commercial vendors, such as vectors or even conjunction assessment outputs like CDMs, introduces multiple solutions into the enterprise and creates significant operational difficulties in trying to adjudicate among them.

[sec14.1_p3] There is presently a debate within the industry regarding the best way to proceed with event evaluation in the presence of multiple solutions. CARA/JSC FOD encounters a version of this situation in miniature each day in that most conjunction events possess two solutions: one that uses the DOD predicted position for the primary object state and a second that uses the O/O predicted ephemeris for that state. Counterintuitively, it is not always clear which of these two sources for the primary object predicted state is superior.

[sec14.1_p4] O/Os typically have more and better past position data on their satellite, either from telemetry tracking or onboard GPS fixes, and they also understand the configuration of their satellite better and thus should be able to calculate the components of the ballistic coefficient with more precision. However, O/O orbit determination software atmospheric density models often introduce more error into predicted states than the model used by the 18 SDS. The choice of a preferred data source is thus not straightforward because a number of factors influence the selection: the capabilities of the O/O's orbit determination software, the particular orbit that the satellite occupies, the degree to which the space weather situation is perturbed, and the span from the present time to TCA, among others.

[sec14.1_p5] To reliably choose between these two information sources, CARA has performed a Verification and Validation (V&V) activity of the orbit determination products for selected NASA O/Os and has developed rubrics for selecting a particular solution (DOD- or O/O-based) source in particular contexts. A V&V activity of this type requires several months' worth of data and is complicated to perform, necessitating specialized tools and subject matter expert interpretation of the results.

[sec14.1_p6] An alternative to establishing a rubric of precedence for the multi-solution situation is using an assimilative modeling approach similar to that used for hurricane modeling. The results from multiple providers are collected and comparatively analyzed, both visually and with formal clustering algorithms. Although such a procedure appears attractive at first, it encounters difficulties in implementation: first, it requires a sufficient number of providers supplying solutions to obtain meaningful data clusters, and at present, the number of different providers is too small to use a clustering approach; second, because of explainable and therefore expected discontinuities in space surveillance data sets, it is not unusual for minority submissions to an assimilative framework to represent the superior solution.

[sec14.1_p7] Finally, there are proposals in the academic literature for the direct fusion of different orbital solutions, including CDMs from different providers, into a single solution. Carpenter (2020) represents the most promising of such proposals. While the theoretical development is indisputable, practical problems remain: the combination methodology relies directly on realistic covariances accompanying each state estimate, and the theory requires the cross-correlations between the covariance matrices arising from different providers to be characterized and deployed in the fusion paradigm, yet there is no obvious method for establishing these cross-correlations.

[sec14.1_p8] Given the present difficulties with the presence of multiple SSA and conjunction assessment products, NASA has concluded that it is necessary to limit the use of commercial SSA data to the combination of commercial and DOD tracking information.

---

### sec14.2 -- Validate All Non-SSN Data

[sec14.2_p1] All non-SSN sources supplying data to be used in orbit determination solutions to maintain the catalog must be regularly calibrated to ensure the removal of any data biases that might skew the conjunction assessment screening results. Using raw observation data from commercial vendors (as opposed to post-processed solutions) enables straightforward data characterization and numerical validation.

[sec14.2_p2] Commercial vendors supporting NASA are regularly asked to track and submit the tracking data taken on calibration satellites for which precision ephemerides are available. The commercial tracking data are compared to the precision ephemerides and the residuals characterized for each observable: mean errors can be subtracted from the data themselves and thus do not affect the data quality, error variances can be used as sensor data weighting factors in the orbit determination process, and non-Gaussianity and variation of the means and variances with time can indicate an unstable situation that may render a particular commercial provider's data unsuitable for use.

---

### sec14.3 -- Cost/Benefit Analysis Before Purchasing Commercial SSA Data

[sec14.3_p1] While following the recommendations of L.1 and L.2 above would allow the introduction and proper use of commercial SSA data in NASA conjunction assessment, a third consideration focuses on whether the use of these data would in fact result in a significant operational benefit. NASA, as a U.S. Government entity, follows the guidance of the National Space Policy to receive conjunction assessment services from the 18 SDS and 19 SDS. While it is true that adding properly calibrated commercial tracking data should result in a more reliable orbit determination solution with a smaller uncertainty, the effect may be such that the use of these additional data would rarely, if ever, result in a different operational decision regarding whether to require and execute a conjunction mitigation action.

[sec14.3_p2] Most conjunction situations receive adequate SSN tracking data to the degree that adding some additional data does not change the risk assessment appreciably and therefore does not alter the operational decisions. If, for example, having commercial data changed a conjunction Pc value from 5E-05 to 1E-04, this would cause a mitigation action to be taken when, without the data, it probably would not have been if the Pc threshold for maneuvering were set at 1E-04, so the operational outcome was changed to lower the risk. However, the risk exposure if the mitigation action were not taken is still very low: an actual risk of a 1 in 10,000 chance of a collision versus the 1 in 20,000 chance calculated with the SSN data alone.

[sec14.3_p3] The benefit would depend on how many times per year these decisions would be altered and thus how much risk is mitigated. By some estimates, over 90% of the objects larger than 1cm and thus able to render a satellite inoperative in a collision are too small to be tracked and thus are not considered in the conjunction risk assessment process. Therefore, part of the trade space is to consider how much adding commercial data reduces collision risk compared to the background risk. This decision would likely vary by orbit altitude based on trackability of objects of various sizes and the existing debris population.

---

## sec15 -- Appendix M: Use of the Probability of Collision (Pc) as the Risk Assessment Metric for Conjunction Assessment

### sec15.1 -- Introduction

[sec15.1_p1] Satellite conjunction assessment comprises two principal goals: to protect important assets from premature and unplanned mission failure due to preventable collisions and to assist in the regulation and minimization of space debris production. The first of these goals is in its details largely left up to the individual O/O to adjudicate. However, the second goal of overall space debris production minimization is much broader in scope. Born of the desire to keep key orbital corridors free from debris pollution to allow their perpetual continued use by all space actors, this goal transcends the health and preservation of the individual mission. It is in response to this second goal that actual conjunction risk assessment and mitigation thresholds are articulated.

[sec15.1_p2] The official CARA statement of purpose is: to take prudent measures, at reasonable cost, to improve safety of flight, without placing an undue burden on mission performance. This statement includes a number of qualitative terms, such as "prudent," "reasonable," and "undue burden"; for ultimately, it is not a scientific conclusion but a series of prudential judgments to assemble guidelines that work to prevent space debris pollution while accommodating the competing claims of mission execution and inherent levels of risk assumed simply by launching a satellite at all.

[sec15.1_p3] A 2020 NASA ODPO space debris catalog generated for CARA contained over 300,000 objects larger than 1 cm in physical size, which is the level at which an object is typically considered able to penetrate a satellite's shielding and render it inoperative. With tracking improvements and the launch of additional satellites, one can expect a regularly maintained space catalog of perhaps 30,000 to 50,000 objects. This means that, roughly speaking, about five-sixths of the objects large enough to leave a satellite in a failed and uncontrolled state if a collision should occur with them will be untracked. Given then that only about one-sixth of the close-approach events are trackable and in principle actionable, the overall conjunction risk analysis process should be commensurate with a situation in which greater than 85% of similarly likely collisions cannot be addressed at all and thus constitute a risk that simply has to be accepted.

[sec15.1_p4] Despite the large background collision likelihood that is accepted simply by launching a satellite at all, there is still great value to on-orbit conjunction analysis, risk assessment, and mitigation. First, because collisions between protected payloads and large secondary objects will produce, by several orders of magnitude, the most debris fragments, there is substantial benefit to avoiding collisions of this type. Second, when serious conjunctions of any type are identified and are well understood, these also represent straightforward situations for which due diligence counsels risk assessment and possible mitigation action. It is only when the situation is poorly determined that requiring remediation actions through an overweening conservatism is not appropriate, given the size of the accepted background risk.

---

### sec15.2 -- Point of Reference: Conjunction Risk Assessment Based on Miss Distance

[sec15.2_p1] In the early days of conjunction assessment, satellite position uncertainty data were not regularly available, so the only orbital data product on which to base conjunction risk mitigation decisions was the predicted miss distance at TCA between the two conjuncting satellites. Because the answers to questions about how threatening larger miss distances were and how large mitigation actions should be were not known, very conservative miss-distance thresholds were then embraced, often based on very little analysis, which led to a great deal of operational anxiety and unnecessary mitigation actions. Such a situation could not be sustained operationally: it went beyond prudence and presented an undue burden to mission operations. Methods were thus developed that considered the characterized error in the state estimates to determine the likelihood that the two trajectories at TCA would have a close approach smaller than the combined sizes of the two satellites.

[sec15.2_p2] Approaches that give an actual likelihood that the miss distance will be small enough to cause a collision are certainly an advance over the use of the miss distance alone. It is important to remember, however, that the probabilistic answer they produce is solely a function of the quality of the astrodynamics data that are used as input to the computation. In truth, the probability of collision for any given conjunction is actually either 1 or 0: the conjunction is either going to result in a collision, or it is not. The probabilistic framing comes from the degree of predictive certainty of the outcome brought by the quality of the astrodynamics data in the particular case. There is no "definitive" collision likelihood value for a particular conjunction. The probabilistic framework is helpful and desirable, but in the end, it reflects what is known about the conjunction situation rather than being a stand-alone, absolute assessment of collision likelihood.

---

### sec15.3 -- Use of Pc for Conjunction Risk Assessment Requirements

[sec15.3_p1] Many conjunction risk assessment metrics have been proposed in the critical literature; Hejduk and Snow (2019) give a useful overview, description, and attempted categorization. In choosing a particular collision likelihood metric for requirements specification purposes, different considerations are relevant: suitability to NASA's risk posture regarding conjunction risk assessment; ability to represent the collision likelihood in a manner commensurate with this risk posture; straightforwardness and tractability of calculation; ease of conceptual grasp of the metric and its origins; and acceptance within the conjunction assessment industry.

[sec15.3_p2] From these considerations, CARA has chosen the Probability of Collision (Pc) as the metric for high-risk conjunction identification. Of the different options examined, this metric was the best candidate to evaluate collision risk given the desired context of balancing safety of flight with minimizing the disruption of mission objectives. It is straightforward to calculate and, with certain recent improvements, can be computed quite accurately in many astrodynamically demanding circumstances. It can be explained relatively easily to, and be interpreted by, decision makers. It can be turned into a statement about miss distances if this framework is desired. Finally, it is the emerging metric of choice among the military, civil, and commercial conjunction assessment industries.

[sec15.3_p3] The Pc metric, as calculated by analytic methods, was first explained formally in 1992 (Foster and Estes). A comprehensive treatment of the calculation and associated issues can be found in Chan (2008), although this monograph is now somewhat dated and does not consider many of the newer collision likelihood calculation proposals.

---

### sec15.4 -- Pc and Probability Dilution

[sec15.4_p1] The main objection to the Pc metric is a phenomenon called "probability dilution" in which low values of the Pc do not necessarily guarantee safety although they can offer support for refraining from a mitigation action. As stated previously, probabilistic conjunction assessment metrics take on their probabilistic nature from the uncertainties in the state estimation data used to characterize the miss distance at TCA.

[sec15.4_p2] It is not difficult to understand the alignment of inputs that creates a high Pc value: the calculated miss distance between the two objects is small, and the primary and secondary object covariances are reasonably small also, so one easily envisions that of the entire set of possible actual miss distances, some notable portion will be smaller than the hard-body radius. But what if one or both covariances are extremely large? The range of possible miss distances now becomes extremely large. Therefore, a relatively smaller portion of the possible miss-distance values will be smaller than the hard-body radius, and the Pc will take on a small value. In the earlier case the likelihood of collision was small because the data showed safety; in this latter case, the likelihood is also small, but only because one has very little idea where one or both satellites actually will be at TCA. This latter case is called "probability dilution" (a term introduced by Alfano 2005b) because the small value of the Pc is not achieved by the certainty of well-established state estimates but through "dilution" by relatively poor state estimates.

[sec15.4_p3] The practical difficulty with embracing an extremely conservative position centers on the number of increased mitigation actions. An analysis by Hejduk et al. (2019) indicated that, for a protected asset in a 700 km circular orbit, the adoption of the conservative "ellipse overlap" technique would increase the number of mitigation actions by a factor of anywhere from 6 to 10. Increases by a factor of as much as an order of magnitude simply cannot be borne without resulting in major disruption, if not nullification, of the mission activities.

[sec15.4_p4] In addition to these practical impediments, a conceptual dissonance is introduced in embracing an extremely conservative risk-assessment strategy given the much larger collision background risk from untracked/uncataloged objects. As the amount of tracking information for a dilution-region conjunction is worsened, the satellite covariances increase in size, and the Pc decreases until ultimately it results in a value of 0 (to machine precision). This asymptotic value of nullity aligns nicely with the zero-value Pc that is imputed to conjunctions with untracked/uncataloged objects. As one progresses from knowing a lot, to a little, to nothing at all about a conjunction, the Pc moves conceptually from a higher value, to a lower value, to a zero value. For an extremely conservative approach such as that of minimizing covariance ellipsoid overlap, an opposing and inconsistent dynamic is observed: the less one knows about the conjunction, the larger the covariance ellipsoids are, and therefore larger and more frequent maneuvers are required. Given that about 85% of the conjunctions that a satellite actually faces cannot be addressed at all because nothing is known about them, it does not make sense that, for the small number of conjunctions for which only poor data are available, large and disruptive mitigations are required.

[sec15.4_p5] Instead, the use of the Pc, with the dilution region dynamic explicitly recognized, is a more natural fit for the situation that is actually encountered. It is thus acceptable to treat low-Pc dilution region cases as a small extension of the untracked/uncataloged object conjunctions and not pursue a mitigation action. Requiring conjunction remediation for low-Pc cases with poor data quality when such cases are truly very similar to the approximately 85% of conjunctions that are believed to exist but for which no mitigation is possible is not considered to be a prudent measure.

[sec15.4_p6] While not appropriate for broad implementation, conservative approaches such as ellipsoid overlap minimization are not without merit. In situations in which the collision risk may be pushed higher by other factors, such as a high-consequence conjunction due to the potential to produce large amounts of debris, it may be desirable to employ a conservative method when in the dilution region. Finally, there have been interesting proposals recently for risk assessment approaches based not on the probability of collision but on the related concept of examining confidence intervals on the miss distance; such proposals include those of Carpenter (2019) and Elkantassi and Davison (2022).

---

### sec15.5 -- Conclusion

[sec15.5_p1] It is emphasized that the above guidance describes and explains why Pc was chosen by NASA CARA as the risk-assessment metric to be used to determine the circumstances under which a mitigation action is required of missions to promote preservation of orbital corridors from debris pollution. O/Os may always pursue additional mitigation actions in a manner that increases the conservatism of their safety profile. This guidance indicates that mitigation actions are required only when the Pc exceeds the appropriate mitigation threshold. Missions may always pursue additional such actions, either episodically or as part of a standing strategy, if they wish a more risk-adverse safety posture.

---

## sec16 -- Appendix N: Pc Calculation Approaches

### sec16.1 -- Introduction

[sec16.1_p1] This appendix discusses the selection of the Probability of Collision (Pc) as the risk assessment parameter to use for conjunction assessment requirements compliance. While this parameter is widely used in the conjunction assessment industry, issues related to its calculation exist that merit extended discussion. The most frequently used analytical techniques to calculate the Pc are well established and computationally efficient but include assumptions that restrict their use and make the calculations vulnerable to error for a small fraction of conjunctions. Numerical techniques also exist, such as Monte Carlo Pc estimation. Predictably, these make fewer assumptions and are more widely applicable, but they are much more computationally demanding.

[sec16.1_p2] To accomplish this goal, this appendix addresses the following technical areas: a step-by-step description of the conjunction plane two-dimensional Pc calculation and its enabling simplifications and assumptions; examination and repair/expansion of input data to the calculation, focusing mostly on the orbital state covariance matrices; discussion of the use of Monte Carlo techniques for high-fidelity Pc computation; a test to determine whether an analytical or numerical technique should be used for a particular conjunction; approaches to choosing the hard-body radius for the Pc calculation; and discussion of alternative analytic Pc calculation methods, specifically the two- and three-dimensional "Nc" estimation methods.

---

### sec16.2 -- Conjunction Plane Analytic Pc Calculation

[sec16.2_p1] The conjunction plane method of Pc calculation, which is by far the most widely used approach in the conjunction assessment industry, was developed for the Space Shuttle Program and first described in the literature in 1992 (Foster and Estes). There have been a number of important treatments since that time -- e.g., Akella and Alfriend (2000), Patera (2001), Alfano (2005a), Chan (2008), Garcia-Pelayo (2016), and Elrod (2019) -- but all rely on the same basic methodology: applying reasonable assumptions to enable analytical approximations.

[sec16.2_p2] The first step is to recognize that Pc calculations depend on the relative positions and uncertainties of the two objects, so moving to a framework that views the problem this way is helpful. Subtraction of the two objects' positions produces a relative position vector, the magnitude of which is the miss distance between the two objects (at TCA). Similarly, because interest is in the relative rather than absolute position uncertainty, it is possible to combine the two objects' covariances to form a joint (relative) covariance and allocate that to one "end" of the relative position vector, as shown in Figure N-1.

[Figure N-1: Relative Positions and Uncertainties of Two Objects]

[sec16.2_p3] The second step relates to modeling the combined sizes of the primary and secondary objects. The general approach is to place a circumscribing sphere about the primary and then do the same thing for the secondary. These two objects' size spheres can be combined into one super-sphere (also called the "collision sphere") and placed at one end of the relative miss vector, as shown in Figure N-2. If the miss vector should shrink to be smaller than the radius of the collision sphere (also called the hard-body radius), that would be the equivalent of the two original spheres encroaching on each other at TCA.

[Figure N-2: Combining the Sizes of the Primary and Secondary Objects]

[sec16.2_p4] The third step is to envision the situation at TCA in which all the uncertainty is assigned to the secondary object's end of the relative miss vector. If the encounter is presumed to take place extremely quickly -- and this is in most conjunctions a good assumption because the satellites' relative velocity usually exceeds 10 km/sec -- then two assumptions can be made: the trajectories are essentially rectilinear during the encounter period, and the covariances can be considered static. This means that the encounter can be envisioned as in Figure N-3.

[Figure N-3: Geometry at TCA]

[sec16.2_p5] The passing of the primary object by the secondary can be seen as following a "soda straw" straight trajectory whose cylindrical radius is the same as that for the hard-body radius. Since the joint covariance represents the uncertainty of one "end" of the miss-distance vector, this dot can be presumed to be potentially in any place within the ellipsoid, meaning that in some portion of those realizations, it will fall within the soda straw. The entire situation can thus be reduced from three to two dimensions and analyzed as a phenomenon that occurs on a plane that is orthogonal to the relative velocity vector. This procedure defines the "conjunction plane," which can be viewed in two equivalent representations as shown in Figure N-4.

[Figure N-4: Two Equivalent Representations of the Conjunction Plane]

[sec16.2_p6] [Equation N-1: In the first conjunction plane representation, the Pc is given by a two-dimensional integral of the bivariate Gaussian probability density function (defined by the joint covariance matrix C and relative miss vector r) over the hard-body-radius circular area A. This is the foundational Foster and Estes (1992) formulation.] There are several different approaches to evaluating this integral. Foster and Estes (1992) applied a two-dimensional quadrature technique. Chan (2008) uses equivalent-area transforms to produce a single-dimensional integral. Garcia-Pelayo et al. (2016) derives a multi-term series approximation. Elrod (2019) formulates the integral in terms of complementary error functions and uses Gauss-Chebyshev quadrature with nodal symmetry.

[sec16.2_p7] [Equation N-2: In the second conjunction plane representation, the Pc is given by a one-dimensional integral involving error functions (Alfano, 2005a), with integration over the hard-body radius R, using the miss distance components and the principal axis covariance standard deviations.] For most conjunctions, Gauss-Chebyshev quadrature provides an efficient and accurate means to calculate the one-dimensional integral. However, for conjunctions involving relatively large hard-body radii (or, equivalently, small covariance ellipse dimensions), this method can potentially become inaccurate.

[sec16.2_p8] [Equation N-3 and N-4: The second conjunction plane representation also provides an extremely efficient means to calculate an upper limit estimate for the Pc value. This upper bound corresponds to the two-dimensional integral over the square that circumscribes the hard-body radius circle, which has an analytical solution involving products of error function differences. This circumscribing square probability estimate does not require any numerical integration at all but instead only requires the computation of four error or complementary error functions.]

[sec16.2_p9] It is helpful to review the four assumptions employed by the conjunction plane Pc calculation methods: (1) Statistical Independence -- the two objects' uncertainty distributions are statistically independent so that the joint covariance can be obtained by simple addition. (2) Rectilinear Motion -- it is reasonable to presume rectilinear motion and static covariances during the encounter. (3) Gaussian Position Distributions and Negligible Velocity Uncertainties -- the position vector state errors at TCA follow Gaussian distributions and velocity uncertainties are negligibly small. (4) Temporally Isolated Event -- the conjunction presents a single, well-defined event. To address conjunctions that do not satisfy these assumptions, two alternative analytical methods are available: the "three-dimensional Nc" method and the "two-dimensional Nc" method.

---

### sec16.3 -- Three-Dimensional Nc Method Analytic Pc Calculations

[sec16.3_p1] The relatively infrequent conjunctions that do not satisfy the conjunction plane method assumptions must be addressed with a different methodology. Coppola (2012) proposed a method for single encounters designed to account for non-linear orbital motion and velocity uncertainties. Chan (2015) contested Coppola's formulation. NASA CARA implemented the three-dimensional Pc method in software (Hall et al. 2017a) and subsequently discovered that, for some conjunctions, it can produce Pc estimates that differ significantly from high-fidelity Monte Carlo Pc calculations.

[sec16.3_p2] Shelton and Junkins (2019) provided a key insight into why the original three-dimensional Pc approximation fails in certain situations. Their analysis indicates that accurate Pc approximations require that the state uncertainty PDFs of the two satellites be estimated accurately in the volume of space where they overlap the most. Hall (2021) reformulated the method to explicitly incorporate this concept. The reformulated approach approximates the curvilinear motion of each satellite using a first-order Taylor series expansion centered on a state that coincides with the maximum overlap of the PDFs for the two satellites, rather than on the mean orbital state.

[sec16.3_p3] [Equation N-5 and N-6: The three-dimensional Nc method involves three numerical integrations. The outermost time integration estimates the number of collisions statistically expected during a risk assessment interval. The collision rate at each time is expressed as a two-dimensional integral over the unit sphere, with the integrand being the product of an average velocity term and a multi-variate normal function evaluated on the surface of the collision sphere. Lebedev quadrature provides an efficient method for numerical integration over the unit sphere.]

[sec16.3_p4] Figure N-5 shows the time-dependent collision rate and the cumulative Pc calculated using the three-dimensional Nc method for an archived CARA conjunction that has a high relative velocity (approximately 13 km/s) but fails to satisfy the rectilinear motion and Gaussian PDF assumptions. In this case, the three-dimensional Nc method calculates a Pc value which accurately matches the Monte Carlo from TCA method Pc estimate, but that is a factor of fifteen larger than the erroneous conjunction plane Pc estimate. It is of interest that the peak point of risk accumulation occurs nine seconds before the TCA, with essentially all the risk accumulated by 8 seconds before TCA.

[Figure N-5: Pc Rate and Cumulative Pc for a Non-Rectilinear Conjunction]

[sec16.3_p5] Figure N-6 shows the collision rate and the cumulative Pc for a low relative velocity conjunction (approximately 3 m/s) that fails to satisfy the rectilinear motion, temporally isolated, and Gaussian PDF assumptions. In this case, the three-dimensional Nc algorithm estimates a Pc value which matches the Monte Carlo from TCA method Pc estimate, but that is a factor of 330 larger than the erroneous conjunction plane Pc estimate.

[Figure N-6: Pc Rate and Cumulative Pc for a Low Velocity Conjunction]

---

### sec16.4 -- Two-Dimensional Nc Method Analytic Pc Calculations

[sec16.4_p1] For temporally isolated conjunctions, the integration over time in the three-dimensional Nc method can be approximated analytically, ultimately yielding a single unit-sphere integral. This relatively efficient "two-dimensional Nc" method yields accurate Pc estimates for high velocity, temporally isolated conjunctions. However, it is not applicable to low velocity events that are temporally extended or blended; these conjunctions still require the use of the three-dimensional Nc method or Monte Carlo estimation.

[sec16.4_p2] [Equation N-7 through N-11: The two-dimensional Nc formulation involves rewriting the MVN function in exponential form with a time-dependent quadratic function Q, finding its minimum time T, expanding to second order about T, and analytically evaluating the time integral. This yields a single two-dimensional integral over the unit sphere computed using Lebedev quadrature. The key quantities involve an effective Mahalanobis distance that accounts for curvilinear trajectory effects.]

---

### sec16.5 -- Comparison of Pc Estimates for Temporally Isolated Conjunctions

[sec16.5_p1] Figure N-7 shows a comparison of collision probabilities for 63,603 temporally isolated conjunctions extracted from the NASA CARA database for the period 2017-05-01 through 2019-08-15 and for events with 2D Pc > 10^-7 (Hall, 2021). The vertical axes plot Monte Carlo from TCA method Pc estimates (also referred to as two-body Monte Carlo method Pc estimates, or TBMC-Pc estimates). The horizontal axes plot the corresponding semi-analytical approximations: two-dimensional Pc on the left graph, three-dimensional Nc in the center, and two-dimensional Nc on the right. Black points indicate analytical Pc estimates that agree reasonably well with the Monte Carlo estimates. Yellow points violate a null-hypothesis that the two are equal at a p-value of 10^-3 significance level, and red points at a more stringent level of p-value of 10^-6.

[sec16.5_p2] Overall, the two-dimensional conjunction plane Pc comparison contains 254 yellow and 436 red points, which both significantly exceed the number of disagreements expected from purely statistical variations, even though together they represent a small fraction (approximately 1%) of the original conjunctions. The three-dimensional Nc method matches Monte Carlo Pc estimates better, producing only 66 yellow and zero red points. The two-dimensional Nc method produces very nearly the same results as the three-dimensional Nc method but requires much less computation time.

[Figure N-7: Comparison of Monte Carlo Collision Probabilities with the Two-Dimensional Pc Method (left), the Three-Dimensional Nc Method (center), and the Two-Dimensional Nc Method (right) for a Large Set of CARA Conjunctions]

---

### sec16.6 -- Pc Calculations for Multi-encounter Interactions

[sec16.6_p1] An additional feature of the two- and three-dimensional Nc methods is their ability to address explicitly the amalgamated risk of repeating conjunctions. While most conjunctions are temporally isolated, there are two conjunction types that exhibit different behaviors: those produced by objects in orbits in synodic alignment that generate a temporal sequence of conjunctions, and those involving objects that orbit close to each other for extended periods generating extended interactions with multiple close approaches. In both cases, situations can arise in which each encounter in the series falls below a Pc mitigation threshold, but the combined risk of all of the encounters exceeds that threshold.

[sec16.6_p2] [Equation N-12 and N-13: The total statistically expected number of collisions for a multi-encounter interaction is given by a summation of Nc values for each conjunction in the sequence. The upper and lower Pc bounds for the combined interaction are given by the maximum individual Nc value as the lower bound and one minus the product of (one minus each individual Nc value) as the upper bound.]

[sec16.6_p3] Figure N-8 shows an example of a multi-encounter interaction, in which a pair of satellites experience four conjunction events over about a five-hour period. Each of the individual conjunctions produces a Pc value in the upper yellow region (between 10^-5 and 10^-4). After the third conjunction, the cumulative Pc exceeds 10^-4 -- a value frequently selected as a risk mitigation threshold. So, while these events would not necessarily prompt a mitigation action if examined individually, when considered collectively they do appear to represent a situation of sufficiently high collision likelihood to warrant mitigation.

[Figure N-8: Cumulative Three-Dimensional Nc Risk Over Four Repeating Conjunctions]

---

### sec16.7 -- Input Covariance Data Considerations

[sec16.7_p1] Most calculations are only as good as the input data that drive them, and Pc calculation is no exception. The purpose of this section is to address the routine improvement and basic error checking extended to covariances as part of the Pc calculation. These activities fall into three basic groups: correction for known problems in propagation, covariance formation and stability issues, and correlation between primary and secondary covariances.

---

#### sec16.7.1 -- Propagation Error Compensation

[sec16.7.1_p1] Historically, accuracy analysis of state estimate and uncertainty estimates has focused on products that emerge directly from the orbit determination fit. While these direct orbit-determination products are foundational, it is important to remember that most SSA activities, and conjunction assessment in particular, are conducted with predictions propagated from the orbit-determination epoch solution, often over a non-trivial duration that spans many orbital revolutions into the future. The batch orbit-determination analysis method used by the DOD produces a formation covariance that represents the expected at-epoch state uncertainty; when the state is propagated forward, a parallel process can also propagate the covariance forward in time using the same dynamical models, although in a linearized way.

[sec16.7.1_p2] Despite the use of appropriate models to propagate the covariance forward in time, a number of additional sources of error manifest themselves during the propagation interval yet are not part of the dynamical model used during the fit. Techniques have been developed to account for them, the most familiar of which is the addition of process noise during propagation. A second approach, which is the one used by DOD, applies parameters to the covariance before propagation to guide the propagation process in producing a more realistic result. For DOD covariances, two different consider parameters are applied to compensate for two distinct expected errors during propagation: atmospheric density forecast error and satellite frontal area uncertainty.

[sec16.7.1_p3] The atmospheric density forecast error was studied with the Jacchia-Bowman High Accuracy Satellite Drag Model (HASDM) 2009 by comparing predicted densities (using predicted space weather indices) to actual densities and producing polynomial fits of the relative density error as a function of satellite perigee height and the Ap and Dst space weather indices. These fits produce a variance term that can be added to the ballistic coefficient variance in the covariance.

[Figure N-9: Behavior of Relative Density Error by Perigee Height and Solar Activity]

[sec16.7.1_p4] The amount of drag acceleration a satellite encounters is also governed by the frontal area that the satellite presents to the atmosphere, reflected in the ballistic coefficient (B) equation. [Equation N-14: The ballistic coefficient B equals the drag coefficient CD times the satellite frontal area A divided by the satellite mass M.] Stabilized satellites should manifest a stable B term, but rotating satellites can exhibit a range of B terms. A history of regularized B histories for individual satellites is examined and a relative error and variance calculated, and this variance is added to the drag variance in the covariance as a corrective term.

[sec16.7.1_p5] There is additional subtlety regarding the exact way these consider parameters are applied. A typical propagation consists of two conceptual stages: the first stage is propagation forward from the epoch time to the present time, using measured and thus highly reliable space weather indices; and the second stage is from the present time to the desired final propagation time, using predicted space weather indices. The satellite rotation consider parameter is applied at epoch for the entire interval. Atmospheric density forecast error is added only for the forward-in-time portion of the propagation.

[Figure N-10: Two-phase Application of Consider Parameters]

[sec16.7.1_p6] If the CDMs generated by the DOD are used for conjunction assessment applications, all the consider parameter activities described above are already performed -- the propagated covariances that the CDM contains have had these two consider parameters applied during the covariance propagation executed at the DOD work center.

---

#### sec16.7.2 -- Defective Covariances

[sec16.7.2_p1] There are several ways in which a covariance contained in a CDM can be defective. Null covariances (all-zero) are sometimes observed, usually arising from a conjunction assessment screening for which the O/O-provided ephemeris does not contain covariance data. In such a case, it is possible to compute the Pc either using only the one covariance that the CDM message contains or by applying a special technique that determines the maximum Pc possible presuming that the null covariance could take on any possible value (Frisbee 2015).

[sec16.7.2_p2] Default covariances are diagonal covariances that contain a value of ten earth radii squared for each of the three position variances. The presence of this covariance indicates that a precision, special-perturbation solution for the object was not possible; the state estimate arose from a general-perturbation solution, and an orbit-determination-produced covariance was not available. Such a result does not constitute a basis for conjunction risk mitigation actions.

[sec16.7.2_p3] Non-positive-semidefinite covariances, now quite rare due to improvements to the DOD operational system, are covariances that contain negative eigenvalues. A recent paper (Hall 2017b) examined the issue in some detail and compared different matrix repair algorithms, finding that most repair approaches yield equivalent answers in terms of the resultant calculated Pc. An "eigenvalue clipping" procedure was developed in which any negative eigenvalues are set to a small positive or zero value.

---

#### sec16.7.3 -- Covariance Correlation

[sec16.7.3_p1] For nearly all the broader conjunction assessment conduct during the past decade, practitioners operated with the presumption that the primary and secondary objects' covariances could be considered uncorrelated. The principal source of potentially correlated error was presumed to be uncharacterized but correlated errors in space sensor observations used by both objects. Because most primaries receive many observations from many different sensors, it was seen as unlikely that this would introduce much correlation.

[sec16.7.3_p2] With the initiative to include outside-of-fit prediction error characterization into DOD satellite covariances, the issue of covariance cross-correlation began to be rethought. The principal outside-of-fit prediction error is global atmospheric density forecast error due to inadequate space weather index prediction. Because this is a global error, it is likely to be shared among large classes of objects.

[sec16.7.3_p3] [Equation N-15: A study by Casali et al. (2018) provides a full development of this theory. The decorrelated joint covariance at TCA equals the sum of the secondary and primary covariances at TCA, minus two correction terms. Each correction term is the product of the density forecast errors germane to both satellites and the sensitivity vectors mapping drag acceleration error to satellite position error at TCA for each object.] A recent enhancement to the DOD operational system has placed all of this information in the CDM itself, allowing the direct calculation of the decorrelated joint covariance.

[sec16.7.3_p4] A heuristic probing of the situation reveals that different levels of Pc changes are possible due to cross-correlation remediation. Orbits insensitive to atmospheric drag are little affected. Head-on conjunctions are expected to be left mostly unaffected. Crossing events are perhaps the most susceptible to cross-correlation effects, especially if the drag level of both satellites is similar. The plot in Figure N-11 profiles 250 conjunctions with non-negligible drag and shows that for somewhat more than half of the events, the ratio hovers near unity. For about one-third of the cases, the decrease in Pc is notable, in many instances more than an order of magnitude. For the remaining cases, there is an increase in Pc from a factor of 1.5 to 5.

[Figure N-11: Profiles of 250 Conjunctions with Primary and Secondary Satellites of Non-negligible Drag]

---

### sec16.8 -- Monte Carlo Pc Calculation Techniques

[sec16.8_p1] Analytic approaches to Pc calculation are more computationally efficient than Monte Carlo methods, especially the conjunction plane two-dimensional Pc method. However, analytic methods require certain enabling assumptions that are not necessarily valid for all conjunctions. Monte Carlo approaches require fewer enabling assumptions, but they are not typically employed as the first method of computation. As described by Hall et al. (2018) and Hall (2021), there are two strains of the Monte Carlo method that are regularly employed: "Monte Carlo from epoch," in which the orbit-determination-epoch mean states and covariances are used to generate sampled states with potentially long multiday high-fidelity propagations; and "Monte Carlo from TCA," in which the mean states and covariances predicted at TCA are used with only relatively short propagations required.

---

#### sec16.8.1 -- Monte Carlo from Epoch

[sec16.8.1_p1] The principal appeal of calculating the Pc using the Monte Carlo from epoch approach is that it requires almost no simplifying or restrictive assumptions, making it as close to a "gold standard" for Pc estimation as can be devised. The most elaborate instantiation of this technique uses the full eight-dimensional orbital state vectors along with the associated 8x8 covariance matrices for the primary and secondary satellites. For each Monte Carlo trial, a state perturbation is obtained by performing a random draw from the distribution using the covariance matrix, these perturbations alter the initial states, both sampled epoch states are propagated forward, a TCA identified, and a check performed to determine whether the miss distance at TCA is smaller than the hard-body radius.

[sec16.8.1_p2] Several subtleties require attention for LEO conjunctions: the same force models and settings must be used for Monte Carlo propagations as were used for the original orbit-determination solution; correlation between primary and secondary covariances should be considered; and Monte Carlo from epoch often requires extremely long computation run times. For example, validating a Pc estimate of approximately 10^-7 at the 95% confidence level for a conjunction with TCA five days from epoch would require an estimated two years of execution time on a 20-CPU server (Hall et al. 2018).

[sec16.8.1_p3] A study effort determined that the Monte Carlo from epoch method appears to be needed only when the two objects stay in such close proximity that they experience a sequence of multiple, comparable-risk close approaches during a multiday risk assessment projection interval. For closely spaced co-orbiting objects, these conjunctions may become effectively "blended" in time with one another such that collision probability accumulates appreciably even during the mid-point times between encounters.

[sec16.8.1_p4] [Equation N-16: The total probability from amalgamating individual encounter Pc values using DeMorgan's Law of Complements is one minus the product of (one minus each individual Pc value). This potentially overestimates the overall collision likelihood because it presumes that the individual events are statistically independent, but in fact they may not be. This total probability estimate provides an upper bound for the amalgamated risk over the time interval of interest.]

[sec16.8.1_p5] Figure N-12 shows a temporal risk plot for a repeating conjunction sequence. The pink shaded area shows the Monte Carlo Pc estimation confidence region, and the pink line shows the best estimate Monte Carlo result. The black upper line, which is the upper bound estimate from the three-dimensional Nc function, is within the confidence interval of the Monte Carlo results and thus is a reasonable realization of the repeating conjunctions' cumulative risk.

[Figure N-12: Three-Dimensional Nc Temporal Risk Plot with Monte Carlo from Epoch Result Overlay (in Pink)]

[sec16.8.1_p6] Because it is complicated to set up the execution environment for the Monte Carlo from epoch calculation, and because "gold standard" results require assembling extensive environmental data and software settings identical to the original DOD orbit-determination solutions, it is envisioned that the ability to run this strain of Monte Carlo estimation will remain with NASA CARA.

---

#### sec16.8.2 -- Monte Carlo from TCA

[sec16.8.2_p1] A much more computationally efficient variant, Monte Carlo from TCA (or two-body Monte Carlo, TBMC-Pc estimation), begins with the primary and secondary objects' equinoctial element states propagated to TCA. Each sampled primary and secondary state is propagated both forward and backward in time to find the pair's TCA and determine whether the corresponding miss distance is smaller than the hard-body radius. Since one is beginning from TCA, the propagations are short, meaning that an efficient two-body motion propagation scheme usually provides an accurate trajectory approximation, vastly improving computational efficiency by a factor of 10,000 to 100,000 (Hall et al. 2018).

[sec16.8.2_p2] A number of studies have indicated that propagated covariances represented in Cartesian space fail to represent the actual uncertainty distributions, due to a mismatch between elongated in-track uncertainty volumes and the forced representation as Cartesian ellipsoids. The actual in-track error volume should follow the curved trajectory of the orbit, but the Cartesian covariance is limited to the rectilinear representation.

[Figure N-13: Mismatch between Elongated In-track Covariances and Forced Cartesian Ellipsoidal Representation]

[sec16.8.2_p3] A study by Sabol et al. (2010) found that it is not the specific state representation in which the propagation is executed but rather the one in which the propagated covariance is rendered that ultimately governs the realism of the uncertainty distribution. Specifically, if the covariance is rendered and used in a curvilinear state representation, such as equinoctial elements, then it tends to represent the error volume much more accurately. A non-representative Cartesian covariance transformed into an equinoctial covariance becomes a representative covariance.

[sec16.8.2_p4] The detailed procedure is: (1) Convert both objects' states and covariances at TCA to equinoctial elements. (2) Generate perturbations for each object based on equinoctial covariances. (3) Combine with mean equinoctial states to generate sampled states. (4) Convert sampled states from equinoctial elements to Cartesian coordinates using the non-linear transformation. (5) Propagate using two-body equations of motion to find the new TCA. (6) Determine whether the new miss distance is less than the hard-body radius and register a hit if so. (7) Repeat until all trials are processed. (8) Pc is hits divided by total trials.

[sec16.8.2_p5] A study by Hall et al. (2018) evaluated 373 high-Pc conjunctions with both Monte Carlo from epoch and Monte Carlo from TCA, showing strong agreement with the largest deviations about 0.2 of an order of magnitude in Pc, considered below operational significance.

[Figure N-14: Comparative Results of 373 High-Pc Conjunctions]

[sec16.8.2_p6] A further study involved 63,603 temporally isolated conjunction events. Figure N-15 shows that the two-dimensional Pc method underestimates from TCA Pc values by a factor of 1.5 or more in 0.258% of the investigated cases. For those cases that showed large disparities, Monte Carlo from epoch reruns matched the output from Monte Carlo from TCA. The two- and three-dimensional Nc methods do not similarly underestimate.

[Figure N-15: Comparative Results of 63,603 Conjunction Events]

[sec16.8.2_p7] The development and testing of the two- and three-dimensional Nc analytic methods have led NASA CARA to recommend that these methods supplant routine use of Monte Carlo from TCA. Testing indicates that the Nc calculation methods outperform Monte Carlo from TCA and are at least an order of magnitude more computationally efficient, especially for events with small Pc values.

[sec16.8.2_p8] A dedicated study on covariance Gaussianity (Lechtenberg 2019b) found that at a 5% significance level, only 60% of cases could be considered to conform to a multivariate Gaussian distribution in Cartesian coordinates. However, since such a large fraction of investigated cases show good agreement between Monte Carlo at TCA and the two-dimensional Pc, clearly the Gaussianity of the covariances in the Cartesian framework does not matter appreciably for the Pc calculation for high-Pc events. This is because for the probability of collision to be high, the central parts of the covariances have to overlap substantially, making tail behavior less important. There is the lingering question of the eventual failure of linearized dynamics in propagating covariances, but this is believed to require propagation durations much longer than encountered for most conjunctions -- on the order of weeks.

---

### sec16.9 -- Choosing an Appropriate Hard-Body Radius

[sec16.9_p1] The hard-body radius represents the combined size of the primary and secondary objects. It is not just a required input to the Pc calculation; it is also one of the governing parameters: the Pc value varies roughly in proportion to the square of the hard-body radius, so an increase of the hard-body radius by a factor of three increases the calculated Pc by a factor of nine. Because of this sensitivity, it is important not to overstate the hard-body radius simply for "conservatism." The best overall strategy for applying conservatism is to apply it at the end of the process by lowering the Pc threshold at which mitigation actions should occur.

[sec16.9_p2] Mashiku and Hejduk (2019) completed a study of different hard-body radius calculation and setting strategies. (1) Large a priori value: for some years it was standard practice to choose a notably larger value than the expected actual size; 20 meters was typically used. Nearly all O/Os have moved away from this strategy. (2) Circumscribing sphere: the most commonly used present operational technique, placing a sphere's center at the center of mass and defining the radius by the length to the satellite's most distant extremity, then increasing by the expected size of the secondary object. (3) Maximum projected area into any plane: determining in advance the maximum area the satellite can project into any plane and using a hard-body radius circle of that equivalent area. (4) Projection into actual conjunction plane: the most accurate but most difficult approach, requiring a three-dimensional CAD model plus knowledge of inertial attitude at TCA.

[sec16.9_p3] A reasonable via media would appear to be approach (3), which keeps the hard-body radius value grounded in reality and free from excessive conservatism but avoids the difficulties of gathering and maintaining shape and attitude data.

[sec16.9_p4] CARA has undertaken an effort to estimate hard-body radii of unknown orbiting objects based on radar cross section (RCS) measurements obtained by the Space Fence radar system (Baars and Hall 2022; Hall and Baars 2022). RCS-based hard-body radius estimates are typically uncertain by factors of two to three, so it is essential that Pc estimates account for these uncertainties. [Equation N-17: For an unknown secondary, the effective combined hard-body radius equals the square root of the sum of the squared mean combined radius (primary radius plus mean secondary radius) and the secondary radius variance. This formula accounts for uncertainty in secondary object size when computing Pc.] CARA recommends calculating Pc values using this effective combined hard-body radius for all conjunctions involving secondary objects with uncertain sizes.

[sec16.9_p5] Currently, Space Fence RCS measurements sufficient for size estimation are available for over 90% of CARA conjunctions involving unknown secondary objects. Analysis indicates that about 98% of such conjunctions involve secondary objects with mean hard-body radius estimates less than 35 cm, and only about 0.3% have mean radius greater than 1.5 m. For unknown objects with insufficient RCS data, one can assign a reasonably conservative default size estimate of 1.5 m.

---

## sec17 -- Appendix O: Collision Consequence

### sec17.1 -- Introduction

[sec17.1_p1] It is common in conjunction assessment circles to speak of "collision risk metrics" such as the Probability of Collision (Pc), but in fact, calling such parameters risk metrics is a misnomer. As laid out formally by Kaplan and Garrick (1981) and adapted by NASA's own project risk assessment paradigms, risk is actually the combination of the likelihood of a detrimental event and the consequence if such an event should come to pass. Conjunction risk assessment metrics such as the Pc are simply measures of collision likelihood, which is only part of the overall risk assessment. Because conjunction assessment practice arose originally from individual missions whose focus was to protect their own spacecraft, it seemed unnecessary to add any considerations beyond collision likelihood.

[sec17.1_p2] When the conjunction assessment risk analysis for groups of satellites or an entire agency or nation is considered, the problem broadens. There is the desire to see individual missions protected from catastrophic termination, but there is a second parallel consideration: the preservation of orbital corridors from debris pollution to ensure their utility for future missions. The NASA Orbital Debris Program Office (ODPO) has outlined spacecraft design and disposal recommendations to mitigate orbital debris production, but there has not yet been an effort to establish clear requirements for debris minimization through on-orbit conjunction assessment activities during mission lifetimes. To reduce the production of debris, one must not only consider the likelihood of any given collision but also evaluate the amount of debris that such a conjunction, should it result in a collision, may engender.

---

### sec17.2 -- Debris Production Determination Methodology

[sec17.2_p1] Different types of satellite collisions can produce substantially different amounts of debris. The ODPO has studied this phenomenon through staged collisions of satellites with simulated debris objects in vacuum chambers and established relationships among conjunction parameters and the number and size of the resultant debris objects (Johnson et al. 2001). The basic distinction is between catastrophic collisions, in which both the primary and secondary object are fully fragmented and generate large amounts of debris, and non-catastrophic collisions in which the smaller/lighter satellite is fully fragmented but the larger/heavier satellite only cratered, generating much smaller amounts of debris.

[sec17.2_p2] [Equation O-1: The catastrophic/non-catastrophic distinction is governed by the relative kinetic energy of the encounter, epsilon, defined as the ratio of the lighter object's mass to the heavier object's mass multiplied by half the square of the relative velocity. When this value exceeds 40,000 Joules per kilogram, the collision is expected to be catastrophic.] [Equation O-2: The ODPO developed a relationship estimating the number of debris pieces greater than a specified characteristic length Lc: for non-catastrophic collisions, the count depends on the product of the relative velocity and the lighter object's mass raised to the 0.75 power; for catastrophic collisions, it depends on the sum of both masses raised to the 0.75 power.] According to Lechtenberg and Hejduk (2019) and Lechtenberg (2019a), "trackable" fragments are those with characteristic lengths exceeding a cutoff of Lc = 0.05 m.

[sec17.2_p3] These relationships conform to first-order intuition regarding expected relative levels of debris production. The full versus partial fragmentation division introduces a discontinuity in the debris production relationship, and the entire dynamic, illustrated by Figure O-1, shows the extremely broad range of debris production outcomes -- from very few debris pieces to thousands -- all of which represent real possibilities for satellite conjunctions.

[Figure O-1: Range of Debris Production Possibilities]

[sec17.2_p4] [Equation O-3: Hall and Baars (2022) introduced a "fragmentation probability" metric that measures a conjunction's probability of producing more than a threshold number of trackable fragments F. This is calculated as the expectation value of the product of the collision probability (which depends on both objects' hard-body radii) and a unit step function indicating whether the estimated fragment count exceeds the threshold F.]

[sec17.2_p5] Since the debris production potential can vary so greatly among different conjunctions, it makes sense that conjunction remediation requirements should take cognizance of this parameter. The conjunction relative velocity is provided directly in the CDM, and the HBR and mass of the primary object are known by the satellite O/O. The HBR and mass of the secondary object, however, are more elusive; in the great majority of conjunctions, this object is a debris object for which there is no a priori information. One must therefore look to estimation processes.

---

### sec17.3 -- Estimating the Sizes of Unknown Objects

[sec17.3_p1] Stokely et al (2006) describe the NASA size estimation model (SEM), which provides a semi-empirical method to convert radar cross-section (RCS) measurements for an unknown satellite into a statistical ensemble of estimates for the "characteristic length" of the object. This characteristic length is not equivalent to the hard-body radius but instead represents the average of the largest dimensions of an object measured along three orthogonal axes. [Equation O-4: The SEM converts RCS measurements into characteristic length estimates using three regimes based on the ratio of RCS to squared radar wavelength, with different formulas for large, small, and intermediate values of this ratio.]

[sec17.3_p2] As described in Hall and Baars (2022), numerous RCS measurements are used per satellite over extended time periods (e.g., approximately 1,000 measurements acquired over approximately 2 years) to analyze the resulting characteristic length distributions to estimate both the unknown hard-body radii and the associated uncertainties. The analysis is restricted to objects with twenty or more well calibrated and quality-curated Space Fence system RCS measurements.

[sec17.3_p3] [Equation O-5 and O-6: The mean characteristic length for a satellite is approximated as the average of all individual RCS-based characteristic length estimates. The corresponding variance is the average of squared deviations from the mean. Because the variable radar reflection properties of a satellite cause some of the variation, the empirical PDF approximation tends to overestimate the variance of the object's actual characteristic size distribution.]

[sec17.3_p4] [Equation O-7: A calibration factor is introduced to relate the circumscribing HBR to the characteristic length. For idealized spherical objects, the HBR would simply be half the characteristic length. The calibration factor, expressed in logarithmic form, is assumed time-invariant but varies from satellite to satellite.] Hall and Baars (2022) uses a set of relatively small calibration satellites with convex cuboid shapes from the DISCOS database, restricted to satellites with circumscribing HBR of 0.35 m or less.

[sec17.3_p5] [Equation O-8 through O-10: The circumscribing HBR for a box-shaped satellite is calculated as half the diagonal of its bounding box dimensions. Each calibration satellite provides an independent measurement of the logarithmic calibration factor. A Bayesian analysis approach estimates the mean logarithmic calibration factor and variance from the full calibration set.] For the calibration epoch date of 2022-01-15, sufficient RCS data are available for 586 of the DISCOS box-shaped satellites, yielding a mean calibration factor of 0.319 and standard deviation of 0.507.

[Figure O-2: CDF of the Ratio of Calibrated HBR Values to Known HBR Values for 586 Satellites]

[sec17.3_p6] The CDF indicates that calibration uncertainties limit the 95% confidence accuracy range of the HBR estimation process to within a factor of 2.4 for potential underestimations and to within a factor of 3.1 for potential overestimations.

---

### sec17.4 -- Estimating the Masses of Unknown Objects

[sec17.4_p1] Several approaches may be used to estimate the mass of an unknown object. The first involves the use of the orbit determination-based ballistic coefficient (BC) estimate while the second uses the orbit determination-based solar radiation pressure coefficient (SRPC) estimate. For satellites that experience measurable atmospheric drag orbital perturbations, the BC estimation method is preferred. However, for satellites with a perigee height above 450 km in altitude, the SRPC mass estimates are somewhat more accurate.

---

#### sec17.4.1 -- RCS + Ballistic Coefficient Method

[sec17.4.1_p1] [Equation O-11 through O-13: The RCS+BC mass estimation method approximates the characteristic mass of a LEO satellite as the product of the projected area (estimated from the characteristic length), the inverse ballistic coefficient, and the drag coefficient. A positive scalar calibration factor quantifies the bias and uncertainty of this estimation process. The calibration analysis uses DISCOS satellites with known masses to estimate the logarithmic calibration factor.]

[sec17.4.1_p2] [Equation O-14 through O-16: The mean projected area is approximated from the mean characteristic length and associated variance. The inverse ballistic coefficient PDF is modeled as a lognormal distribution to avoid producing physically unrealistic negative values. The inverse ballistic coefficient and uncertainty for each satellite are estimated from orbit determination solutions accumulated over the six months prior to the calibration epoch date, using a weighted averaging scheme.]

[sec17.4.1_p3] [Equation O-17 through O-20: The weighted average inverse ballistic coefficient combines multiple orbit determination solutions weighted by their uncertainties. The variance estimate conservatively uses the larger of two methods: one assuming statistical independence and one accounting for observed scatter. The drag coefficient is modeled as a uniformly distributed variable with bounds of 2.1 to 2.9, yielding a mean of 2.5.]

[sec17.4.1_p4] [Equation O-21: The mass calibration process uses the same Bayesian method as the HBR calibration. Each calibration satellite with sufficient RCS data and VCM BC estimates provides an independent measurement of the logarithmic mass calibration factor.] For the calibration epoch date of 2022-01-15, sufficient data are available for 554 satellites, yielding a mean logarithmic calibration factor of -0.159 and standard deviation of 0.959.

[Figure O-3: CDF of the Ratio of Calibrated RCS+BC Mass Estimates to Known Masses for 554 Satellites]

[sec17.4.1_p5] The CDF indicates that calibration uncertainties limit the 95% confidence accuracy range of the RCS+BC mass estimation process to within a factor of 4.1 for potential mass underestimations and to within a factor of 10.4 for potential mass overestimations.

---

#### sec17.4.2 -- RCS + Solar Radiation Pressure Coefficient Method

[sec17.4.2_p1] [Equation O-22 and O-23: The RCS+SRPC mass estimation method approximates the characteristic mass as the product of the projected area, the inverse solar radiation pressure coefficient, and the reflectivity coefficient. The same calibration factor approach is applied as for the RCS+BC method.] The analysis assumes a uniform PDF for the reflectivity coefficient with bounds of 1.0 to 1.4, where the lower bound corresponds to dark bodies and the upper bound represents the theoretical value for a Lambertian sphere with 90% reflectance.

[sec17.4.2_p2] The analysis restricts RCS+SRPC mass estimation to satellites with perigee altitudes above 450 km to ensure that imperfectly modeled atmospheric drag perturbations do not bias the solar radiation pressure coefficients. For the calibration epoch date of 2022-01-15, sufficient data are available for 303 satellites, yielding a mean logarithmic calibration factor of 0.173 and standard deviation of 0.884. The RCS+SRPC method provides somewhat more accurate mass estimates than the RCS+BC method for satellites above 450 km, although both methods provide rough mass estimates overall.

[Figure O-4: CDF of the Ratio of Calibrated RCS+SRPC Mass Estimates to Known Masses for 303 Satellites]

---

### sec17.5 -- The Probability of Exceeding a Threshold Number of Trackable Fragments

[sec17.5_p1] [Equation O-24: For known-on-unknown conjunctions, the fragmentation probability is estimated using Monte Carlo methods, averaging over five random variables that account for the size and mass estimation uncertainties of the secondary object. The fragmentation probability is estimated as the average over all Monte Carlo trials of the product of the collision probability and the binary indicator of whether the fragment count exceeds the threshold.] A conjunction's fragmentation probability cannot exceed the collision probability, making it a convenient companion metric for risk assessment.

[sec17.5_p2] Using a threshold of 1,000 trackable fragments to assess an unacceptably high level of risk to the orbital environment, among the conjunctions analyzed, 1,734 represent red-level close approach risks with collision probability at or above 10^-4. Among these, 122 (or 7%) also have fragmentation probability at or above 10^-4, representing red-level environmental risks as well. A more moderate threshold of 100 fragments yields about 20% of conjunctions with dual high risk. The range of 100 to 1,000 fragments indicates that 5% to 20% of high mission risk conjunctions could also pose elevated risk to the orbital environment.

[Figure O-5: Collision and Fragmentation Probabilities for Known-on-Unknown Conjunctions]

[sec17.5_p3] No O/O would be asked to take advantage of the low-debris-producing-event possibility for relaxed mitigation requirements. However, from an orbit regime protection point of view, it would not be unreasonable to relax the threshold mildly for situations that are non-catastrophic and only expected to create a small amount of debris. The use of fragmentation propensity may also be desirable for triage situations in which a mission is experiencing more serious conjunctions than it is possible to remediate, directing mitigation focus to those conjunctions that hold the greatest debris production potential.

---

## sec18 -- Appendix P: Event Actionability

### sec18.1 -- Introduction

[sec18.1_p1] The term "conjunction event actionability" refers to the assessment of the propagated state and state uncertainty information for the two satellites in conjunction to determine whether these data constitute a reasonable basis for conjunction risk assessment and mitigation actions. It does not include the further step of deciding whether mitigation actions are excessively invasive or detrimental to the mission's objectives. One arrives at this decision point only after determining that the orbital data feeding the analysis are durable and thus are enabling credible collision risk calculations.

[sec18.1_p2] The desire for a conservative risk approach does not make the problem simpler. If the input astrodynamics data are questionable, it is also questionable whether any proposed mitigation action will actually improve the situation because such an action could in fact make the situation worse. It is necessary to establish a method for identifying those cases in which the input data may be sufficiently poor to make meaningful risk assessment impossible.

[sec18.1_p3] It is important to recognize that the question is whether the orbital data properly represent the two objects' states and uncertainties and not whether the associated uncertainties are large or "too large." If one embraces Pc as the principal collision likelihood assessment metric, excessively large covariances have the effect of depressing the Pc value. If, despite a somewhat tracking-data-impoverished situation, a worrisome Pc is still produced, a fortiori the situation should be treated as serious and worthy of mitigation.

[sec18.1_p4] This area of conjunction risk assessment possesses the least precision in terms of clear, documentable standards. The certification of orbit-determination updates as valid and propagation intervals as sufficiently representative is rendered not through conformity to strict rules but rather through the judgment of expert orbital analysts. There are two separate aspects to consider: the states and covariances at epoch as outputs of the orbit-determination process, and the states and covariances propagated forward to TCA.

---

### sec18.2 -- Evaluation of the Propriety of the Orbit-Determination Fit

[sec18.2_p1] A quality orbit-determination fit is a foundational element in any use of state and uncertainty data. Because the USSPACECOM catalog is at present usually the sole supplier for all data on the secondary object, the focus has to be placed on the orbit-determination fit mechanism used for this catalog, which is not a sequential estimator but a minimum-variance batch process. The time-span of tracking data, data density, sensor composition, and retention of sensor data must all be considered.

[sec18.2_p2] Despite decades of attempts, astrodynamics specialists for DOD have concluded that hard-and-fast rules for determining the control settings and data handling for batch updates are simply not possible. The only way truly to evaluate the propriety of an orbit-determination fit is a manual review by an expert with access to tables of residuals, residual plots, before-and-after values of key orbital elements, and the ability to attempt multiple corrections with different settings. However, a set of guidelines can be provided that, if violated, would typically prompt a manual review. Orbit-determination results that do conform to all of the general rules can, with a high degree of confidence, be accepted as a basis for conjunction risk assessment.

---

### sec18.3 -- Force Model Settings and Non-Conservative Force Model Parameter Values

[sec18.3_p1] The selectable force models that can be applied include: (1) Geopotential -- a spherical harmonic representation of the Earth's actual geopotential, with minimum required order determined by satellite orbit characteristics. (2) Lunar-Solar Perturbations -- third-body effects from the sun and moon that should always be engaged for precision solutions. (3) Solid Earth Tides -- gravitational deformation of the Earth by the moon; standard practice to include but unlikely to make an orbit determination questionable if omitted. (4) Atmospheric Drag -- the major non-conservative force affecting LEO orbits, with the entire ballistic coefficient treated as a single solved-for parameter. (5) Solar Radiation Pressure -- momentum imparted by solar electromagnetic radiation, with a solar radiation pressure coefficient analogous to the ballistic coefficient.

---

### sec18.4 -- Orbit Determination Fit Span (Length of Update Interval)

[sec18.4_p1] When using batch estimation, determining the period back in time for data retrieval greatly affects the quality of the resultant orbit determination. If the data period is too long, prediction error is increased; if too short, a robust drag or solar radiation pressure solution is not possible; and if there are too few measurements, the solution is not reliably representative.

[sec18.4_p2] The USSPACECOM batch update process employs the Dynamic Length of Update Interval (LUPI) Assignment (DLA) algorithm to examine the measurement data history and assign the proper LUPI. Updates with LUPI lengths shorter than the default minimum or longer than the theoretical maximum cannot produce reliable state updates and covariances and thus cannot serve as a basis for conjunction risk assessment.

---

### sec18.5 -- Residual Acceptance

[sec18.5_p1] Between iterations of the non-linear regressive solver, measurement residuals against the provisional solution are examined, and data points with excessively large residuals are eliminated to prevent the fit from being influenced disproportionately by a few measurements. This is necessary because incidences of satellite cross-tagging are frequent enough that provision must be made for eliminating inconsistent sensor observations. If too much measurement information is eliminated, the fit may not be properly representative. When the percent of residuals eliminated exceeds a certain value, the fit merits manual review.

---

### sec18.6 -- Weighted Root-Mean-Square of Residuals

[sec18.6_p1] A weighted root-mean-square (WRMS) is used to assess fit quality, in which the square of each residual is divided by the variance of the a priori expected error for that observable type from that particular sensor. If residual errors and actual sensor errors are Gaussian, the WRMS should equal unity. If the WRMS is too large, the fit is not convincing. A WRMS less than unity, while not necessarily bad, would also warrant review as it usually results from too few data in the orbit determination.

---

### sec18.7 -- Default Covariance

[sec18.7_p1] There are situations in which the automatic precision orbit-determination solution cannot converge, and the system executes a general perturbations analytic solution employing the same theory used to produce TLEs. This produces a state with 1-2 km of expected error in LEO but no covariance, indicated by a diagonal covariance with elements of ten Earth radii. General perturbation solutions are not sufficiently accurate to serve as a basis for conjunction mitigation actions.

---

### sec18.8 -- Review Process

[sec18.8_p1] If full orbit-determination data are present and a subject matter expert is available, situations in which the general rules are violated can be investigated and adjudicated. If this level of access is not available, the rules serve as a one-sided test: if the current orbit determination conforms to all rules, it can be presumed durable and can serve as a basis for conjunction risk assessment and mitigation action planning.

---

### sec18.9 -- Evaluation of Propagation from Epoch to TCA

[sec18.9_p1] In addition to considering the propriety of the orbit-determination fits, one must consider whether any issues with the propagation itself might make the TCA products unsuitable for risk assessment. The overall conclusion is that the USSPACECOM approach to producing astrodynamics products at TCA is robust and contains mechanisms to address the major sources of expected propagation error. If an orbit determination sustains the criteria for an acceptable fit, it can be presumed that this fit propagated to TCA renders a state and covariance acceptable for conjunction risk assessment, except in a few narrow instances.

---

#### sec18.9.1 -- Propagated Covariance is Defective

[sec18.9.1_p1] Null covariances (all zeroes) are the equivalent of not having a covariance at all and result in inactionability. It is possible to use maximalist techniques such as those developed by Alfano (2005b) and by Frisbee (2015) to proceed with only one covariance, but these techniques are exceedingly conservative and not recommended as a routine proxy.

[sec18.9.1_p2] Default covariances with diagonal elements of ten Earth radii will force any calculated Pc to zero and sometimes introduce convergence problems. Whether judged as non-actionable or producing a Pc of zero, both lead to the same conclusion of not requiring a mitigation action. Non-positive-semidefinite covariances arise due to truncation and interpolation errors, not fundamental problems with the orbit determination, and can be straightforwardly repaired using eigenvalue clipping techniques. A modification to the USSPACECOM operational system has been implemented to essentially eliminate these situations.

---

#### sec18.9.2 -- Excessive Propagation Interval to Retain Gaussian Covariance Behavior

[sec18.9.2_p1] While debate continues in the critical literature about Gaussian uncertainty volumes and the difficulties of the Cartesian framework, the conjunction assessment enterprise has been able largely to set aside this issue by recognizing that it is not relevant to high-Pc events because of the amount of covariance overlap necessary to produce a high Pc. Furthermore, the issue can be remediated by performing Monte Carlo from TCA with random sampling conducted from the equinoctial representation of the covariance. Based on NASA CARA's analysis, this problem is not crippling for the categories of events that actually matter to conjunction risk assessment.

---

#### sec18.9.3 -- Excessive Propagation Interval Due to a Lack of Tracking Data

[sec18.9.3_p1] The USSPACECOM orbit-determination process places the epoch time at the time of the last observation. If no new tracking data are received for some time, a quite long propagation may be required. As the propagation proceeds, uncertainties in the propagated covariance grow appropriately, and adjustments for atmospheric density forecast error and satellite frontal area uncertainty are included. A large covariance will in most cases depress the calculated Pc, naturally correcting for a situation in which state uncertainties are so large as to be uncomfortable for action.

[sec18.9.3_p2] Nonetheless, a rule of thumb is that the propagation interval should not be longer than the fit-span/LUPI. It is generally considered dangerous to try to propagate longer than the span of measurement data that directed the orbit-determination update. It is reasonable to refrain from any mitigation action for conjunctions for which either object's orbit determination at TCA was generated with so extended a propagation interval.

---

#### sec18.9.4 -- Excessive Atmospheric Density Error Due to Solar Storms

[sec18.9.4_p1] The USSPACECOM orbit-determination process adjusts the covariance for anticipated atmospheric density forecast error through a consider parameter. The JBH09 atmospheric density model contains the "Anemomilos" model that attempts to predict CME size, arrival time, and geoeffectiveness. However, the performance of Anemomilos during solar cycle 25 has been disappointing: normed on data from earlier cycles, the model is not producing representative results presently. This model engages only when the predicted Dst parameter value is smaller than -75 nanoTeslas.

[sec18.9.4_p2] Additional insight can be gained by constructing Space Weather Trade Space (SWTS) plots that explore how collision likelihood would change if atmospheric density were mismodeled. Because atmospheric density and ballistic coefficient are multiplicatively coupled, varying one has the same effect as changing the other. The plots vary the nominal ballistic coefficient by half an order of magnitude in each direction. Three morphologies are meaningful: an invariant situation where Pc changes little despite large variations; a "ridge" situation where the current Pc sits at the maximum and any mismodeling lowers it; and a variation situation where mismodeling could either increase or decrease the Pc.

[Figure P-1: SWTS Plot for Invariant Situation]

[Figure P-2: SWTS Plot for "Ridge" Situation]

[Figure P-3: SWTS Plot for Variation Situation]

---

### sec18.10 -- Summary

[sec18.10_p1] The orbit determination and propagation process nearly always produces data that can be considered actionable. There are certain indications that an orbit-determination fit may potentially fall short, and an even smaller number that would counsel dismissal of an orbit determination as the basis for conjunction risk assessment. It is similarly infrequent that the propagation results to TCA can be questioned, and these situations can usually be addressed in other ways. In both situations, there are additional one-sided tests that, if passed, can reassure the user about the propriety of the data.

---

### sec18.11 -- Annex: Orbit-Determination Quality Check Thresholds

[sec18.11_p1] Table P-1 and Table P-2 provide specific thresholds for geopotential order, atmospheric drag modeling, solar radiation pressure modeling, and acceptable ranges for the ballistic coefficient and solar radiation pressure coefficient, organized by perigee height, eccentricity, and object type. Geopotential zonal and tesseral order requirements range from 36 (for low perigee) down to 8 (for high altitude orbits). Drag should be solved for at perigee heights below 2000 km for low-eccentricity orbits and below 1000 km for high-eccentricity orbits. Solar radiation pressure should be solved for at perigee heights above 500 km.

[sec18.11_p2] Table P-3 and Figure P-4 provide LUPI minimum and maximum values organized by energy dissipation rate. LUPI lower bounds range from 1.25 to 14 days, and upper bounds range from 7 to 18 days depending on the EDR value. Additionally: fewer than 80% residual acceptance would generally prompt manual review; WRMS thresholds for excessive values are 1.5 for payloads, 2.0 for rocket bodies/platforms, and 5.0 for debris/unknown objects.

---

## sec19 -- Appendix Q: Notional Display Flow for Conjunction Event Processing

### sec19.1 -- Introduction

[sec19.1_p1] This appendix illustrates how the individual pieces of the conjunction event processing discipline come together to perform on-orbit conjunction risk assessment. The operational orbital safety process is illustrated through a series of graphical displays that conjunction risk assessors use to determine whether a mitigation action is warranted and, if so, what sorts of actions might be effective. The displays are a subset of those used by the NASA CARA group and are presented for a recent actual event that resulted in the planning and execution of a mitigation action.

[sec19.1_p2] NASA executes conjunction assessment screenings once every eight hours. This particular conjunction appeared at the 6.5 days-to-TCA point with an initial Pc greater than 1E-07, placing it in "yellow" status from the very beginning. The displays follow this event from initial detection to the mitigation action commitment point, which for this event was one day prior to TCA.

---

### sec19.2 -- Primary and Secondary Object Background Information

[sec19.2_p1] In making a risk assessment determination, it is important to have background information about the orbits and maintenance history of the primary and secondary objects. Different orbits have different properties and expected stability, and maintenance histories and parameters testify to the quality of the orbit determination available.

---

#### sec19.2.1 -- Primary Object Information

[sec19.2.1_p1] The primary is in a nearly circular, sun-synchronous LEO orbit with an average altitude of about 700 km. The energy dissipation rate (EDR) value indicates mild drag effects. Both drag and solar radiation pressure are modeled with reasonable solved-for values. The WRMS value is low, the tracking density is reasonably large, and tracking data was collected within the last day, so propagation to TCA will not extend very far. Overall, this primary satellite is well maintained with no indication of problems with the fit or propagation.

---

#### sec19.2.2 -- Secondary Object Information

[sec19.2.2_p1] The secondary object's orbit is not substantially different from the primary's, although the inclination is somewhat lower. Due to a different area-to-mass ratio, the drag influence is much higher: the high EDR value indicates significant drag effect confirmed by a large solved-for ballistic coefficient. The tracking levels are poor at 0.5 tracks per day, an order of magnitude smaller than the primary. The CDM reveals 15 observations spanning a 9.5-day period for the latest orbit determination, with the LUPI at only half the maximum value and thus tolerable. The WRMS is high but below the quality threshold of 5 for a debris object.

[sec19.2.2_p2] While the maintenance history and propagation situation for this object is not the best, it is good enough to serve as a basis for risk assessment, presuming that other issues such as space weather do not ultimately jeopardize the credibility of the propagation.

---

### sec19.3 -- Primary and Secondary Object Trajectories

[sec19.3_p1] The ground trace plot (Figure Q-1) depicts the two satellite trajectories and identifies the closest-approach point over Antarctica, far from the field of regard of any SSN ground sensors. The relative velocity of almost 14 km/sec is both very fast and typical for LEO conjunctions, large enough to allow hyperkinetic assumptions and simplified Pc calculation approaches such as the 2D Pc. The approach angle describes an oblique encounter, which is the typically encountered situation.

[Figure Q-1: Ground Trace Plot]

---

### sec19.4 -- Conjunction Plane Information

[sec19.4_p1] The Conjunction Plane Plot (Figure Q-2) shows the relative miss vector along the x-axis terminating at a circle representing the hard-body radius. The ellipses represent the combined uncertainty of both objects' positions resolved into the collision plane. For low-likelihood conjunctions, the HBR circle is far outside the three-sigma ellipse. In this case, the circle is within the one-sigma error ellipse with visible ellipse definition, indicating more varied uncertainties.

[Figure Q-2: Conjunction Plane Plot]

---

### sec19.5 -- Collision Probability Evolution

[sec19.5_p1] The Pc time history (Figure Q-3) shows the probability of collision for both the DOD-based solution (ASW) and the owner/operator ephemeris-based solution (O/O). Graphic markers are filled for updates that received new tracking. The secondary object is not particularly well tracked, with most updates based only on space weather coefficient updates rather than fresh tracking data. The significant state update containing new tracking (just under two days to TCA) is barely under the mitigation threshold, and after additional space weather updates, the last update before the commitment point exceeds the red threshold.

[Figure Q-3: Probability of Collision Time-History Plot]

[sec19.5_p2] While there is large variation in the Pc history, it is not so extreme as to indicate that the situation is too unstable for mitigation action planning. Based provisionally on this analysis, it would be recommended for this O/O to take a mitigation action.

---

### sec19.6 -- Miss Distance Evolution

[sec19.6_p1] The total miss distance history (Figure Q-4) is examined alongside the Pc history. There are cases in which miss distance appears to reduce collision risk but covariance evolution keeps collision likelihood high. In this event, the miss distance evolution is not in obvious lock-step with the Pc evolution, but the miss distance has moved to a very low value in the last update, small enough to have driven the Pc above the mitigation threshold. The miss distance behavior does not work against a conclusion that collision mitigation is appropriate.

[Figure Q-4: Total Miss Distance Plot]

---

#### sec19.6.1 -- Miss Distance Component Behavior

[sec19.6.1_p1] The componentized miss distance plot (Figure Q-5) separates the miss distance into radial, in-track, and cross-track components. Ideal behavior is stable miss distance values with shrinking uncertainties as propagation length decreases. The in-track results essentially exhibit this behavior. The cross-track results are somewhat less well-behaved but converge. The radial component, the most determinative for the Pc, does not behave so nicely: it changes sign and does not uniformly focus on a single value. However, when fresh tracking is obtained, it jumps to a worrisome value near zero and stays reasonably stable to the commitment point, giving additional confidence that the high Pc should be taken seriously.

[Figure Q-5: Componentized Miss Distance Plot]

---

#### sec19.6.2 -- Normalized Component Miss Distance Value Comparison

[sec19.6.2_p1] The State Comparison Consistency Plot (Figure Q-6) represents normalized values: the change in componentized miss distance divided by the expected error from the previous update. Ideal performance would have updates sitting near the zero line, with approximately 68% of ratios falling within the plus/minus one-sigma region. The behavior shown is quite desirable, with most values near the zero-line and few deviating beyond one sigma. This is a reasonably well-behaved event whose final result can be trusted to yield an actionable result.

[Figure Q-6: State Comparison Consistency Plot]

---

### sec19.7 -- Space Weather Considerations

#### sec19.7.1 -- Solar Storms

[sec19.7.1_p1] The solar storm prediction display (Figure Q-7) shows history and prediction of the Dst parameter, with a threshold of -75 nanoTeslas indicating a disturbed situation. In this event, there was a dip below this value, then a recovery increase, then residual dipping before steadying off. The prediction shows a linear increase to the zero value indicating stability. The space weather situation was somewhat perturbed in the recent history but appears to have largely stabilized at the current time, with the future situation looking only more stable. One can be reasonably comfortable acting on the present data at the maneuver commitment point.

[Figure Q-7: Solar Storm Prediction]

---

#### sec19.7.2 -- Space Weather Modeling

[sec19.7.2_p1] The Space Weather Trade-Space plot (Figure Q-8) shows the expected effect on Pc if space weather has been mismodeled. The midpoint represents the current solution, and the morphology indicates that this event sits on a "ridge" where any mismodeling of atmospheric density will result in a lower Pc. If the current Pc is below the action threshold, any mismodeling would only lower it further, and the event can be considered safe. A one-color presentation indicates insensitivity to mismodeling. If the midpoint is on a "slope," mismodeling could push Pc either direction.

[Figure Q-8: Space Weather Trade-Space Plot]

---

### sec19.8 -- Mitigation Maneuver Planning

[sec19.8_p1] The Maneuver Trade-Space plot (Figure Q-9) assists O/Os in planning mitigation actions. A potential burn from a variety of sizes and execution times is placed into the primary object's ephemeris, new Pcs for the main conjunction and any other arising conjunctions are calculated, and an amalgamated Pc is produced across the full range of burn intensities and times. Colors indicate the Pc achieved for each combination. Choosing a maneuver resulting in a Pc of 1E-07 (dark blue) would fully mitigate the conjunction risk.

[Figure Q-9: Maneuver Trade-Space Plot]

[sec19.8_p2] The longer one waits to perform the maneuver, the larger it must be. Maneuvering early allows a smaller maneuver to fully mitigate the conjunction. However, waiting an additional half-day may allow more tracking data to shrink the covariance and push the Pc below the threshold, potentially eliminating the need for a maneuver entirely. This tension must be resolved by the O/O: commit early for a smaller maneuver, or wait hoping the need evaporates completely, with the risk that a larger maneuver would be required.

[sec19.8_p3] In this example, the collision likelihood exceeded the CARA mitigation threshold (1E-04) at the maneuver commitment point, and available mitigation actions were not large enough to negatively affect the satellite's main mission appreciably. Therefore, the O/O made the straightforward decision to perform a mitigation action.
